{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [1. Introduction to Text Mining and NLP](#01)\n",
    "* [2. Extracting, cleaning and pre-processing text](#02)\n",
    "* [3. Analyzing sentence structure](#03)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"01\"></a>\n",
    "<H1 style=\"color: MediumBlue; font-size: 34px; background-color: #E6E6FA; padding: 30px 20px;\">1. Introduction to Text Mining and NLP</H1>\n",
    "\n",
    "#### Table of contents\n",
    "\n",
    "* [Introduction](#01-00)\n",
    "* [Reading external files](#01-01)\n",
    "* [Generating docx reports](#01-02)\n",
    "* [CSV files and Pandas DataFrames](#01-03)\n",
    "* [NLTK](#01-04)\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"01-00\"></a>\n",
    "<H2 style=\"color: MediumBlue; font-size: 21px; background-color: #F0F8FF; padding: 15px 20px;\">Introduction</H2>\n",
    "\n",
    "* Supervised learning\n",
    "* Unsupervised learning (clustering)\n",
    "* Reinforcement learning (learning by experience with penalty/reward)\n",
    "\n",
    "How to find if the learning equation is giving the right prediction -> k-fold training/testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"01-01\"></a>\n",
    "<H2 style=\"color: MediumBlue; font-size: 21px; background-color: #F0F8FF; padding: 15px 20px;\">Reading external files</H2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir('/Users/macbook/python')  # Change directory\n",
    "os.getcwd()  # Get current working directory\n",
    "os.listdir(os.getcwd())\n",
    "\n",
    "os.remove('file/path/here')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/macbook/python/ai-edureka--ai-ml-masters-course/ai-edureka--ai-ml-masters--05-chatgpt4/chatgpt4--00-prerequisites--03-nlp')\n",
    "with open('test_file.txt', 'r') as fp_read:\n",
    "    with open('new_file.txt', 'w+') as fp_write:\n",
    "        for line in fp_read:\n",
    "            fp_write.write(line)\n",
    "fp_read.close()\n",
    "fp_write.close()\n",
    "\n",
    "# w+ mode: creates the file if doesn't exist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"01-02\"></a>\n",
    "<H2 style=\"color: MediumBlue; font-size: 21px; background-color: #F0F8FF; padding: 15px 20px;\">Generating reports</H2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install python-docx\n",
    "import docx\n",
    "document = docx.Document('demo.docx')\n",
    "len(document.paragraphs)\n",
    "document.paragraphs[0].text\n",
    "len(document.paragraphs[0].runs)  # number of words\n",
    "\n",
    "document.save('demo.docx')  # save the updated Word document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "from docx.shared import Inches\n",
    "\n",
    "document = Document()\n",
    "document.add_heading('Title 0', 0)\n",
    "p = document.add_paragraph('A new paragraph having some ')\n",
    "p.add_run('bold ').bold = True\n",
    "p.add_run('and some ')\n",
    "p.add_run('italic').italic = True\n",
    "\n",
    "document.add_heading('Title 1', level=1)\n",
    "\n",
    "document.add_paragraph('Intense quote', style='Intense Quote')\n",
    "document.add_paragraph('First list item', style='List Bullet')\n",
    "document.add_paragraph('First list item', style='List Number')\n",
    "\n",
    "document.add_picture('download.png', width=Inches(1.25))\n",
    "\n",
    "records = (\n",
    "    (3, '101', 'Spam'),\n",
    "    (5, '153', 'Eggs'),\n",
    "    (45, '64', 'Spam and eggs')\n",
    ")\n",
    "\n",
    "table = document.add_table(rows=1, cols=3)\n",
    "header_cells = table.rows[0].cells\n",
    "header_cells[0].text = 'Qty'\n",
    "header_cells[1].text = 'Id'\n",
    "header_cells[2].text = 'Desc'\n",
    "for qty, id, desc in records:\n",
    "    row_cells = table.add_row().cells\n",
    "    row_cells[0].text = str(qty)\n",
    "    row_cells[1].text = id\n",
    "    row_cells[2].text = str(desc)\n",
    "    \n",
    "document.add_page_break()\n",
    "\n",
    "document.save('test.docx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"01-03\"></a>\n",
    "<H2 style=\"color: MediumBlue; font-size: 21px; background-color: #F0F8FF; padding: 15px 20px;\">CSV files and Pandas DataFrames</H2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "df = pd.read_csv('demo.csv')\n",
    "length = len(df)\n",
    "df['new_column'] = pd.Series(np.random.randn(length), index=df.index)\n",
    "df\n",
    "df.to_csv('demo_copy.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2 style=\"color: MediumBlue; font-size: 21px; background-color: #F0F8FF; padding: 15px 20px;\">NLTK</H2>\n",
    "\n",
    "- [NLTK Documentation](https://www.nltk.org/)   \n",
    "- [API Reference](https://www.nltk.org/api/nltk.html)   \n",
    "- [Example usage](https://www.nltk.org/howto.html)   \n",
    "- [NLTK Wiki](https://github.com/nltk/nltk/wiki)\n",
    "- [NLTK on GitHub](https://github.com/nltk/nltk)   \n",
    "  \n",
    "   \n",
    "- [Complete list of NLTK Data](https://www.nltk.org/nltk_data/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Install NLTK:\n",
    "$ pip install nltk\n",
    "\n",
    "# 2. Install NLTK Data:\n",
    "$ python\n",
    ">>> import nltk\n",
    ">>> nltk.download()\n",
    "# Opens the NLTK Data window\n",
    "#   - For central installation, change the download path to: /usr/local/share/nltk_data\n",
    "#   - In the Collections tab, Click on All, then Download button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.1\n",
      "-----\n",
      "Name: nltk\n",
      "Version: 3.8.1\n",
      "Summary: Natural Language Toolkit\n",
      "Home-page: https://www.nltk.org/\n",
      "Author: NLTK Team\n",
      "Author-email: nltk.team@gmail.com\n",
      "License: Apache License, Version 2.0\n",
      "Location: /Users/macbook/anaconda3/lib/python3.7/site-packages\n",
      "Requires: click, joblib, regex, tqdm\n",
      "Required-by: pugnlp\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "print(nltk.__version__)\n",
    "\n",
    "print('-----')\n",
    "\n",
    "!pip show nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chunkers',\n",
       " 'corpora',\n",
       " 'grammars',\n",
       " 'help',\n",
       " 'misc',\n",
       " 'models',\n",
       " 'sentiment',\n",
       " 'stemmers',\n",
       " 'taggers',\n",
       " 'tokenizers']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir('/usr/local/share/nltk_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abc',\n",
       " 'abc.zip',\n",
       " 'alpino',\n",
       " 'alpino.zip',\n",
       " 'bcp47.zip',\n",
       " 'biocreative_ppi',\n",
       " 'biocreative_ppi.zip',\n",
       " 'brown',\n",
       " 'brown.zip',\n",
       " 'brown_tei',\n",
       " 'brown_tei.zip',\n",
       " 'cess_cat',\n",
       " 'cess_cat.zip',\n",
       " 'cess_esp',\n",
       " 'cess_esp.zip',\n",
       " 'chat80',\n",
       " 'chat80.zip',\n",
       " 'city_database',\n",
       " 'city_database.zip',\n",
       " 'cmudict',\n",
       " 'cmudict.zip',\n",
       " 'comparative_sentences',\n",
       " 'comparative_sentences.zip',\n",
       " 'comtrans.zip',\n",
       " 'conll2000',\n",
       " 'conll2000.zip',\n",
       " 'conll2002',\n",
       " 'conll2002.zip',\n",
       " 'conll2007.zip',\n",
       " 'crubadan',\n",
       " 'crubadan.zip',\n",
       " 'dependency_treebank',\n",
       " 'dependency_treebank.zip',\n",
       " 'dolch',\n",
       " 'dolch.zip',\n",
       " 'europarl_raw',\n",
       " 'europarl_raw.zip',\n",
       " 'extended_omw.zip',\n",
       " 'floresta',\n",
       " 'floresta.zip',\n",
       " 'framenet_v15',\n",
       " 'framenet_v15.zip',\n",
       " 'framenet_v17',\n",
       " 'framenet_v17.zip',\n",
       " 'gazetteers',\n",
       " 'gazetteers.zip',\n",
       " 'genesis',\n",
       " 'genesis.zip',\n",
       " 'gutenberg',\n",
       " 'gutenberg.zip',\n",
       " 'ieer',\n",
       " 'ieer.zip',\n",
       " 'inaugural',\n",
       " 'inaugural.zip',\n",
       " 'indian',\n",
       " 'indian.zip',\n",
       " 'jeita.zip',\n",
       " 'kimmo',\n",
       " 'kimmo.zip',\n",
       " 'knbc.zip',\n",
       " 'lin_thesaurus',\n",
       " 'lin_thesaurus.zip',\n",
       " 'mac_morpho',\n",
       " 'mac_morpho.zip',\n",
       " 'machado.zip',\n",
       " 'masc_tagged.zip',\n",
       " 'movie_reviews',\n",
       " 'movie_reviews.zip',\n",
       " 'mte_teip5',\n",
       " 'mte_teip5.zip',\n",
       " 'names',\n",
       " 'names.zip',\n",
       " 'nombank.1.0.zip',\n",
       " 'nonbreaking_prefixes',\n",
       " 'nonbreaking_prefixes.zip',\n",
       " 'nps_chat',\n",
       " 'nps_chat.zip',\n",
       " 'omw-1.4.zip',\n",
       " 'omw.zip',\n",
       " 'opinion_lexicon',\n",
       " 'opinion_lexicon.zip',\n",
       " 'panlex_swadesh.zip',\n",
       " 'paradigms',\n",
       " 'paradigms.zip',\n",
       " 'pe08',\n",
       " 'pe08.zip',\n",
       " 'pil',\n",
       " 'pil.zip',\n",
       " 'pl196x',\n",
       " 'pl196x.zip',\n",
       " 'ppattach',\n",
       " 'ppattach.zip',\n",
       " 'problem_reports',\n",
       " 'problem_reports.zip',\n",
       " 'product_reviews_1',\n",
       " 'product_reviews_1.zip',\n",
       " 'product_reviews_2',\n",
       " 'product_reviews_2.zip',\n",
       " 'propbank.zip',\n",
       " 'pros_cons',\n",
       " 'pros_cons.zip',\n",
       " 'ptb',\n",
       " 'ptb.zip',\n",
       " 'qc',\n",
       " 'qc.zip',\n",
       " 'reuters.zip',\n",
       " 'rte',\n",
       " 'rte.zip',\n",
       " 'semcor.zip',\n",
       " 'senseval',\n",
       " 'senseval.zip',\n",
       " 'sentence_polarity',\n",
       " 'sentence_polarity.zip',\n",
       " 'sentiwordnet',\n",
       " 'sentiwordnet.zip',\n",
       " 'shakespeare',\n",
       " 'shakespeare.zip',\n",
       " 'sinica_treebank',\n",
       " 'sinica_treebank.zip',\n",
       " 'smultron',\n",
       " 'smultron.zip',\n",
       " 'state_union',\n",
       " 'state_union.zip',\n",
       " 'stopwords',\n",
       " 'stopwords.zip',\n",
       " 'subjectivity',\n",
       " 'subjectivity.zip',\n",
       " 'swadesh',\n",
       " 'swadesh.zip',\n",
       " 'switchboard',\n",
       " 'switchboard.zip',\n",
       " 'timit',\n",
       " 'timit.zip',\n",
       " 'toolbox',\n",
       " 'toolbox.zip',\n",
       " 'treebank',\n",
       " 'treebank.zip',\n",
       " 'twitter_samples',\n",
       " 'twitter_samples.zip',\n",
       " 'udhr',\n",
       " 'udhr.zip',\n",
       " 'udhr2',\n",
       " 'udhr2.zip',\n",
       " 'unicode_samples',\n",
       " 'unicode_samples.zip',\n",
       " 'universal_treebanks_v20.zip',\n",
       " 'verbnet',\n",
       " 'verbnet.zip',\n",
       " 'verbnet3',\n",
       " 'verbnet3.zip',\n",
       " 'webtext',\n",
       " 'webtext.zip',\n",
       " 'wordnet.zip',\n",
       " 'wordnet2021.zip',\n",
       " 'wordnet2022',\n",
       " 'wordnet2022.zip',\n",
       " 'wordnet31.zip',\n",
       " 'wordnet_ic',\n",
       " 'wordnet_ic.zip',\n",
       " 'words',\n",
       " 'words.zip',\n",
       " 'ycoe',\n",
       " 'ycoe.zip']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "os.listdir(nltk.data.find('corpora'))\n",
    "\n",
    "# SEE nltk.data:\n",
    "#  Reference > https://www.nltk.org/api/nltk.data.html\n",
    "#  Examples  > https://www.nltk.org/howto/data.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Sense and Sensibility by Jane Austen 1811 ] CHAPTER 1 The family of Dashwood had long been settled in Sussex . Their estate was large , and their residence was at Norland Park "
     ]
    }
   ],
   "source": [
    "words = nltk.corpus.gutenberg.words('austen-sense.txt')\n",
    "for word in words[:34]:\n",
    "    print(word, sep=' ', end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neg', 'pos']\n",
      "['pos/cv000_29590.txt', 'pos/cv001_18431.txt', 'pos/cv002_15918.txt', 'pos/cv003_11664.txt', 'pos/cv004_11636.txt', 'pos/cv005_29443.txt', 'pos/cv006_15448.txt', 'pos/cv007_4968.txt', 'pos/cv008_29435.txt', 'pos/cv009_29592.txt', 'pos/cv010_29198.txt', 'pos/cv011_12166.txt', 'pos/cv012_29576.txt', 'pos/cv013_10159.txt', 'pos/cv014_13924.txt', 'pos/cv015_29439.txt', 'pos/cv016_4659.txt', 'pos/cv017_22464.txt', 'pos/cv018_20137.txt', 'pos/cv019_14482.txt', 'pos/cv020_8825.txt', 'pos/cv021_15838.txt', 'pos/cv022_12864.txt', 'pos/cv023_12672.txt', 'pos/cv024_6778.txt', 'pos/cv025_3108.txt', 'pos/cv026_29325.txt', 'pos/cv027_25219.txt', 'pos/cv028_26746.txt', 'pos/cv029_18643.txt', 'pos/cv030_21593.txt', 'pos/cv031_18452.txt', 'pos/cv032_22550.txt', 'pos/cv033_24444.txt', 'pos/cv034_29647.txt', 'pos/cv035_3954.txt', 'pos/cv036_16831.txt', 'pos/cv037_18510.txt', 'pos/cv038_9749.txt', 'pos/cv039_6170.txt', 'pos/cv040_8276.txt', 'pos/cv041_21113.txt', 'pos/cv042_10982.txt', 'pos/cv043_15013.txt', 'pos/cv044_16969.txt', 'pos/cv045_23923.txt', 'pos/cv046_10188.txt', 'pos/cv047_1754.txt', 'pos/cv048_16828.txt', 'pos/cv049_20471.txt', 'pos/cv050_11175.txt', 'pos/cv051_10306.txt', 'pos/cv052_29378.txt', 'pos/cv053_21822.txt', 'pos/cv054_4230.txt', 'pos/cv055_8338.txt', 'pos/cv056_13133.txt', 'pos/cv057_7453.txt', 'pos/cv058_8025.txt', 'pos/cv059_28885.txt', 'pos/cv060_10844.txt', 'pos/cv061_8837.txt', 'pos/cv062_23115.txt', 'pos/cv063_28997.txt', 'pos/cv064_24576.txt', 'pos/cv065_15248.txt', 'pos/cv066_10821.txt', 'pos/cv067_19774.txt', 'pos/cv068_13400.txt', 'pos/cv069_10801.txt', 'pos/cv070_12289.txt', 'pos/cv071_12095.txt', 'pos/cv072_6169.txt', 'pos/cv073_21785.txt', 'pos/cv074_6875.txt', 'pos/cv075_6500.txt', 'pos/cv076_24945.txt', 'pos/cv077_22138.txt', 'pos/cv078_14730.txt', 'pos/cv079_11933.txt', 'pos/cv080_13465.txt', 'pos/cv081_16582.txt', 'pos/cv082_11080.txt', 'pos/cv083_24234.txt', 'pos/cv084_13566.txt', 'pos/cv085_1381.txt', 'pos/cv086_18371.txt', 'pos/cv087_1989.txt', 'pos/cv088_24113.txt', 'pos/cv089_11418.txt', 'pos/cv090_0042.txt', 'pos/cv091_7400.txt', 'pos/cv092_28017.txt', 'pos/cv093_13951.txt', 'pos/cv094_27889.txt', 'pos/cv095_28892.txt', 'pos/cv096_11474.txt', 'pos/cv097_24970.txt', 'pos/cv098_15435.txt', 'pos/cv099_10534.txt', 'pos/cv100_11528.txt', 'pos/cv101_10175.txt', 'pos/cv102_7846.txt', 'pos/cv103_11021.txt', 'pos/cv104_18134.txt', 'pos/cv105_17990.txt', 'pos/cv106_16807.txt', 'pos/cv107_24319.txt', 'pos/cv108_15571.txt', 'pos/cv109_21172.txt', 'pos/cv110_27788.txt', 'pos/cv111_11473.txt', 'pos/cv112_11193.txt', 'pos/cv113_23102.txt', 'pos/cv114_18398.txt', 'pos/cv115_25396.txt', 'pos/cv116_28942.txt', 'pos/cv117_24295.txt', 'pos/cv118_28980.txt', 'pos/cv119_9867.txt', 'pos/cv120_4111.txt', 'pos/cv121_17302.txt', 'pos/cv122_7392.txt', 'pos/cv123_11182.txt', 'pos/cv124_4122.txt', 'pos/cv125_9391.txt', 'pos/cv126_28971.txt', 'pos/cv127_14711.txt', 'pos/cv128_29627.txt', 'pos/cv129_16741.txt', 'pos/cv130_17083.txt', 'pos/cv131_10713.txt', 'pos/cv132_5618.txt', 'pos/cv133_16336.txt', 'pos/cv134_22246.txt', 'pos/cv135_11603.txt', 'pos/cv136_11505.txt', 'pos/cv137_15422.txt', 'pos/cv138_12721.txt', 'pos/cv139_12873.txt', 'pos/cv140_7479.txt', 'pos/cv141_15686.txt', 'pos/cv142_22516.txt', 'pos/cv143_19666.txt', 'pos/cv144_5007.txt', 'pos/cv145_11472.txt', 'pos/cv146_18458.txt', 'pos/cv147_21193.txt', 'pos/cv148_16345.txt', 'pos/cv149_15670.txt', 'pos/cv150_12916.txt', 'pos/cv151_15771.txt', 'pos/cv152_8736.txt', 'pos/cv153_10779.txt', 'pos/cv154_9328.txt', 'pos/cv155_7308.txt', 'pos/cv156_10481.txt', 'pos/cv157_29372.txt', 'pos/cv158_10390.txt', 'pos/cv159_29505.txt', 'pos/cv160_10362.txt', 'pos/cv161_11425.txt', 'pos/cv162_10424.txt', 'pos/cv163_10052.txt', 'pos/cv164_22447.txt', 'pos/cv165_22619.txt', 'pos/cv166_11052.txt', 'pos/cv167_16376.txt', 'pos/cv168_7050.txt', 'pos/cv169_23778.txt', 'pos/cv170_3006.txt', 'pos/cv171_13537.txt', 'pos/cv172_11131.txt', 'pos/cv173_4471.txt', 'pos/cv174_9659.txt', 'pos/cv175_6964.txt', 'pos/cv176_12857.txt', 'pos/cv177_10367.txt', 'pos/cv178_12972.txt', 'pos/cv179_9228.txt', 'pos/cv180_16113.txt', 'pos/cv181_14401.txt', 'pos/cv182_7281.txt', 'pos/cv183_18612.txt', 'pos/cv184_2673.txt', 'pos/cv185_28654.txt', 'pos/cv186_2269.txt', 'pos/cv187_12829.txt', 'pos/cv188_19226.txt', 'pos/cv189_22934.txt', 'pos/cv190_27052.txt', 'pos/cv191_29719.txt', 'pos/cv192_14395.txt', 'pos/cv193_5416.txt', 'pos/cv194_12079.txt', 'pos/cv195_14528.txt', 'pos/cv196_29027.txt', 'pos/cv197_29328.txt', 'pos/cv198_18180.txt', 'pos/cv199_9629.txt', 'pos/cv200_2915.txt', 'pos/cv201_6997.txt', 'pos/cv202_10654.txt', 'pos/cv203_17986.txt', 'pos/cv204_8451.txt', 'pos/cv205_9457.txt', 'pos/cv206_14293.txt', 'pos/cv207_29284.txt', 'pos/cv208_9020.txt', 'pos/cv209_29118.txt', 'pos/cv210_9312.txt', 'pos/cv211_9953.txt', 'pos/cv212_10027.txt', 'pos/cv213_18934.txt', 'pos/cv214_12294.txt', 'pos/cv215_22240.txt', 'pos/cv216_18738.txt', 'pos/cv217_28842.txt', 'pos/cv218_24352.txt', 'pos/cv219_18626.txt', 'pos/cv220_29059.txt', 'pos/cv221_2695.txt', 'pos/cv222_17395.txt', 'pos/cv223_29066.txt', 'pos/cv224_17661.txt', 'pos/cv225_29224.txt', 'pos/cv226_2618.txt', 'pos/cv227_24215.txt', 'pos/cv228_5806.txt', 'pos/cv229_13611.txt', 'pos/cv230_7428.txt', 'pos/cv231_10425.txt', 'pos/cv232_14991.txt', 'pos/cv233_15964.txt', 'pos/cv234_20643.txt', 'pos/cv235_10217.txt', 'pos/cv236_11565.txt', 'pos/cv237_19221.txt', 'pos/cv238_12931.txt', 'pos/cv239_3385.txt', 'pos/cv240_14336.txt', 'pos/cv241_23130.txt', 'pos/cv242_10638.txt', 'pos/cv243_20728.txt', 'pos/cv244_21649.txt', 'pos/cv245_8569.txt', 'pos/cv246_28807.txt', 'pos/cv247_13142.txt', 'pos/cv248_13987.txt', 'pos/cv249_11640.txt', 'pos/cv250_25616.txt', 'pos/cv251_22636.txt', 'pos/cv252_23779.txt', 'pos/cv253_10077.txt', 'pos/cv254_6027.txt', 'pos/cv255_13683.txt', 'pos/cv256_14740.txt', 'pos/cv257_10975.txt', 'pos/cv258_5792.txt', 'pos/cv259_10934.txt', 'pos/cv260_13959.txt', 'pos/cv261_10954.txt', 'pos/cv262_12649.txt', 'pos/cv263_19259.txt', 'pos/cv264_12801.txt', 'pos/cv265_10814.txt', 'pos/cv266_25779.txt', 'pos/cv267_14952.txt', 'pos/cv268_18834.txt', 'pos/cv269_21732.txt', 'pos/cv270_6079.txt', 'pos/cv271_13837.txt', 'pos/cv272_18974.txt', 'pos/cv273_29112.txt', 'pos/cv274_25253.txt', 'pos/cv275_28887.txt', 'pos/cv276_15684.txt', 'pos/cv277_19091.txt', 'pos/cv278_13041.txt', 'pos/cv279_18329.txt', 'pos/cv280_8267.txt', 'pos/cv281_23253.txt', 'pos/cv282_6653.txt', 'pos/cv283_11055.txt', 'pos/cv284_19119.txt', 'pos/cv285_16494.txt', 'pos/cv286_25050.txt', 'pos/cv287_15900.txt', 'pos/cv288_18791.txt', 'pos/cv289_6463.txt', 'pos/cv290_11084.txt', 'pos/cv291_26635.txt', 'pos/cv292_7282.txt', 'pos/cv293_29856.txt', 'pos/cv294_11684.txt', 'pos/cv295_15570.txt', 'pos/cv296_12251.txt', 'pos/cv297_10047.txt', 'pos/cv298_23111.txt', 'pos/cv299_16214.txt', 'pos/cv300_22284.txt', 'pos/cv301_12146.txt', 'pos/cv302_25649.txt', 'pos/cv303_27520.txt', 'pos/cv304_28706.txt', 'pos/cv305_9946.txt', 'pos/cv306_10364.txt', 'pos/cv307_25270.txt', 'pos/cv308_5016.txt', 'pos/cv309_22571.txt', 'pos/cv310_13091.txt', 'pos/cv311_16002.txt', 'pos/cv312_29377.txt', 'pos/cv313_18198.txt', 'pos/cv314_14422.txt', 'pos/cv315_11629.txt', 'pos/cv316_6370.txt', 'pos/cv317_24049.txt', 'pos/cv318_10493.txt', 'pos/cv319_14727.txt', 'pos/cv320_9530.txt', 'pos/cv321_12843.txt', 'pos/cv322_20318.txt', 'pos/cv323_29805.txt', 'pos/cv324_7082.txt', 'pos/cv325_16629.txt', 'pos/cv326_13295.txt', 'pos/cv327_20292.txt', 'pos/cv328_10373.txt', 'pos/cv329_29370.txt', 'pos/cv330_29809.txt', 'pos/cv331_8273.txt', 'pos/cv332_16307.txt', 'pos/cv333_8916.txt', 'pos/cv334_10001.txt', 'pos/cv335_14665.txt', 'pos/cv336_10143.txt', 'pos/cv337_29181.txt', 'pos/cv338_8821.txt', 'pos/cv339_21119.txt', 'pos/cv340_13287.txt', 'pos/cv341_24430.txt', 'pos/cv342_19456.txt', 'pos/cv343_10368.txt', 'pos/cv344_5312.txt', 'pos/cv345_9954.txt', 'pos/cv346_18168.txt', 'pos/cv347_13194.txt', 'pos/cv348_18176.txt', 'pos/cv349_13507.txt', 'pos/cv350_20670.txt', 'pos/cv351_15458.txt', 'pos/cv352_5524.txt', 'pos/cv353_18159.txt', 'pos/cv354_8132.txt', 'pos/cv355_16413.txt', 'pos/cv356_25163.txt', 'pos/cv357_13156.txt', 'pos/cv358_10691.txt', 'pos/cv359_6647.txt', 'pos/cv360_8398.txt', 'pos/cv361_28944.txt', 'pos/cv362_15341.txt', 'pos/cv363_29332.txt', 'pos/cv364_12901.txt', 'pos/cv365_11576.txt', 'pos/cv366_10221.txt', 'pos/cv367_22792.txt', 'pos/cv368_10466.txt', 'pos/cv369_12886.txt', 'pos/cv370_5221.txt', 'pos/cv371_7630.txt', 'pos/cv372_6552.txt', 'pos/cv373_20404.txt', 'pos/cv374_25436.txt', 'pos/cv375_9929.txt', 'pos/cv376_19435.txt', 'pos/cv377_7946.txt', 'pos/cv378_20629.txt', 'pos/cv379_21963.txt', 'pos/cv380_7574.txt', 'pos/cv381_20172.txt', 'pos/cv382_7897.txt', 'pos/cv383_13116.txt', 'pos/cv384_17140.txt', 'pos/cv385_29741.txt', 'pos/cv386_10080.txt', 'pos/cv387_11507.txt', 'pos/cv388_12009.txt', 'pos/cv389_9369.txt', 'pos/cv390_11345.txt', 'pos/cv391_10802.txt', 'pos/cv392_11458.txt', 'pos/cv393_29327.txt', 'pos/cv394_5137.txt', 'pos/cv395_10849.txt', 'pos/cv396_17989.txt', 'pos/cv397_29023.txt', 'pos/cv398_15537.txt', 'pos/cv399_2877.txt', 'pos/cv400_19220.txt', 'pos/cv401_12605.txt', 'pos/cv402_14425.txt', 'pos/cv403_6621.txt', 'pos/cv404_20315.txt', 'pos/cv405_20399.txt', 'pos/cv406_21020.txt', 'pos/cv407_22637.txt', 'pos/cv408_5297.txt', 'pos/cv409_29786.txt', 'pos/cv410_24266.txt', 'pos/cv411_15007.txt', 'pos/cv412_24095.txt', 'pos/cv413_7398.txt', 'pos/cv414_10518.txt', 'pos/cv415_22517.txt', 'pos/cv416_11136.txt', 'pos/cv417_13115.txt', 'pos/cv418_14774.txt', 'pos/cv419_13394.txt', 'pos/cv420_28795.txt', 'pos/cv421_9709.txt', 'pos/cv422_9381.txt', 'pos/cv423_11155.txt', 'pos/cv424_8831.txt', 'pos/cv425_8250.txt', 'pos/cv426_10421.txt', 'pos/cv427_10825.txt', 'pos/cv428_11347.txt', 'pos/cv429_7439.txt', 'pos/cv430_17351.txt', 'pos/cv431_7085.txt', 'pos/cv432_14224.txt', 'pos/cv433_10144.txt', 'pos/cv434_5793.txt', 'pos/cv435_23110.txt', 'pos/cv436_19179.txt', 'pos/cv437_22849.txt', 'pos/cv438_8043.txt', 'pos/cv439_15970.txt', 'pos/cv440_15243.txt', 'pos/cv441_13711.txt', 'pos/cv442_13846.txt', 'pos/cv443_21118.txt', 'pos/cv444_9974.txt', 'pos/cv445_25882.txt', 'pos/cv446_11353.txt', 'pos/cv447_27332.txt', 'pos/cv448_14695.txt', 'pos/cv449_8785.txt', 'pos/cv450_7890.txt', 'pos/cv451_10690.txt', 'pos/cv452_5088.txt', 'pos/cv453_10379.txt', 'pos/cv454_2053.txt', 'pos/cv455_29000.txt', 'pos/cv456_18985.txt', 'pos/cv457_18453.txt', 'pos/cv458_8604.txt', 'pos/cv459_20319.txt', 'pos/cv460_10842.txt', 'pos/cv461_19600.txt', 'pos/cv462_19350.txt', 'pos/cv463_10343.txt', 'pos/cv464_15650.txt', 'pos/cv465_22431.txt', 'pos/cv466_18722.txt', 'pos/cv467_25773.txt', 'pos/cv468_15228.txt', 'pos/cv469_20630.txt', 'pos/cv470_15952.txt', 'pos/cv471_16858.txt', 'pos/cv472_29280.txt', 'pos/cv473_7367.txt', 'pos/cv474_10209.txt', 'pos/cv475_21692.txt', 'pos/cv476_16856.txt', 'pos/cv477_22479.txt', 'pos/cv478_14309.txt', 'pos/cv479_5649.txt', 'pos/cv480_19817.txt', 'pos/cv481_7436.txt', 'pos/cv482_10580.txt', 'pos/cv483_16378.txt', 'pos/cv484_25054.txt', 'pos/cv485_26649.txt', 'pos/cv486_9799.txt', 'pos/cv487_10446.txt', 'pos/cv488_19856.txt', 'pos/cv489_17906.txt', 'pos/cv490_17872.txt', 'pos/cv491_12145.txt', 'pos/cv492_18271.txt', 'pos/cv493_12839.txt', 'pos/cv494_17389.txt', 'pos/cv495_14518.txt', 'pos/cv496_10530.txt', 'pos/cv497_26980.txt', 'pos/cv498_8832.txt', 'pos/cv499_10658.txt', 'pos/cv500_10251.txt', 'pos/cv501_11657.txt', 'pos/cv502_10406.txt', 'pos/cv503_10558.txt', 'pos/cv504_29243.txt', 'pos/cv505_12090.txt', 'pos/cv506_15956.txt', 'pos/cv507_9220.txt', 'pos/cv508_16006.txt', 'pos/cv509_15888.txt', 'pos/cv510_23360.txt', 'pos/cv511_10132.txt', 'pos/cv512_15965.txt', 'pos/cv513_6923.txt', 'pos/cv514_11187.txt', 'pos/cv515_17069.txt', 'pos/cv516_11172.txt', 'pos/cv517_19219.txt', 'pos/cv518_13331.txt', 'pos/cv519_14661.txt', 'pos/cv520_12295.txt', 'pos/cv521_15828.txt', 'pos/cv522_5583.txt', 'pos/cv523_16615.txt', 'pos/cv524_23627.txt', 'pos/cv525_16122.txt', 'pos/cv526_12083.txt', 'pos/cv527_10123.txt', 'pos/cv528_10822.txt', 'pos/cv529_10420.txt', 'pos/cv530_16212.txt', 'pos/cv531_26486.txt', 'pos/cv532_6522.txt', 'pos/cv533_9821.txt', 'pos/cv534_14083.txt', 'pos/cv535_19728.txt', 'pos/cv536_27134.txt', 'pos/cv537_12370.txt', 'pos/cv538_28667.txt', 'pos/cv539_20347.txt', 'pos/cv540_3421.txt', 'pos/cv541_28835.txt', 'pos/cv542_18980.txt', 'pos/cv543_5045.txt', 'pos/cv544_5108.txt', 'pos/cv545_12014.txt', 'pos/cv546_11767.txt', 'pos/cv547_16324.txt', 'pos/cv548_17731.txt', 'pos/cv549_21443.txt', 'pos/cv550_22211.txt', 'pos/cv551_10565.txt', 'pos/cv552_10016.txt', 'pos/cv553_26915.txt', 'pos/cv554_13151.txt', 'pos/cv555_23922.txt', 'pos/cv556_14808.txt', 'pos/cv557_11449.txt', 'pos/cv558_29507.txt', 'pos/cv559_0050.txt', 'pos/cv560_17175.txt', 'pos/cv561_9201.txt', 'pos/cv562_10359.txt', 'pos/cv563_17257.txt', 'pos/cv564_11110.txt', 'pos/cv565_29572.txt', 'pos/cv566_8581.txt', 'pos/cv567_29611.txt', 'pos/cv568_15638.txt', 'pos/cv569_26381.txt', 'pos/cv570_29082.txt', 'pos/cv571_29366.txt', 'pos/cv572_18657.txt', 'pos/cv573_29525.txt', 'pos/cv574_22156.txt', 'pos/cv575_21150.txt', 'pos/cv576_14094.txt', 'pos/cv577_28549.txt', 'pos/cv578_15094.txt', 'pos/cv579_11605.txt', 'pos/cv580_14064.txt', 'pos/cv581_19381.txt', 'pos/cv582_6559.txt', 'pos/cv583_29692.txt', 'pos/cv584_29722.txt', 'pos/cv585_22496.txt', 'pos/cv586_7543.txt', 'pos/cv587_19162.txt', 'pos/cv588_13008.txt', 'pos/cv589_12064.txt', 'pos/cv590_19290.txt', 'pos/cv591_23640.txt', 'pos/cv592_22315.txt', 'pos/cv593_10987.txt', 'pos/cv594_11039.txt', 'pos/cv595_25335.txt', 'pos/cv596_28311.txt', 'pos/cv597_26360.txt', 'pos/cv598_16452.txt', 'pos/cv599_20988.txt', 'pos/cv600_23878.txt', 'pos/cv601_23453.txt', 'pos/cv602_8300.txt', 'pos/cv603_17694.txt', 'pos/cv604_2230.txt', 'pos/cv605_11800.txt', 'pos/cv606_15985.txt', 'pos/cv607_7717.txt', 'pos/cv608_23231.txt', 'pos/cv609_23877.txt', 'pos/cv610_2287.txt', 'pos/cv611_21120.txt', 'pos/cv612_5461.txt', 'pos/cv613_21796.txt', 'pos/cv614_10626.txt', 'pos/cv615_14182.txt', 'pos/cv616_29319.txt', 'pos/cv617_9322.txt', 'pos/cv618_8974.txt', 'pos/cv619_12462.txt', 'pos/cv620_24265.txt', 'pos/cv621_14368.txt', 'pos/cv622_8147.txt', 'pos/cv623_15356.txt', 'pos/cv624_10744.txt', 'pos/cv625_12440.txt', 'pos/cv626_7410.txt', 'pos/cv627_11620.txt', 'pos/cv628_19325.txt', 'pos/cv629_14909.txt', 'pos/cv630_10057.txt', 'pos/cv631_4967.txt', 'pos/cv632_9610.txt', 'pos/cv633_29837.txt', 'pos/cv634_11101.txt', 'pos/cv635_10022.txt', 'pos/cv636_15279.txt', 'pos/cv637_1250.txt', 'pos/cv638_2953.txt', 'pos/cv639_10308.txt', 'pos/cv640_5378.txt', 'pos/cv641_12349.txt', 'pos/cv642_29867.txt', 'pos/cv643_29349.txt', 'pos/cv644_17154.txt', 'pos/cv645_15668.txt', 'pos/cv646_15065.txt', 'pos/cv647_13691.txt', 'pos/cv648_15792.txt', 'pos/cv649_12735.txt', 'pos/cv650_14340.txt', 'pos/cv651_10492.txt', 'pos/cv652_13972.txt', 'pos/cv653_19583.txt', 'pos/cv654_18246.txt', 'pos/cv655_11154.txt', 'pos/cv656_24201.txt', 'pos/cv657_24513.txt', 'pos/cv658_10532.txt', 'pos/cv659_19944.txt', 'pos/cv660_21893.txt', 'pos/cv661_2450.txt', 'pos/cv662_13320.txt', 'pos/cv663_13019.txt', 'pos/cv664_4389.txt', 'pos/cv665_29538.txt', 'pos/cv666_18963.txt', 'pos/cv667_18467.txt', 'pos/cv668_17604.txt', 'pos/cv669_22995.txt', 'pos/cv670_25826.txt', 'pos/cv671_5054.txt', 'pos/cv672_28083.txt', 'pos/cv673_24714.txt', 'pos/cv674_10732.txt', 'pos/cv675_21588.txt', 'pos/cv676_21090.txt', 'pos/cv677_17715.txt', 'pos/cv678_13419.txt', 'pos/cv679_28559.txt', 'pos/cv680_10160.txt', 'pos/cv681_9692.txt', 'pos/cv682_16139.txt', 'pos/cv683_12167.txt', 'pos/cv684_11798.txt', 'pos/cv685_5947.txt', 'pos/cv686_13900.txt', 'pos/cv687_21100.txt', 'pos/cv688_7368.txt', 'pos/cv689_12587.txt', 'pos/cv690_5619.txt', 'pos/cv691_5043.txt', 'pos/cv692_15451.txt', 'pos/cv693_18063.txt', 'pos/cv694_4876.txt', 'pos/cv695_21108.txt', 'pos/cv696_29740.txt', 'pos/cv697_11162.txt', 'pos/cv698_15253.txt', 'pos/cv699_7223.txt', 'pos/cv700_21947.txt', 'pos/cv701_14252.txt', 'pos/cv702_11500.txt', 'pos/cv703_16143.txt', 'pos/cv704_15969.txt', 'pos/cv705_11059.txt', 'pos/cv706_24716.txt', 'pos/cv707_10678.txt', 'pos/cv708_28729.txt', 'pos/cv709_10529.txt', 'pos/cv710_22577.txt', 'pos/cv711_11665.txt', 'pos/cv712_22920.txt', 'pos/cv713_29155.txt', 'pos/cv714_18502.txt', 'pos/cv715_18179.txt', 'pos/cv716_10514.txt', 'pos/cv717_15953.txt', 'pos/cv718_11434.txt', 'pos/cv719_5713.txt', 'pos/cv720_5389.txt', 'pos/cv721_29121.txt', 'pos/cv722_7110.txt', 'pos/cv723_8648.txt', 'pos/cv724_13681.txt', 'pos/cv725_10103.txt', 'pos/cv726_4719.txt', 'pos/cv727_4978.txt', 'pos/cv728_16133.txt', 'pos/cv729_10154.txt', 'pos/cv730_10279.txt', 'pos/cv731_4136.txt', 'pos/cv732_12245.txt', 'pos/cv733_9839.txt', 'pos/cv734_21568.txt', 'pos/cv735_18801.txt', 'pos/cv736_23670.txt', 'pos/cv737_28907.txt', 'pos/cv738_10116.txt', 'pos/cv739_11209.txt', 'pos/cv740_12445.txt', 'pos/cv741_11890.txt', 'pos/cv742_7751.txt', 'pos/cv743_15449.txt', 'pos/cv744_10038.txt', 'pos/cv745_12773.txt', 'pos/cv746_10147.txt', 'pos/cv747_16556.txt', 'pos/cv748_12786.txt', 'pos/cv749_17765.txt', 'pos/cv750_10180.txt', 'pos/cv751_15719.txt', 'pos/cv752_24155.txt', 'pos/cv753_10875.txt', 'pos/cv754_7216.txt', 'pos/cv755_23616.txt', 'pos/cv756_22540.txt', 'pos/cv757_10189.txt', 'pos/cv758_9671.txt', 'pos/cv759_13522.txt', 'pos/cv760_8597.txt', 'pos/cv761_12620.txt', 'pos/cv762_13927.txt', 'pos/cv763_14729.txt', 'pos/cv764_11739.txt', 'pos/cv765_19037.txt', 'pos/cv766_7540.txt', 'pos/cv767_14062.txt', 'pos/cv768_11751.txt', 'pos/cv769_8123.txt', 'pos/cv770_10451.txt', 'pos/cv771_28665.txt', 'pos/cv772_12119.txt', 'pos/cv773_18817.txt', 'pos/cv774_13845.txt', 'pos/cv775_16237.txt', 'pos/cv776_20529.txt', 'pos/cv777_10094.txt', 'pos/cv778_17330.txt', 'pos/cv779_17881.txt', 'pos/cv780_7984.txt', 'pos/cv781_5262.txt', 'pos/cv782_19526.txt', 'pos/cv783_13227.txt', 'pos/cv784_14394.txt', 'pos/cv785_22600.txt', 'pos/cv786_22497.txt', 'pos/cv787_13743.txt', 'pos/cv788_25272.txt', 'pos/cv789_12136.txt', 'pos/cv790_14600.txt', 'pos/cv791_16302.txt', 'pos/cv792_3832.txt', 'pos/cv793_13650.txt', 'pos/cv794_15868.txt', 'pos/cv795_10122.txt', 'pos/cv796_15782.txt', 'pos/cv797_6957.txt', 'pos/cv798_23531.txt', 'pos/cv799_18543.txt', 'pos/cv800_12368.txt', 'pos/cv801_25228.txt', 'pos/cv802_28664.txt', 'pos/cv803_8207.txt', 'pos/cv804_10862.txt', 'pos/cv805_19601.txt', 'pos/cv806_8842.txt', 'pos/cv807_21740.txt', 'pos/cv808_12635.txt', 'pos/cv809_5009.txt', 'pos/cv810_12458.txt', 'pos/cv811_21386.txt', 'pos/cv812_17924.txt', 'pos/cv813_6534.txt', 'pos/cv814_18975.txt', 'pos/cv815_22456.txt', 'pos/cv816_13655.txt', 'pos/cv817_4041.txt', 'pos/cv818_10211.txt', 'pos/cv819_9364.txt', 'pos/cv820_22892.txt', 'pos/cv821_29364.txt', 'pos/cv822_20049.txt', 'pos/cv823_15569.txt', 'pos/cv824_8838.txt', 'pos/cv825_5063.txt', 'pos/cv826_11834.txt', 'pos/cv827_18331.txt', 'pos/cv828_19831.txt', 'pos/cv829_20289.txt', 'pos/cv830_6014.txt', 'pos/cv831_14689.txt', 'pos/cv832_23275.txt', 'pos/cv833_11053.txt', 'pos/cv834_22195.txt', 'pos/cv835_19159.txt', 'pos/cv836_12968.txt', 'pos/cv837_27325.txt', 'pos/cv838_24728.txt', 'pos/cv839_21467.txt', 'pos/cv840_16321.txt', 'pos/cv841_3967.txt', 'pos/cv842_5866.txt', 'pos/cv843_15544.txt', 'pos/cv844_12690.txt', 'pos/cv845_14290.txt', 'pos/cv846_29497.txt', 'pos/cv847_1941.txt', 'pos/cv848_10036.txt', 'pos/cv849_15729.txt', 'pos/cv850_16466.txt', 'pos/cv851_20469.txt', 'pos/cv852_27523.txt', 'pos/cv853_29233.txt', 'pos/cv854_17740.txt', 'pos/cv855_20661.txt', 'pos/cv856_29013.txt', 'pos/cv857_15958.txt', 'pos/cv858_18819.txt', 'pos/cv859_14107.txt', 'pos/cv860_13853.txt', 'pos/cv861_1198.txt', 'pos/cv862_14324.txt', 'pos/cv863_7424.txt', 'pos/cv864_3416.txt', 'pos/cv865_2895.txt', 'pos/cv866_29691.txt', 'pos/cv867_16661.txt', 'pos/cv868_11948.txt', 'pos/cv869_23611.txt', 'pos/cv870_16348.txt', 'pos/cv871_24888.txt', 'pos/cv872_12591.txt', 'pos/cv873_18636.txt', 'pos/cv874_11236.txt', 'pos/cv875_5754.txt', 'pos/cv876_9390.txt', 'pos/cv877_29274.txt', 'pos/cv878_15694.txt', 'pos/cv879_14903.txt', 'pos/cv880_29800.txt', 'pos/cv881_13254.txt', 'pos/cv882_10026.txt', 'pos/cv883_27751.txt', 'pos/cv884_13632.txt', 'pos/cv885_12318.txt', 'pos/cv886_18177.txt', 'pos/cv887_5126.txt', 'pos/cv888_24435.txt', 'pos/cv889_21430.txt', 'pos/cv890_3977.txt', 'pos/cv891_6385.txt', 'pos/cv892_17576.txt', 'pos/cv893_26269.txt', 'pos/cv894_2068.txt', 'pos/cv895_21022.txt', 'pos/cv896_16071.txt', 'pos/cv897_10837.txt', 'pos/cv898_14187.txt', 'pos/cv899_16014.txt', 'pos/cv900_10331.txt', 'pos/cv901_11017.txt', 'pos/cv902_12256.txt', 'pos/cv903_17822.txt', 'pos/cv904_24353.txt', 'pos/cv905_29114.txt', 'pos/cv906_11491.txt', 'pos/cv907_3541.txt', 'pos/cv908_16009.txt', 'pos/cv909_9960.txt', 'pos/cv910_20488.txt', 'pos/cv911_20260.txt', 'pos/cv912_5674.txt', 'pos/cv913_29252.txt', 'pos/cv914_28742.txt', 'pos/cv915_8841.txt', 'pos/cv916_15467.txt', 'pos/cv917_29715.txt', 'pos/cv918_2693.txt', 'pos/cv919_16380.txt', 'pos/cv920_29622.txt', 'pos/cv921_12747.txt', 'pos/cv922_10073.txt', 'pos/cv923_11051.txt', 'pos/cv924_29540.txt', 'pos/cv925_8969.txt', 'pos/cv926_17059.txt', 'pos/cv927_10681.txt', 'pos/cv928_9168.txt', 'pos/cv929_16908.txt', 'pos/cv930_13475.txt', 'pos/cv931_17563.txt', 'pos/cv932_13401.txt', 'pos/cv933_23776.txt', 'pos/cv934_19027.txt', 'pos/cv935_23841.txt', 'pos/cv936_15954.txt', 'pos/cv937_9811.txt', 'pos/cv938_10220.txt', 'pos/cv939_10583.txt', 'pos/cv940_17705.txt', 'pos/cv941_10246.txt', 'pos/cv942_17082.txt', 'pos/cv943_22488.txt', 'pos/cv944_13521.txt', 'pos/cv945_12160.txt', 'pos/cv946_18658.txt', 'pos/cv947_10601.txt', 'pos/cv948_24606.txt', 'pos/cv949_20112.txt', 'pos/cv950_12350.txt', 'pos/cv951_10926.txt', 'pos/cv952_25240.txt', 'pos/cv953_6836.txt', 'pos/cv954_18628.txt', 'pos/cv955_25001.txt', 'pos/cv956_11609.txt', 'pos/cv957_8737.txt', 'pos/cv958_12162.txt', 'pos/cv959_14611.txt', 'pos/cv960_29007.txt', 'pos/cv961_5682.txt', 'pos/cv962_9803.txt', 'pos/cv963_6895.txt', 'pos/cv964_6021.txt', 'pos/cv965_26071.txt', 'pos/cv966_28832.txt', 'pos/cv967_5788.txt', 'pos/cv968_24218.txt', 'pos/cv969_13250.txt', 'pos/cv970_18450.txt', 'pos/cv971_10874.txt', 'pos/cv972_26417.txt', 'pos/cv973_10066.txt', 'pos/cv974_22941.txt', 'pos/cv975_10981.txt', 'pos/cv976_10267.txt', 'pos/cv977_4938.txt', 'pos/cv978_20929.txt', 'pos/cv979_18921.txt', 'pos/cv980_10953.txt', 'pos/cv981_14989.txt', 'pos/cv982_21103.txt', 'pos/cv983_22928.txt', 'pos/cv984_12767.txt', 'pos/cv985_6359.txt', 'pos/cv986_13527.txt', 'pos/cv987_6965.txt', 'pos/cv988_18740.txt', 'pos/cv989_15824.txt', 'pos/cv990_11591.txt', 'pos/cv991_18645.txt', 'pos/cv992_11962.txt', 'pos/cv993_29737.txt', 'pos/cv994_12270.txt', 'pos/cv995_21821.txt', 'pos/cv996_11592.txt', 'pos/cv997_5046.txt', 'pos/cv998_14111.txt', 'pos/cv999_13106.txt']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    "print(movie_reviews.categories())\n",
    "print(movie_reviews.fileids('pos'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"02\"></a>\n",
    "<H1 style=\"color: MediumBlue; font-size: 34px; background-color: #E6E6FA; padding: 30px 20px;\">2. Extracting, cleaning and pre-processing text</H1>\n",
    "\n",
    "#### Table of contents\n",
    "\n",
    "* [Tokenization](#02-01)   \n",
    "    Words. Sentences. Paragraphs. With regular expressions. Frequency Distributon.\n",
    "* [Stemming](#02-02)\n",
    "* [Lemmatization](#02-03)\n",
    "* [Bigrams, trigrams, n-grams](#02-04)\n",
    "* [Stopwords](#02-05)\n",
    "* [Punctuation](#02-06)\n",
    "* [POS Tagging](#02-07)\n",
    "* [Named Entity Recognition](#02-08)\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"02-01\"></a>\n",
    "<H2 style=\"color: MediumBlue; font-size: 21px; background-color: #F0F8FF; padding: 15px 20px;\">Tokenization – Words</H2>\n",
    "\n",
    "[nltk.tokenize package](https://www.nltk.org/api/nltk.stem.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['here', 'is', 'a', 'sample', 'text', 'to', 'tokenize', '.']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "data = \"here is a sample text to tokenize.\"\n",
    "tokens = word_tokenize(data)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2 style=\"color: MediumBlue; font-size: 21px; background-color: #F0F8FF; padding: 15px 20px;\">Tokenization – Sentences</H2>\n",
    "\n",
    "[nltk.tokenize package](https://www.nltk.org/api/nltk.stem.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The Sun rises in the East.', 'The Sun sets in the West.']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "\n",
    "data = 'The Sun rises in the East. The Sun sets in the West.'\n",
    "sentences = sent_tokenize(data)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2 style=\"color: MediumBlue; font-size: 21px; background-color: #F0F8FF; padding: 15px 20px;\">Tokenization – Paragraphs</H2>\n",
    "\n",
    "[nltk.tokenize package](https://www.nltk.org/api/nltk.stem.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Paragraph 1.', 'Paragraph 2.']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import blankline_tokenize\n",
    "\n",
    "data = \"\"\"Paragraph 1.\n",
    "\n",
    "Paragraph 2.\"\"\"\n",
    "paragraphs = blankline_tokenize(data)\n",
    "paragraphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2 style=\"color: MediumBlue; font-size: 21px; background-color: #F0F8FF; padding: 15px 20px;\">Tokenization with Regular Expressions</H2>\n",
    "\n",
    "[nltk.tokenize package](https://www.nltk.org/api/nltk.stem.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', '5', '7', '10', '21']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import regexp_tokenize\n",
    "\n",
    "data = \"here is a sample text to tokenize with digits 1, 5, 7 and numbers 10, 21.\"\n",
    "regexp_tokenize(data, pattern='\\d+')\n",
    "# List all tokens that match the pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2 style=\"color: MediumBlue; font-size: 21px; background-color: #F0F8FF; padding: 15px 20px;\">Frequency Distribution</H2>\n",
    "\n",
    "[nltk.probability module](https://www.nltk.org/api/nltk.probability.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FreqDist({'here': 1, 'is': 1, 'a': 1, 'sample': 1, 'text': 1, 'to': 1, 'tokenize': 1, '.': 1})"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "data = \"here is a sample text to tokenize.\"\n",
    "tokens = word_tokenize(data)\n",
    "\n",
    "fdist = FreqDist()              # a Dict {'word': frequency}\n",
    "for token in tokens:\n",
    "    fdist[token.lower()]+=1\n",
    "    \n",
    "print(len(fdist))\n",
    "fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('here', 1),\n",
       " ('is', 1),\n",
       " ('a', 1),\n",
       " ('sample', 1),\n",
       " ('text', 1),\n",
       " ('to', 1),\n",
       " ('tokenize', 1),\n",
       " ('.', 1)]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top10 = fdist.most_common(10)\n",
    "top10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"02-02\"></a>\n",
    "<H2 style=\"color: MediumBlue; font-size: 21px; background-color: #F0F8FF; padding: 15px 20px;\">Stemming</H2>\n",
    "\n",
    "[nltk.stem package](https://www.nltk.org/api/nltk.stem.html)\n",
    "\n",
    "*study*, *studies*, *studying* -> *studi* (not an English word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer  # also LancasterStemmer, SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give\n",
      "give\n",
      "given\n",
      "gave\n"
     ]
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "\n",
    "print(stemmer.stem('give'))\n",
    "print(stemmer.stem('giving'))\n",
    "print(stemmer.stem('given'))\n",
    "print(stemmer.stem('gave'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "giv\n",
      "giv\n",
      "giv\n",
      "gav\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "print(stemmer.stem('give'))\n",
    "print(stemmer.stem('giving'))\n",
    "print(stemmer.stem('given'))\n",
    "print(stemmer.stem('gave'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('arabic', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'hungarian', 'italian', 'norwegian', 'porter', 'portuguese', 'romanian', 'russian', 'spanish', 'swedish')\n",
      "give\n",
      "give\n",
      "given\n",
      "gave\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "print(stemmer.languages)\n",
    "print(stemmer.stem('give'))\n",
    "print(stemmer.stem('giving'))\n",
    "print(stemmer.stem('given'))\n",
    "print(stemmer.stem('gave'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word  The             >  Stem  the            \n",
      "Word  Sun             >  Stem  sun            \n",
      "Word  rises           >  Stem  rise           \n",
      "Word  in              >  Stem  in             \n",
      "Word  the             >  Stem  the            \n",
      "Word  East            >  Stem  east           \n",
      "Word  .               >  Stem  .              \n",
      "Word  The             >  Stem  the            \n",
      "Word  Sun             >  Stem  sun            \n",
      "Word  sets            >  Stem  set            \n",
      "Word  in              >  Stem  in             \n",
      "Word  the             >  Stem  the            \n",
      "Word  West            >  Stem  west           \n",
      "Word  .               >  Stem  .              \n"
     ]
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "data = \"The Sun rises in the East. The Sun sets in the West.\"\n",
    "words = word_tokenize(data)\n",
    "\n",
    "for word in words:\n",
    "    print(f\"Word  {word:15s} >  Stem  {stemmer.stem(word):15s}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"02-03\"></a>\n",
    "<H2 style=\"color: MediumBlue; font-size: 21px; background-color: #F0F8FF; padding: 15px 20px;\">Lemmatization</H2>\n",
    "\n",
    "[nltk.stem.wordnet](https://www.nltk.org/api/nltk.stem.wordnet.html)\n",
    "\n",
    "*study*, *studies*, *studying* -> *study*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog\n",
      "go\n",
      "go\n",
      "go\n",
      "go\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize('dogs'))       # default: 'n' (= assumes all words as Nouns)\n",
    "print(lemmatizer.lemmatize('go', 'v'))\n",
    "print(lemmatizer.lemmatize('went', 'v'))\n",
    "print(lemmatizer.lemmatize('goes', 'v'))\n",
    "print(lemmatizer.lemmatize('going', wordnet.VERB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word  It              >  Lemma  It             \n",
      "Word  originated      >  Lemma  originated     \n",
      "Word  from            >  Lemma  from           \n",
      "Word  the             >  Lemma  the            \n",
      "Word  idea            >  Lemma  idea           \n",
      "Word  that            >  Lemma  that           \n",
      "Word  there           >  Lemma  there          \n",
      "Word  are             >  Lemma  are            \n",
      "Word  readers         >  Lemma  reader         \n",
      "Word  who             >  Lemma  who            \n",
      "Word  prefer          >  Lemma  prefer         \n",
      "Word  learning        >  Lemma  learning       \n",
      "Word  new             >  Lemma  new            \n",
      "Word  skills          >  Lemma  skill          \n",
      "Word  from            >  Lemma  from           \n",
      "Word  the             >  Lemma  the            \n",
      "Word  comfort         >  Lemma  comfort        \n",
      "Word  of              >  Lemma  of             \n",
      "Word  their           >  Lemma  their          \n",
      "Word  drawing         >  Lemma  drawing        \n",
      "Word  room            >  Lemma  room           \n",
      "Word  .               >  Lemma  .              \n"
     ]
    }
   ],
   "source": [
    "data = \"\"\"It originated from the idea that there are readers who prefer \n",
    "learning new skills from the comfort of their drawing room.\"\"\"\n",
    "words = word_tokenize(data)\n",
    "\n",
    "for word in words:\n",
    "    print(f\"Word  {word:15s} >  Lemma  {lemmatizer.lemmatize(word):15s}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"02-04\"></a>\n",
    "<H2 style=\"color: MediumBlue; font-size: 21px; background-color: #F0F8FF; padding: 15px 20px;\">Bigrams, trigams and n-grams</H2>\n",
    "\n",
    "[nltk.util module](https://www.nltk.org/api/nltk.util.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import bigrams, trigrams, ngrams\n",
    "\n",
    "data = \"\"\"It originated from the idea that there are readers who prefer \n",
    "learning new skills from the comfort of their drawing room.\"\"\"\n",
    "words = word_tokenize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('It', 'originated'),\n",
       " ('originated', 'from'),\n",
       " ('from', 'the'),\n",
       " ('the', 'idea'),\n",
       " ('idea', 'that'),\n",
       " ('that', 'there'),\n",
       " ('there', 'are'),\n",
       " ('are', 'readers'),\n",
       " ('readers', 'who'),\n",
       " ('who', 'prefer'),\n",
       " ('prefer', 'learning'),\n",
       " ('learning', 'new'),\n",
       " ('new', 'skills'),\n",
       " ('skills', 'from'),\n",
       " ('from', 'the'),\n",
       " ('the', 'comfort'),\n",
       " ('comfort', 'of'),\n",
       " ('of', 'their'),\n",
       " ('their', 'drawing'),\n",
       " ('drawing', 'room'),\n",
       " ('room', '.')]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams = list(nltk.bigrams(words))\n",
    "bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('It', 'originated', 'from'),\n",
       " ('originated', 'from', 'the'),\n",
       " ('from', 'the', 'idea'),\n",
       " ('the', 'idea', 'that'),\n",
       " ('idea', 'that', 'there'),\n",
       " ('that', 'there', 'are'),\n",
       " ('there', 'are', 'readers'),\n",
       " ('are', 'readers', 'who'),\n",
       " ('readers', 'who', 'prefer'),\n",
       " ('who', 'prefer', 'learning'),\n",
       " ('prefer', 'learning', 'new'),\n",
       " ('learning', 'new', 'skills'),\n",
       " ('new', 'skills', 'from'),\n",
       " ('skills', 'from', 'the'),\n",
       " ('from', 'the', 'comfort'),\n",
       " ('the', 'comfort', 'of'),\n",
       " ('comfort', 'of', 'their'),\n",
       " ('of', 'their', 'drawing'),\n",
       " ('their', 'drawing', 'room'),\n",
       " ('drawing', 'room', '.')]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigrams = list(nltk.trigrams(words))\n",
    "trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('It', 'originated', 'from', 'the'),\n",
       " ('originated', 'from', 'the', 'idea'),\n",
       " ('from', 'the', 'idea', 'that'),\n",
       " ('the', 'idea', 'that', 'there'),\n",
       " ('idea', 'that', 'there', 'are'),\n",
       " ('that', 'there', 'are', 'readers'),\n",
       " ('there', 'are', 'readers', 'who'),\n",
       " ('are', 'readers', 'who', 'prefer'),\n",
       " ('readers', 'who', 'prefer', 'learning'),\n",
       " ('who', 'prefer', 'learning', 'new'),\n",
       " ('prefer', 'learning', 'new', 'skills'),\n",
       " ('learning', 'new', 'skills', 'from'),\n",
       " ('new', 'skills', 'from', 'the'),\n",
       " ('skills', 'from', 'the', 'comfort'),\n",
       " ('from', 'the', 'comfort', 'of'),\n",
       " ('the', 'comfort', 'of', 'their'),\n",
       " ('comfort', 'of', 'their', 'drawing'),\n",
       " ('of', 'their', 'drawing', 'room'),\n",
       " ('their', 'drawing', 'room', '.')]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngrams = list(nltk.ngrams(words, 4))\n",
    "ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"02-05\"></a>\n",
    "<H2 style=\"color: MediumBlue; font-size: 21px; background-color: #F0F8FF; padding: 15px 20px;\">Stopwords</H2>\n",
    "\n",
    "[nltk.util module](https://www.nltk.org/api/nltk.util.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['au',\n",
       " 'aux',\n",
       " 'avec',\n",
       " 'ce',\n",
       " 'ces',\n",
       " 'dans',\n",
       " 'de',\n",
       " 'des',\n",
       " 'du',\n",
       " 'elle',\n",
       " 'en',\n",
       " 'et',\n",
       " 'eux',\n",
       " 'il',\n",
       " 'ils',\n",
       " 'je',\n",
       " 'la',\n",
       " 'le',\n",
       " 'les',\n",
       " 'leur',\n",
       " 'lui',\n",
       " 'ma',\n",
       " 'mais',\n",
       " 'me',\n",
       " 'même',\n",
       " 'mes',\n",
       " 'moi',\n",
       " 'mon',\n",
       " 'ne',\n",
       " 'nos',\n",
       " 'notre',\n",
       " 'nous',\n",
       " 'on',\n",
       " 'ou',\n",
       " 'par',\n",
       " 'pas',\n",
       " 'pour',\n",
       " 'qu',\n",
       " 'que',\n",
       " 'qui',\n",
       " 'sa',\n",
       " 'se',\n",
       " 'ses',\n",
       " 'son',\n",
       " 'sur',\n",
       " 'ta',\n",
       " 'te',\n",
       " 'tes',\n",
       " 'toi',\n",
       " 'ton',\n",
       " 'tu',\n",
       " 'un',\n",
       " 'une',\n",
       " 'vos',\n",
       " 'votre',\n",
       " 'vous',\n",
       " 'c',\n",
       " 'd',\n",
       " 'j',\n",
       " 'l',\n",
       " 'à',\n",
       " 'm',\n",
       " 'n',\n",
       " 's',\n",
       " 't',\n",
       " 'y',\n",
       " 'été',\n",
       " 'étée',\n",
       " 'étées',\n",
       " 'étés',\n",
       " 'étant',\n",
       " 'étante',\n",
       " 'étants',\n",
       " 'étantes',\n",
       " 'suis',\n",
       " 'es',\n",
       " 'est',\n",
       " 'sommes',\n",
       " 'êtes',\n",
       " 'sont',\n",
       " 'serai',\n",
       " 'seras',\n",
       " 'sera',\n",
       " 'serons',\n",
       " 'serez',\n",
       " 'seront',\n",
       " 'serais',\n",
       " 'serait',\n",
       " 'serions',\n",
       " 'seriez',\n",
       " 'seraient',\n",
       " 'étais',\n",
       " 'était',\n",
       " 'étions',\n",
       " 'étiez',\n",
       " 'étaient',\n",
       " 'fus',\n",
       " 'fut',\n",
       " 'fûmes',\n",
       " 'fûtes',\n",
       " 'furent',\n",
       " 'sois',\n",
       " 'soit',\n",
       " 'soyons',\n",
       " 'soyez',\n",
       " 'soient',\n",
       " 'fusse',\n",
       " 'fusses',\n",
       " 'fût',\n",
       " 'fussions',\n",
       " 'fussiez',\n",
       " 'fussent',\n",
       " 'ayant',\n",
       " 'ayante',\n",
       " 'ayantes',\n",
       " 'ayants',\n",
       " 'eu',\n",
       " 'eue',\n",
       " 'eues',\n",
       " 'eus',\n",
       " 'ai',\n",
       " 'as',\n",
       " 'avons',\n",
       " 'avez',\n",
       " 'ont',\n",
       " 'aurai',\n",
       " 'auras',\n",
       " 'aura',\n",
       " 'aurons',\n",
       " 'aurez',\n",
       " 'auront',\n",
       " 'aurais',\n",
       " 'aurait',\n",
       " 'aurions',\n",
       " 'auriez',\n",
       " 'auraient',\n",
       " 'avais',\n",
       " 'avait',\n",
       " 'avions',\n",
       " 'aviez',\n",
       " 'avaient',\n",
       " 'eut',\n",
       " 'eûmes',\n",
       " 'eûtes',\n",
       " 'eurent',\n",
       " 'aie',\n",
       " 'aies',\n",
       " 'ait',\n",
       " 'ayons',\n",
       " 'ayez',\n",
       " 'aient',\n",
       " 'eusse',\n",
       " 'eusses',\n",
       " 'eût',\n",
       " 'eussions',\n",
       " 'eussiez',\n",
       " 'eussent']"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('french')\n",
    "print(len(stopwords))\n",
    "stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"02-06\"></a>\n",
    "<H2 style=\"color: MediumBlue; font-size: 21px; background-color: #F0F8FF; padding: 15px 20px;\">Punctuation</H2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['It',\n",
       " 'originated',\n",
       " 'from',\n",
       " 'the',\n",
       " 'idea',\n",
       " 'that',\n",
       " 'there',\n",
       " 'are',\n",
       " 'readers',\n",
       " 'who',\n",
       " 'prefer',\n",
       " 'learning',\n",
       " 'new',\n",
       " 'skills',\n",
       " 'from',\n",
       " 'the',\n",
       " 'comfort',\n",
       " 'of',\n",
       " 'their',\n",
       " 'drawing',\n",
       " 'room']"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "data = \"\"\"It originated from the idea that there are readers who prefer \n",
    "learning new skills from the comfort of their drawing room.\"\"\"\n",
    "words = word_tokenize(data)\n",
    "\n",
    "punctuation = re.compile(r'[-_.?!,;:()|0-9]')\n",
    "no_punct = []\n",
    "for word in words:\n",
    "    w = punctuation.sub('', word)\n",
    "    if len(w) > 0:\n",
    "        no_punct.append(w)\n",
    "no_punct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"02-07\"></a>\n",
    "<H2 style=\"color: MediumBlue; font-size: 21px; background-color: #F0F8FF; padding: 15px 20px;\">POS tagging</H2>\n",
    "\n",
    "[Example usage](https://www.nltk.org/book/ch05.html)\n",
    "\n",
    "<img src=\"nltk_pos_1.png\" width=\"750\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n",
      "[('Google', 'NN')]\n",
      "[('something', 'NN')]\n",
      "[('on', 'IN')]\n",
      "[('the', 'DT')]\n",
      "[('Internet', 'NN')]\n",
      "[('.', '.')]\n",
      "-----\n",
      "[('Display', 'NN')]\n",
      "[('the', 'DT')]\n",
      "[('results', 'NNS')]\n",
      "[('here', 'RB')]\n",
      "[('.', '.')]\n",
      "-----\n",
      "[('Mary', 'NNP')]\n",
      "[('is', 'VBZ')]\n",
      "[('driving', 'VBG')]\n",
      "[('a', 'DT')]\n",
      "[('big', 'JJ')]\n",
      "[('car', 'NN')]\n",
      "[('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "data = 'Google something on the Internet. Display the results here. Mary is driving a big car.'\n",
    "sentences = sent_tokenize(data)\n",
    "\n",
    "for sentence in sentences:\n",
    "    print('-----')\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    for word in words:\n",
    "        print(nltk.pos_tag([word]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Mary', 'NNP')]\n",
      "[(' ', 'NN')]\n",
      "[('is', 'VBZ')]\n",
      "[(' ', 'NN')]\n",
      "[('driving', 'VBG')]\n",
      "[(' ', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[(' ', 'NN')]\n",
      "[('big', 'JJ')]\n",
      "[(' ', 'NN')]\n",
      "[('car.', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer('(?u)\\W+|\\$[\\d\\.]+|\\S+')\n",
    "\n",
    "data = 'Mary is driving a big car.'\n",
    "words = tokenizer.tokenize(sentence)\n",
    "for word in words:\n",
    "    print(nltk.pos_tag([word]))\n",
    "    \n",
    "# inserting an empty space between the words -> removes the dependency relation between words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"nltk_pos_2.png\" width=\"750\">\n",
    "<br>\n",
    "<img src=\"nltk_pos_3.png\" width=\"750\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"02-08\"></a>\n",
    "<H2 style=\"color: MediumBlue; font-size: 21px; background-color: #F0F8FF; padding: 15px 20px;\">Named Entity Recognition (NER)</H2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg baseProfile=\"full\" height=\"168px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight:normal; font-style: normal; font-size: 16px;\" version=\"1.1\" viewBox=\"0,0,448.0,168.0\" width=\"448px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">S</text></svg><svg width=\"8.92857%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">The</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">DT</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"4.46429%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"14.2857%\" x=\"8.92857%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">GPE</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">Korean</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"16.0714%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"19.6429%\" x=\"23.2143%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">President</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"33.0357%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"12.5%\" x=\"42.8571%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">stays</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">VBZ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"49.1071%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"7.14286%\" x=\"55.3571%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">in</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"58.9286%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"8.92857%\" x=\"62.5%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">the</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">DT</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"66.9643%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"23.2143%\" x=\"71.4286%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">FACILITY</text></svg><svg width=\"46.1538%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">Blue</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"23.0769%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"53.8462%\" x=\"46.1538%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">House</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"73.0769%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"83.0357%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"5.35714%\" x=\"94.6429%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">.</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">.</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"97.3214%\" y1=\"1.2em\" y2=\"3em\" /></svg>"
      ],
      "text/plain": [
       "Tree('S', [('The', 'DT'), Tree('GPE', [('Korean', 'NNP')]), ('President', 'NNP'), ('stays', 'VBZ'), ('in', 'IN'), ('the', 'DT'), Tree('FACILITY', [('Blue', 'NNP'), ('House', 'NNP')]), ('.', '.')])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import ne_chunk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "data = 'The Korean President stays in the Blue House.'\n",
    "words = word_tokenize(data)\n",
    "tags = nltk.pos_tag(words)\n",
    "ner = ne_chunk(tags)\n",
    "ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg baseProfile=\"full\" height=\"168px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight:normal; font-style: normal; font-size: 16px;\" version=\"1.1\" viewBox=\"0,0,544.0,168.0\" width=\"544px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">S</text></svg><svg width=\"10.2941%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">GPE</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">Apple</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"5.14706%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"7.35294%\" x=\"10.2941%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">is</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">VBZ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"13.9706%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"5.88235%\" x=\"17.6471%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">a</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">DT</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"20.5882%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"10.2941%\" x=\"23.5294%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">fruit</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"28.6765%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"7.35294%\" x=\"33.8235%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">and</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">CC</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"37.5%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"11.7647%\" x=\"41.1765%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">PERSON</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">Apple</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"47.0588%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"7.35294%\" x=\"52.9412%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">is</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">VBZ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"56.6176%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"5.88235%\" x=\"60.2941%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">a</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">DT</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"63.2353%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"13.2353%\" x=\"66.1765%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">company</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"72.7941%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"7.35294%\" x=\"79.4118%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">'s</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">POS</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"83.0882%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"8.82353%\" x=\"86.7647%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">name</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"91.1765%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"4.41176%\" x=\"95.5882%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">.</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">.</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"97.7941%\" y1=\"1.2em\" y2=\"3em\" /></svg>"
      ],
      "text/plain": [
       "Tree('S', [Tree('GPE', [('Apple', 'NNP')]), ('is', 'VBZ'), ('a', 'DT'), ('fruit', 'NN'), ('and', 'CC'), Tree('PERSON', [('Apple', 'NNP')]), ('is', 'VBZ'), ('a', 'DT'), ('company', 'NN'), (\"'s\", 'POS'), ('name', 'NN'), ('.', '.')])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import ne_chunk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "data = \"Apple is a fruit and Apple is a company's name.\"\n",
    "words = word_tokenize(data)\n",
    "tags = nltk.pos_tag(words)\n",
    "ner = ne_chunk(tags)\n",
    "ner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"03\"></a>\n",
    "<H1 style=\"color: MediumBlue; font-size: 34px; background-color: #E6E6FA; padding: 30px 20px;\">3. Analyzing sentence structure</H1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
