{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Latest innovations: speech recognition, machine translation, transformers>embeddings  \n",
    "\n",
    "#### Standard NNs \n",
    "\n",
    "#### Convolutional NNs  \n",
    "\n",
    "#### Recurrent NNs  \n",
    "\n",
    "#### Structured data\n",
    "supervised lrng  \n",
    "\n",
    "#### Unstructured data\n",
    "audio, img  \n",
    "\n",
    "#### Logistic Regression\n",
    "binary classification   \n",
    "\n",
    "#### Linear Regression  \n",
    "\n",
    "#### Deep Learning\n",
    "Learn the parameters that minimize the error  \n",
    "\n",
    "#### Training in Deep Learning  \n",
    "  modifiy the parameters (w, b...) in order to minimize the cost function  \n",
    "\n",
    "#### Parameters:   \n",
    "  - weight (w)  \n",
    "  - bias (b)  \n",
    "  - learning rate (alpha)  \n",
    "    - if alpha too large, we may overshoot the optimal value\n",
    "    - if alpha too small, too many iterations to converge\n",
    "    tuning = plot the curve with various values\n",
    "    **See** ai specialization course 1 'w2-002-logistic-regression-with-nn'\n",
    "\n",
    "#### Loss function: \n",
    "cost for one example  \n",
    "- **L1** loss function  \n",
    "- **L2** loss function\n",
    " \n",
    "#### Cost function: \n",
    "  average loss functions for all examples = `J(w,b)` surface of the curve  \n",
    "\n",
    "#### Gradient Descent algorithm: \n",
    "update w and b  \n",
    "\n",
    "####  Derivative/Slope \n",
    "  (partial derivative = J function of 2+ args): height/width   \n",
    "  for any value of a, if you nudge it by a certain amount, the slope will be ~\n",
    "  Derivatives at each step are used to compute derivatives at following steps  \n",
    "\n",
    "#### Computation Graph: \n",
    "  organizes the computations:  \n",
    "  1. define the neural network structure (# of input units,  # of hidden units, etc.) \n",
    "  2. go forward and compute output with activation  \n",
    "  3. compute cost function  \n",
    "  4. go backward and compute gradients (derivatives) using cost  \n",
    "  5. update parameters (gradient descent)\n",
    "  perform 1 to 4 for I iterations\n",
    "  6. predict\n",
    "  **See** ai specialization course 1 'w2-002-logistic-regression-with-nn'\n",
    "\n",
    "#### Chain rule  \n",
    "  in backpropagation, if we change a, we change, so we change J  \n",
    "\n",
    "#### Backpropagation  \n",
    "[coursera ai: backpropagation](https://www.coursera.org/learn/neural-networks-deep-learning/lecture/6dDj7/backpropagation-intuition-optional).  \n",
    "  compute the derivative of the final output with respect to some other variable usually called Â´dvarÂ´ in code  \n",
    "\n",
    "#### Vectorization  \n",
    "  whenever possible, avoid explicit Â´forÂ´Â loops  \n",
    "\n",
    "#### Broadcasting in Python  \n",
    "  https://numpy.org/doc/stable/user/basics.broadcasting.html  \n",
    "\n",
    "#### Normalization  \n",
    "  divide each row or `x` by its norm `ð‘¥ / â€–ð‘¥â€–`  \n",
    "  increases performance because gradient descent converges faster after normalization  \n",
    "  https://stats.stackexchange.com/questions/211436/why-normalize-images-by-subtracting-datasets-image-mean-instead-of-the-current  \n",
    "\n",
    "#### Softmax   \n",
    "  normalizing function used when algorithm needs to classify two or more classes    \n",
    "\n",
    "#### Standardization\n",
    "  substract the mean of the whole numpy array from each example,     \n",
    "  and then divide each example by the standard deviation of the whole numpy array  \n",
    "  ex: `train_set_x = train_set_x_flatten/255`  \n",
    "\n",
    "#### Hidden\n",
    "not observed, not seen in the training set\n",
    "\n",
    "#### Activation functions\n",
    "[coursera ai: activation functions](https://www.coursera.org/learn/neural-networks-deep-learning/lecture/4dDC1/activation-functions)    \n",
    "[coursera ai: nns-non-linear-activation-functions](https://www.coursera.org/learn/neural-networks-deep-learning/lecture/OASKH/why-do-you-need-non-linear-activation-functions): why do nns need non-linear activation functions   \n",
    "   -> because the nn would just be outputing a linear function of the input   \n",
    "   = standard linear regression.   \n",
    "linear activation function: g(z) = z -> outputs the input  \n",
    "[coursera ai: derivatives of activation functions](https://www.coursera.org/learn/neural-networks-deep-learning/lecture/qcG1j/derivatives-of-activation-functions)   \n",
    "  -> output values based on possible input values (max, min, 0)     \n",
    "    \n",
    "can be different for different layers in the same nn\n",
    "- **sigmoid**: [0..1] -> only better for binary classification (0 or 1)  \n",
    "- **tanh**: hyperbolic tangent [-1..1] -> 0 mean. always better for hidden layers. makes learning easier for the next layer  \n",
    "- **relu**: [1 or 0 (or 0,00000000000x)] *most used* -> faster learning of all  \n",
    "- **leaky relu**:  \n",
    "\n",
    "#### Initialization\n",
    "[coursera ai: random initialization](https://www.coursera.org/learn/neural-networks-deep-learning/lecture/XtFPI/random-initialization).   \n",
    "- **weights**: do not initialize with zeros -> all hidden units are symmetric, they all compute the same function. initialize with random values instead.   \n",
    "`np.random.randn((2,2)) * 0.01`   # '* 0.01' initializes to very small values   \n",
    "if weights too large, GD will be very slow (values in the flat parts of the activation functions)  \n",
    "- **bias**: can be initialized with zeros, as long as weights are note   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BUILDING A NEURAL NETWORK:\n",
    "\n",
    "**SEE** coursera ai: course 1 w3 coding challenge (http://localhost:8888/notebooks/ai-coursera-deeplearning-specialization/course1-nn-deep-learning/w3-001-classification.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load data and preprocess dataset\n",
    "\n",
    "- Figure out the dimensions and shapes of the problem (m_train, m_test, num_px, ...)  \n",
    "- Reshape the datasets such that each example is now a vector of size (num_px * num_px * 3, 1)  \n",
    "- \"Standardize\" the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define layers shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_sizes(X, Y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- input dataset of shape (input size, number of examples)\n",
    "    Y -- labels of shape (output size, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    n_x -- the size of the input layer\n",
    "    n_h -- the size of the hidden layer\n",
    "    n_y -- the size of the output layer\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initialize parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    n_x -- size of the input layer\n",
    "    n_h -- size of the hidden layer\n",
    "    n_y -- size of the output layer\n",
    "    \n",
    "    Returns:\n",
    "    params -- python dictionary containing your parameters:\n",
    "                    W1 -- weight matrix of shape (n_h, n_x)\n",
    "                    b1 -- bias vector of shape (n_h, 1)\n",
    "                    W2 -- weight matrix of shape (n_y, n_h)\n",
    "                    b2 -- bias vector of shape (n_y, 1)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### implement forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    X -- input data of size (n_x, m)\n",
    "    parameters -- python dictionary containing your parameters (output of initialization function)\n",
    "                  parameters = {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2}\n",
    "    \n",
    "    Returns:\n",
    "    A2 -- The sigmoid output of the second activation\n",
    "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\"\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compute cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(A2, Y, parameters):\n",
    "    \"\"\"\n",
    "    Computes the cross-entropy cost given in equation (13)\n",
    "    \n",
    "    Arguments:\n",
    "    A2 -- The sigmoid output of the second activation, of shape (1, number of examples)\n",
    "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
    "    parameters -- python dictionary containing your parameters W1, b1, W2 and b2\n",
    "    [Note that the parameters argument is not used in this function, \n",
    "    but the auto-grader currently expects this parameter.\n",
    "    Future version of this notebook will fix both the notebook \n",
    "    and the auto-grader so that `parameters` is not needed.\n",
    "    For now, please include `parameters` in the function signature,\n",
    "    and also when invoking this function.]\n",
    "    \n",
    "    Returns:\n",
    "    cost -- cross-entropy cost given equation (13)\n",
    "    \n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### implement backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(parameters, cache, X, Y):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation using the instructions above.\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing our parameters \n",
    "                  parameters = {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2}\n",
    "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\".\n",
    "    X -- input data of shape (2, number of examples)\n",
    "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    grads -- python dictionary containing your gradients with respect to different parameters\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### implement parameters update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate = 1.2):\n",
    "    \"\"\"\n",
    "    Updates parameters using the gradient descent update rule given above\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "                  parameters = {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2}\n",
    "    grads -- python dictionary containing your gradients \n",
    "             grads = {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2}\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build your nn model using the previous functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_model(X, Y, n_h, num_iterations = 10000, print_cost=False):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- dataset of shape (2, number of examples)\n",
    "    Y -- labels of shape (1, number of examples)\n",
    "    n_h -- size of the hidden layer\n",
    "    num_iterations -- Number of iterations in gradient descent loop\n",
    "    print_cost -- if True, print the cost every 1000 iterations\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### implement prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(parameters, X):\n",
    "    \"\"\"\n",
    "    Using the learned parameters, predicts a class for each example in X\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    X -- input data of size (n_x, m)\n",
    "    \n",
    "    Returns\n",
    "    predictions -- vector of predictions of our model (red: 0 / blue: 1)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tune hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best NN perfs = scale\n",
    "  scale algorithms: very large NN (lots of hidden layers, of parameters, of connections)  \n",
    "+ scale data: very large amount of data  \n",
    "+ scale computation: CPU, GPU  \n",
    "ex: switch from sigmoid (lrng becomes really slow at extremities) to relu  \n",
    " -> GD works much faster  \n",
    " (Neural Networks and Deep Learning - Semaine 1 - Why is Deep Learning taking off?)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5,)\n",
      "[-0.16041365 -0.11626201  1.23434608 -1.05715004 -0.14967765]\n",
      "2.702829238667423\n",
      "\n",
      "[[-0.49284074]\n",
      " [-0.15013489]\n",
      " [ 0.52552569]\n",
      " [-1.14464424]\n",
      " [-1.0993032 ]]\n",
      "[[-0.49284074 -0.15013489  0.52552569 -1.14464424 -1.0993032 ]]\n",
      "[[ 0.242892    0.07399259 -0.25900047  0.56412732  0.54178141]\n",
      " [ 0.07399259  0.02254049 -0.07889974  0.17185104  0.16504377]\n",
      " [-0.25900047 -0.07889974  0.27617725 -0.60153996 -0.57771208]\n",
      " [ 0.56412732  0.17185104 -0.60153996  1.31021044  1.25831108]\n",
      " [ 0.54178141  0.16504377 -0.57771208  1.25831108  1.20846753]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Rank 1 arrays (value,)\n",
    "a = np.random.randn(5)\n",
    "print(a.shape)\n",
    "print(a.T)                # a and a.T are the same\n",
    "print(np.dot(a, a.T))     # value\n",
    "\n",
    "# Column vectors: (value, 1) - Row vectors: (1, value)\n",
    "print(\"\")\n",
    "a = np.random.randn(5,1)  # 5*1 matrix\n",
    "print(a)\n",
    "print(a.T)                # inversed shape\n",
    "print(np.dot(a, a.T))     # vector\n",
    "assert(a.shape == (5,1))  # can help as DOCUMENTATION\n",
    "\n",
    "# ALWAYS use column or row vectors, not rank 1 arrays\n",
    "# OR reshape:\n",
    "a.reshape((5,1))\n",
    "\n",
    "# A trick when you want to flatten a matrix X of shape (a,b,c,d) \n",
    "# to a matrix X_flatten of shape (b âˆ— c âˆ— d, a) is to use:\n",
    "X_flatten = X.reshape(X.shape[0], -1).T\n",
    "\n",
    "## Many software bugs in deep learning \n",
    "## come from having matrix/vector dimensions that don't fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
