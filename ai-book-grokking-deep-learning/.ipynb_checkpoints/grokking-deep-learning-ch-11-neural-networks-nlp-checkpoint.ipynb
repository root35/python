{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NATURAL LANGUAGE PROCESSING\n",
    "\n",
    "Most impact of deep learning on NLP:\n",
    "* neural word embeddings: increases accuracy of NLP algorithms\n",
    "* recurrent neural networks (RNNs): effective at predicting accross sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised NLP: Sentiment classification with IMDB movie reviews\n",
    "\n",
    "[Corpus page](http://ai.stanford.edu/%7Eamaas/data/sentiment/)\n",
    "\n",
    "## Encoding strategy\n",
    "\n",
    "* **output dataset** (target predictions) already a number (1 to 5):   \n",
    "  * adjust the range to be between 0 and 1, so that we can use **binary `softmax`**\n",
    "* **input dataset**: bag of words\n",
    "  * given a review's vocabulary, predict its sentiment\n",
    "  * matrix (reviews * words): each column a word, cell $(i,j)$ tells if review $i$ contains word $j$\n",
    "  * so each row is a vector of 1s (for words in the reviews) and 0s (everywhere else)\n",
    "  * = *one-hot encoding*: most common format for encoding binary data (presence or absence of a feature)\n",
    "\n",
    "## Neural network architecture\n",
    "\n",
    "* `layer_0`: first layer\n",
    "* `weights_0_1`: linear layer -> replaced with an **embedding layer** (shortcut to `layer_1`)\n",
    "  * select rows from `weights_0_1` corresponding to each word in a review, and sum them (or average)\n",
    "  * instead of doing a big vector-matrix multiplication, mostly multiplying 0s (vocab size is 131094)\n",
    "  * only difference: embedding is much faster\n",
    "* `layer_1`: `relu` layer\n",
    "* `weights_1_2`: linear layer\n",
    "* `layer_2`: prediction layer\n",
    "\n",
    "#### Predictions\n",
    "\n",
    "* sigmoid:\n",
    "  * gives an output **between 0 and 1** that can be interpreted as a **probability**\n",
    "  * output should be:\n",
    "    * **_close to 1_** if target label is 1\n",
    "    * **_close to 0_** if target label is 0\n",
    "    * condition for a correct prediction: `np.abs(layer_2 - target_label) < 0.5`\n",
    "    \n",
    "    \n",
    "```\n",
    "print(layer_2, target_label, np.abs(layer_2 - target_label), np.abs(layer_2 - target_label) < 0.5)\n",
    "# [0.90341038] 1 [0.09658962] [ True]\n",
    "# [0.00025796] 1 [0.99974204] [False]\n",
    "# [0.00053331] 0 [0.00053331] [ True]\n",
    "# [0.81981521] 0 [0.81981521] [False]\n",
    "```\n",
    "\n",
    "## The hidden layer\n",
    "\n",
    "* What does the **hidden layer** learn?\n",
    "  * hidden layers group datapoints from a previous layer into $n$ groups ($n$ the number of neurons)\n",
    "  * each hidden neuron takes in a datapoint and answers the question: **_Is this datapoint in my group?_**\n",
    "  * each hidden neuron classifies a datapoint as either *subscribing* or *not subscribing* to its group\n",
    "    * **_similar datapoints_** (layers) subscribe to many of the **_same groups_**\n",
    "    * **_similar inputs_** (words) have **_similar weights_** linking them to various hidden neurons\n",
    "    * hidden neurons are a measure of each word's group affinity\n",
    "  * hidden layer searches for useful groupings of its input\n",
    "  * powerful groupings for the next layer to use to make its predictions\n",
    "* What are **useful groupings**? They do two things:\n",
    "  * must be useful to the prediction of an output label\n",
    "  * must be an actual phenomenon in the data:\n",
    "    * bad groupings just memorize data\n",
    "    * good groupings pick up linguistically useful phenomena:   \n",
    "      ex. a neuron that turns **_off_** when it sees *awful* and **_on_** when it sees *nice*\n",
    "* Problems:\n",
    "  * *it was great, not terrible* creates the same `layer_1` value as *it was terrible, not great*\n",
    "  * network is very unlikely to create a hidden neuron that understands negation\n",
    "\n",
    "## The weights connecting words and hidden neurons\n",
    "\n",
    "* All the weights for \"good\" form the embedding for \"good\"   \n",
    "  They reflect how much the term \"good\" is a member of each group (hidden neuron)\n",
    "* Words with similar predictive power have **similar word embeddings** (weight values)\n",
    "* Words that correlate with similar labels have similar weights connecting them to various hidden neurons,\n",
    "  * because the hidden layer groups them into similar hidden neurons,\n",
    "  * so that the final layer (`weights_1_2`) can make correct predictions\n",
    "* We can see it by taking a word and searching for other words with similar weight values connecting them to each hidden neuron (group)\n",
    "* A neuron has similar meaning to other neurons in the same layer if and only if it has similar weights connecting it to the next and/or previous layers\n",
    "* The meaning of a neuron entirely **depends on the target labels**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import re\n",
    "import itertools\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 25000 reviews, 25000 labels\n",
      "testing : 25000 reviews, 25000 labels\n"
     ]
    }
   ],
   "source": [
    "punc = '''[!()-[]{};:'\\\"\\, <>./?@#$%^&*_~]'''\n",
    "\n",
    "train_dir_pos = '/Users/macbook/code/_dataset_imdb/aclImdb/train/pos'\n",
    "train_dir_neg = '/Users/macbook/code/_dataset_imdb/aclImdb/train/neg'\n",
    "test_dir_pos = '/Users/macbook/code/_dataset_imdb/aclImdb/test/pos'\n",
    "test_dir_neg = '/Users/macbook/code/_dataset_imdb/aclImdb/test/neg'\n",
    "\n",
    "# 12500 files each\n",
    "train_files_pos = os.listdir(train_dir_pos)\n",
    "train_files_neg = os.listdir(train_dir_neg)\n",
    "test_files_pos = os.listdir(test_dir_pos)\n",
    "test_files_neg = os.listdir(test_dir_neg)\n",
    "\n",
    "## TRAINING DATA\n",
    "\n",
    "# labels\n",
    "train_labels = ([1] * len(train_files_pos)) + ([0] * len(train_files_neg))\n",
    "\n",
    "# reviews\n",
    "train_raw_reviews = list()\n",
    "\n",
    "for filename in train_files_pos:\n",
    "    filepath = os.path.join(train_dir_pos, filename)\n",
    "    with open(filepath, 'r') as f:\n",
    "        rev = ' '.join(f.readlines()).strip().lower()\n",
    "        review = re.sub(r'[^\\w\\s]', ' ', rev)\n",
    "        train_raw_reviews.append(review)\n",
    "\n",
    "for filename in train_files_neg:\n",
    "    filepath = os.path.join(train_dir_neg, filename)\n",
    "    with open(filepath, 'r') as f:\n",
    "        rev = ' '.join(f.readlines()).strip().lower()\n",
    "        review = re.sub(r'[^\\w\\s]', ' ', rev)\n",
    "        train_raw_reviews.append(review)\n",
    "\n",
    "## TESTING DATA\n",
    "\n",
    "# labels\n",
    "# Folder '/Users/macbook/code/_dataset_imdb/aclImdb/test/pos' contains negative reviews !!!\n",
    "# test_labels = ([0] * len(test_files_pos)) + ([0] * len(test_files_neg))\n",
    "test_labels = ([1] * len(test_files_pos)) + ([0] * len(test_files_neg))\n",
    "\n",
    "# reviews\n",
    "test_raw_reviews = list()\n",
    "\n",
    "for filename in test_files_pos:\n",
    "    filepath = os.path.join(test_dir_pos, filename)\n",
    "    with open(filepath, 'r') as f:\n",
    "        rev = ' '.join(f.readlines()).strip().lower()\n",
    "        review = re.sub(r'[^\\w\\s]', ' ', rev)\n",
    "        test_raw_reviews.append(review)\n",
    "\n",
    "for filename in test_files_neg:\n",
    "    filepath = os.path.join(test_dir_neg, filename)\n",
    "    with open(filepath, 'r') as f:\n",
    "        rev = ' '.join(f.readlines()).strip().lower()\n",
    "        review = re.sub(r'[^\\w\\s]', ' ', rev)\n",
    "        test_raw_reviews.append(review)\n",
    "\n",
    "print(f'training: {len(train_raw_reviews)} reviews, {len(train_labels)} labels')\n",
    "print(f'testing : {len(test_raw_reviews)} reviews, {len(test_labels)} labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding the input and target datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 102935\n"
     ]
    }
   ],
   "source": [
    "## VOCABULARY INDEX\n",
    "# Only based on training data, unknown words in test dataset set to None\n",
    "# train vocab = 94 463 - train+test vocab = 131 093\n",
    "\n",
    "tokenizer = lambda text: list(set(text.split(' ')))\n",
    "\n",
    "train_sentences = list(map(tokenizer, train_raw_reviews))\n",
    "test_sentences = list(map(tokenizer, test_raw_reviews))\n",
    "\n",
    "# Listing all words in training and test data\n",
    "\n",
    "vocab = set()\n",
    "\n",
    "for sentence in train_sentences:\n",
    "    for word in sentence:\n",
    "        vocab.add(word)\n",
    "\n",
    "for sentence in test_sentences:\n",
    "    for word in sentence:\n",
    "        vocab.add(word)\n",
    "            \n",
    "vocab = list(vocab)\n",
    "print(f'vocab size: {len(vocab)}')\n",
    "\n",
    "vocab_index = {}\n",
    "for index, word in enumerate(vocab):\n",
    "    vocab_index[word] = index\n",
    "\n",
    "## DATASETS\n",
    "# Converting each sentence into a sequence (list of word indices)\n",
    "\n",
    "train_seqs = list()\n",
    "for sentence in train_sentences:\n",
    "    sequence = np.array([vocab_index.get(word) for word in sentence if len(word) > 0], dtype=int)\n",
    "    train_seqs.append(sequence)\n",
    "train_sequences = np.array(train_seqs)\n",
    "\n",
    "test_seqs = list()\n",
    "for sentence in test_sentences:\n",
    "    sequence = np.array([vocab_index.get(word) for word in sentence if len(word) > 0], dtype=int)\n",
    "    test_seqs.append(sequence)\n",
    "test_sequences = np.array(test_seqs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings\n",
    "\n",
    "[Wikipedia: Word_embedding](https://en.wikipedia.org/wiki/Word_embedding)\n",
    "\n",
    "<p style=\"background:#DDEEEE;padding:15px;\">\n",
    "    Vectors whose relative similarities correlate with semantic similarity based on distributional properties\n",
    "    <br/>\n",
    "    Firth: <i>a word is characterized by the company it keeps</i>\n",
    "</p>\n",
    "\n",
    "* Research area of **distributional semantics**\n",
    "* *aims to quantify and categorize **semantic similarities** between linguistic items based on their **distributional properties** in large samples of language data* (Wikipédia)\n",
    "* representing words as vectors, started in the 1960s with the development of the vector space model\n",
    "* In 2013, a team at Google led by Tomas Mikolov created **`word2vec`**, a word embedding toolkit which can train vector space models faster than the previous approaches\n",
    "\n",
    "### Limitations\n",
    "\n",
    "Polysemy and homonymy are not handled properly   \n",
    "words with **multiple meanings** are conflated into a **single representation** (a single vector in the semantic space)   \n",
    "  -> necessity for **multi-sense embeddings**\n",
    ">Most approaches that produce multi-sense embeddings can be divided into two main categories for their word sense representation, i.e., **unsupervised** and **knowledge-based**.[23] Based on word2vec skip-gram, **Multi-Sense Skip-Gram (MSSG)**[24] performs word-sense discrimination and embedding simultaneously, improving its training time, while assuming a specific number of senses for each word. In the **Non-Parametric Multi-Sense Skip-Gram (NP-MSSG)** this number can vary depending on each word. Combining the prior knowledge of lexical databases (e.g., WordNet, ConceptNet, BabelNet), word embeddings and word sense disambiguation, **Most Suitable Sense Annotation (MSSA)[25]** labels word-senses through an unsupervised and knowledge-based approach considering a word’s context in a pre-defined sliding window. Once the words are disambiguated, they can be used in a standard word embeddings technique, so multi-sense embeddings are produced. MSSA architecture allows the disambiguation and annotation process to be performed recurrently in a self-improving manner.\n",
    ">\n",
    ">The use of multi-sense embeddings is known to **improve performance in several NLP tasks**, such as part-of-speech tagging, semantic relation identification, and semantic relatedness. However, tasks involving named entity recognition and sentiment analysis seem not to benefit from a multiple vector representation.[26]\n",
    "   \n",
    "   \n",
    ">Software for training and using word embeddings includes Tomas Mikolov's [Word2vec](https://en.wikipedia.org/wiki/Word2vec), Stanford University's [GloVe](https://en.wikipedia.org/wiki/GloVe_(machine_learning)), AllenNLP's [ELMo](https://en.wikipedia.org/wiki/ELMo), [BERT](https://en.wikipedia.org/wiki/BERT_(language_model)), [fastText](https://en.wikipedia.org/wiki/FastText), [Gensim](https://en.wikipedia.org/wiki/Gensim), Indra[33] and [Deeplearning4j](https://en.wikipedia.org/wiki/Deeplearning4j). Principal Component Analysis (PCA) and T-Distributed Stochastic Neighbour Embedding (t-SNE) are both used to reduce the dimensionality of word vector spaces and visualize word embeddings and clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Network parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "sigmoid = lambda x: 1 / (1 + np.exp(-x))\n",
    "def softmax(x):\n",
    "    temp = np.exp(x)\n",
    "    return temp / np.sum(temp, axis=0, keepdims=True)\n",
    "\n",
    "alpha = 0.01\n",
    "iterations = 2\n",
    "hidden_size = 100\n",
    "\n",
    "train_data_size = len(train_sequences)\n",
    "test_data_size = len(test_sequences)\n",
    "\n",
    "weights_0_1 = 0.2 * np.random.random((len(vocab), hidden_size)) - 0.1\n",
    "weights_1_2 = 0.2 * np.random.random((hidden_size, 1)) - 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning and predicting with training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0 Progress: 0.99996 Train-Acc: 0.99884\n",
      "Iter: 1 Progress: 0.99996 Train-Acc: 0.99822\n"
     ]
    }
   ],
   "source": [
    "train_correct_preds, train_total_preds = 0, 0\n",
    "train_acc = 0.0\n",
    "\n",
    "for it in range(iterations):\n",
    "    \n",
    "    for i in range(train_data_size):\n",
    "        \n",
    "        sequence, target_label = train_sequences[i], train_labels[i]\n",
    "        \n",
    "        # weights_0_1[x]: extracting vectors of all words in sequence (indexing with a list)\n",
    "        word_vectors = weights_0_1[sequence]\n",
    "        layer_1 = sigmoid(np.sum(word_vectors, axis=0))  # embed + sigmoid\n",
    "        layer_2 = sigmoid(np.dot(layer_1, weights_1_2))  # linear + softmax\n",
    "        \n",
    "        layer_2_delta = layer_2 - target_label  # pred - target_pred\n",
    "        layer_1_delta = layer_2_delta.dot(weights_1_2.T)\n",
    "        \n",
    "        weights_0_1[sequence] -= alpha * layer_1_delta  # updating weights of the words in sequence\n",
    "        weights_1_2 -= alpha * np.outer(layer_1, layer_2_delta)\n",
    "        \n",
    "        if np.abs(layer_2_delta) < 0.5:\n",
    "            train_correct_preds += 1\n",
    "        train_total_preds += 1\n",
    "        train_acc = train_correct_preds / float(train_total_preds)\n",
    "        \n",
    "        if (i % 10 == 9):\n",
    "            progress = str(i / float(train_data_size))\n",
    "            sys.stdout.write(f'\\rIter: {it}' \\\n",
    "                           + f' Progress: {progress}'\\\n",
    "                           + f' Train-Acc: {train_acc:.5f}')\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning and predicting with test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test-Acc 0.5\n"
     ]
    }
   ],
   "source": [
    "test_correct_preds, test_total_preds = 0, 0\n",
    "test_acc = 0.0\n",
    "\n",
    "for j in range(test_data_size):\n",
    "    \n",
    "    sequence, target_label = test_sequences[j], test_labels[j]\n",
    "    \n",
    "    word_vectors = weights_0_1[sequence]\n",
    "    layer_1 = sigmoid(np.sum(word_vectors, axis=0))\n",
    "    layer_2 = sigmoid(np.dot(layer_1, weights_1_2))\n",
    "    \n",
    "    if np.abs(layer_2 - target_label) < 0.5:\n",
    "        test_correct_preds += 1\n",
    "    test_total_preds += 1\n",
    "    \n",
    "test_acc = test_correct_preds / float(test_total_preds)\n",
    "print(f'Test-Acc {test_acc}')\n",
    "\n",
    "# !! Folder '/Users/macbook/code/_dataset_imdb/aclImdb/test/pos' contains negative reviews.\n",
    "# So all reviews with 'target_label == 1' are predicted as 0 (negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing word embeddings: visualizing weight similarity\n",
    "\n",
    "* input word: select its corresponding row in `weights_0_1`\n",
    "* each entry in that row represents each weight proceeding from that word to each hidden neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('amazing', -0.0), ('haschiguchi', -0.6317337806357755), ('shortsightedness', -0.6376352345185063), ('sapir', -0.6448039135797875), ('abner', -0.6448376806024975), ('chekhov', -0.6496655227088957), ('kehna', -0.6513570764963157), ('alsion', -0.6527235465071505), ('6100', -0.6547546690316058), ('80yr', -0.6571166527124129), ('schwartzenegger', -0.6574348458112316), ('larroquette', -0.6594053730151328), ('paleontology', -0.6603381885943841), ('1967', -0.6631118085294591), ('35c', -0.6648377121735045), ('uproots', -0.665032801359282), ('margraet', -0.6652087394618698), ('roberto', -0.6657892334805985), ('chjaractor', -0.6677867319114845), ('berti', -0.6687005558229965)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "def similar(target):\n",
    "    target_index = vocab_index.get(target)\n",
    "    scores = Counter()\n",
    "    for word,index in vocab_index.items():\n",
    "        raw_difference = weights_0_1[index] - weights_0_1[target_index]\n",
    "        squared_difference = raw_difference * raw_difference\n",
    "        scores[word] = -math.sqrt(sum(squared_difference))\n",
    "    \n",
    "    return scores.most_common(20)\n",
    "\n",
    "print(similar('terrible'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meaning is derived from loss\n",
    "\n",
    "<p style=\"background:#DDEEEE;padding:15px;\">\n",
    "    <b>Learning</b> = Adjust each weight in the <b>correct direction</b> by the <b>correct amount</b> so `error` reduces to 0\n",
    "</p>\n",
    "<br/>\n",
    "\n",
    "<div style=\"background:#DDEEEE;padding:15px;\">\n",
    "    <p>\n",
    "        <b>The secret</b>: For any <code>input</code> and <code>goal_pred</code>, an exact relationship is defined between <code>error</code> and <code>weight</code>, found by combining the <code>prediction</code> and <code>error</code> formula.\n",
    "    </p>\n",
    "    <p style=\"text-align:center\">\n",
    "        <code>error = ((0.5 * weight) - goal_target) ** 2</code>\n",
    "    </p>\n",
    "    <p>\n",
    "        <code>(0.5 * weight)</code> the backpropagation part, <code>0.5</code> the <code>input</code>\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "* NNs don't really learn data, they minimize the loss function (including forward propagation)\n",
    "* the choice of loss function determines the neural network's knowledge   \n",
    "<br/>   \n",
    "\n",
    "* If a network is overfitting, you can **augment the loss function** by :\n",
    "  * choosing simpler nonlinearities\n",
    "  * smaller layer sizes\n",
    "  * shallower architectures\n",
    "  * larger datasets\n",
    "  * or more aggressive regularization techniques\n",
    "* All have a similar effect of the loss function and similar consequence on the behavior of the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word analogies\n",
    "\n",
    "* If we train the previous network on a large enough corpus, we'll be able to:\n",
    "  * take the vector for `king`\n",
    "  * subtract from it the vector for `man`\n",
    "  * add in the vector for `woman`\n",
    "  * then search for the most similar vector (other than those in the query)\n",
    "  * most often it is the vector for `queen`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(102935, 1) (102935, 100) (102935, 100)\n",
      "[('terrible', -0.4905085610937908), ('good', -0.5026199224223888), ('salò', -0.5218130501909815), ('sbd', -0.531838353839204), ('loans', -0.5422102403450381), ('giles', -0.5428412871190563), ('withholding', -0.5446938718395303), ('discretionary', -0.5481240906829359), ('assessing', -0.5500615956821244), ('carville', -0.5505426847949003)]\n"
     ]
    }
   ],
   "source": [
    "def analogy(positive=['terrible', 'good'], negative=['bad']):\n",
    "\n",
    "    norms = np.sum(weights_0_1 * weights_0_1, axis=1)\n",
    "    norms.resize(norms.shape[0], 1)\n",
    "    \n",
    "    normed_weights = weights_0_1 * norms\n",
    "    print(norms.shape, weights_0_1.shape, normed_weights.shape)\n",
    "    \n",
    "    query_vect = np.zeros(len(weights_0_1[0]))\n",
    "    for word in positive:\n",
    "        query_vect += normed_weights[vocab_index.get(word)]\n",
    "    for word in negative:\n",
    "        query_vect -= normed_weights[vocab_index.get(word)]\n",
    "    \n",
    "    scores = Counter()\n",
    "    for word, index in vocab_index.items():\n",
    "        raw_difference = weights_0_1[index] - query_vect\n",
    "        squared_difference = raw_difference * raw_difference\n",
    "        scores[word] = -math.sqrt(sum(squared_difference))\n",
    "    \n",
    "    return scores.most_common(10)\n",
    "\n",
    "print(analogy(['terrible', 'good'], ['bad']))   # terrible - bad + good = ?\n",
    "print(analogy(['elizabeth', 'he'], ['she']))    # elizabeth - she + he = ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For the next chapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['comment this movie is impossible  is terrible  very improbable  bad interpretati', 'horrible waste of time   bad acting  plot  directing  this is the most boring mo', 'this movie stinks  the stench resembles bad cowpies that sat in the sun too long']\n",
      "\n",
      "['this is actually one of my favorite films  i would recommend that everyone watch', 'this movie is terrible but it has some good effects ', 'malcolm mcdowell has not had too many good movies lately and this is no differen']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "norms = np.sum(weights_0_1 * weights_0_1, axis=1)\n",
    "norms.resize(norms.shape[0], 1)\n",
    "normed_weights = weights_0_1 * norms\n",
    "\n",
    "def make_sentence_vector(words):\n",
    "    indices = list(map(lambda x: vocab_index.get(x), \\\n",
    "              filter(lambda x: x in vocab_index, words)))\n",
    "    return np.mean(normed_weights[indices], axis=0)\n",
    "\n",
    "reviews2vectors = list()\n",
    "for review in train_sentences:\n",
    "    reviews2vectors.append(make_sentence_vector(review))\n",
    "reviews2vectors = np.array(reviews2vectors)\n",
    "\n",
    "def most_similar_reviews(review):\n",
    "    v = make_sentence_vector(review)\n",
    "    \n",
    "    scores = Counter()\n",
    "    for index, value in enumerate(reviews2vectors.dot(v)):\n",
    "        scores[index] = value\n",
    "    \n",
    "    most_similar = list()\n",
    "    for index, score in scores.most_common(3):\n",
    "        most_similar.append(train_raw_reviews[index][0:80])\n",
    "    \n",
    "    return most_similar\n",
    "\n",
    "print(most_similar_reviews(['boring', 'awful']))\n",
    "print()\n",
    "print(most_similar_reviews(['nice', 'good']))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
