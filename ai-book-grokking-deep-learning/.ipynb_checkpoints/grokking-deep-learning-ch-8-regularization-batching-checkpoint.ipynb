{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OVERFITTING\n",
    "\n",
    "* The more layers, the more prone the network is to overfit.   \n",
    "  The smaller a network is, the less it's able to overfit.\n",
    "* **Overfitting**: perfect predictions on training data, but bad results with test data.\n",
    "* **Causes**: NNS can get worse:\n",
    "  * if the nn is **trained too much** -> stop   \n",
    "    testing accuracy starts decreasing while training accuracy still increasing\n",
    "  * if the nn **learns the noise** in the dataset instead of making decisions based only on the **true signal**   \n",
    "<br/>   \n",
    "  \n",
    "* How to avoid overfitting? With regularization\n",
    "* How do you get NNs to learn only the **true signal** (essence of inputs) and ignore the noise (non-discriminant features),    \n",
    "to **capture only the general information and ignore the fine-grained details**?\n",
    "\n",
    "# Regularization method 1: Early stopping\n",
    "\n",
    "* stop training when NN starts getting worse   \n",
    "  = don't let the nn train long enough to learn it   \n",
    "  = cheapest form of regularization   \n",
    "\n",
    "### Example\n",
    "\n",
    "In the following example:\n",
    "* the model predicts **perfectly on training** data but **poorly on test** data:\n",
    "```\n",
    "Iter 350  Train-Err 0.108 Train-Acc 1.0  Test-Err 0.654 Test-Acc 0.807\n",
    "```\n",
    "* and the test accuracy **decreases** after 100 iterations even though the training accuracy keeps increasing   \n",
    "  test accuracy drops to 77.1% when training finishes (training accuracy at 100%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_images: (1000, 784), train_labels: (1000, 10)\n",
      "test_images:  (10000, 784), test_labels:  (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "\n",
    "# x_train, x_test:\n",
    "#   uint8 arrays of grayscale image data with shapes (num_samples, 28, 28)\n",
    "# y_train, y_test: \n",
    "#   uint8 arrays of digit labels (integers in range 0-9) with shapes (num_samples,)\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, y_train = x_train[0:1000], y_train[0:1000]\n",
    "# x_test, y_test = x_test[0:1000], y_test[0:1000]\n",
    "\n",
    "# IMAGES\n",
    "train_images = x_train.reshape(1000, 28*28) / 255\n",
    "test_images = x_test.reshape(len(x_test), 28*28) / 255\n",
    "\n",
    "# LABELS: one_hot vectors\n",
    "# label '4' = [0, 0, 0, 0, 1, 0, 0...]\n",
    "train_labels = np.zeros((len(y_train), 10))\n",
    "for i, l in enumerate(y_train):\n",
    "    train_labels[i][l] = 1\n",
    "\n",
    "test_labels = np.zeros((len(y_test), 10))\n",
    "for i, l in enumerate(y_test):\n",
    "    test_labels[i][l] = 1\n",
    "\n",
    "print(f'train_images: {train_images.shape}, train_labels: {train_labels.shape}')    \n",
    "print(f'test_images:  {test_images.shape}, test_labels:  {test_labels.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Network parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "relu = lambda x: (x >= 0) * x  # x if x>0, 0 otherwise\n",
    "relu2deriv = lambda x: x >= 0  # 1 if input>0, 0 otherwise\n",
    "\n",
    "alpha = 0.005\n",
    "iterations = 1501\n",
    "hidden_size = 40\n",
    "\n",
    "pixels_per_image = 784\n",
    "num_labels = 10\n",
    "num_train_images = len(train_images)\n",
    "num_test_images = len(test_images)\n",
    "\n",
    "weights_0_1 = 0.2 * np.random.random((pixels_per_image, hidden_size)) - 0.1\n",
    "weights_1_2 = 0.2 * np.random.random((hidden_size, num_labels)) - 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning and predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0  Train-Err 0.722 Train-Acc 0.537  Test-Err 0.601 Test-Acc 0.702\n",
      "Iter 50  Train-Err 0.204 Train-Acc 0.966  Test-Err 0.437 Test-Acc 0.894\n",
      "Iter 100  Train-Err 0.166 Train-Acc 0.984  Test-Err 0.482 Test-Acc 0.869\n",
      "Iter 150  Train-Err 0.145 Train-Acc 0.991  Test-Err 0.513 Test-Acc 0.854\n",
      "Iter 200  Train-Err 0.130 Train-Acc 0.998  Test-Err 0.538 Test-Acc 0.846\n",
      "Iter 250  Train-Err 0.120 Train-Acc 0.999  Test-Err 0.577 Test-Acc 0.831\n",
      "Iter 300  Train-Err 0.113 Train-Acc 0.999  Test-Err 0.614 Test-Acc 0.818\n",
      "Iter 350  Train-Err 0.108 Train-Acc 1.0  Test-Err 0.654 Test-Acc 0.807\n",
      "Iter 400  Train-Err 0.106 Train-Acc 0.998  Test-Err 0.691 Test-Acc 0.797\n",
      "Iter 450  Train-Err 0.105 Train-Acc 0.998  Test-Err 0.712 Test-Acc 0.793\n",
      "Iter 500  Train-Err 0.104 Train-Acc 0.999  Test-Err 0.729 Test-Acc 0.788\n",
      "Iter 550  Train-Err 0.104 Train-Acc 0.999  Test-Err 0.745 Test-Acc 0.779\n",
      "Iter 600  Train-Err 0.105 Train-Acc 0.998  Test-Err 0.756 Test-Acc 0.771\n",
      "Iter 650  Train-Err 0.106 Train-Acc 0.998  Test-Err 0.752 Test-Acc 0.772\n",
      "Iter 700  Train-Err 0.106 Train-Acc 1.0  Test-Err 0.749 Test-Acc 0.774\n",
      "Iter 750  Train-Err 0.106 Train-Acc 1.0  Test-Err 0.745 Test-Acc 0.771\n"
     ]
    }
   ],
   "source": [
    "for j in range(iterations):\n",
    "    \n",
    "    #-- Predicting on training data\n",
    "    \n",
    "    train_error, train_correct_cnt = 0.0, 0\n",
    "    \n",
    "    for i in range(num_train_images):\n",
    "        \n",
    "        target_pred = train_labels[i:i+1]\n",
    "        \n",
    "        layer_0 = train_images[i:i+1]\n",
    "        layer_1 = relu(np.dot(layer_0, weights_0_1))\n",
    "        layer_2 = np.dot(layer_1, weights_1_2)\n",
    "        \n",
    "        train_error += np.sum((target_pred - layer_2) ** 2)  # pred - target_pred\n",
    "        train_err = train_error / float(num_train_images)\n",
    "        \n",
    "        train_correct_cnt += int( np.argmax(layer_2) == np.argmax(target_pred) )\n",
    "        train_acc = train_correct_cnt / float(num_train_images)\n",
    "        \n",
    "        layer_2_delta = target_pred - layer_2\n",
    "        layer_1_delta = layer_2_delta.dot(weights_1_2.T) * relu2deriv(layer_1)\n",
    "        \n",
    "        weights_1_2 += alpha * layer_1.T.dot(layer_2_delta)\n",
    "        weights_0_1 += alpha * layer_0.T.dot(layer_1_delta)\n",
    "    \n",
    "    #-- Predicting on test data\n",
    "    \n",
    "    if (j % 50 == 0) or (j == iterations - 1):\n",
    "        \n",
    "        test_error, test_error_cnt = 0.0, 0\n",
    "\n",
    "        for k in range(num_test_images):\n",
    "            \n",
    "            target_pred = test_labels[k:k+1]\n",
    "\n",
    "            layer_0 = test_images[k:k+1]\n",
    "            layer_1 = relu(np.dot(layer_0, weights_0_1))\n",
    "            layer_2 = np.dot(layer_1, weights_1_2)\n",
    "\n",
    "            test_error += np.sum((target_pred - layer_2) ** 2)\n",
    "            test_err = test_error / float(num_test_images)\n",
    "            \n",
    "            test_error_cnt += int( np.argmax(layer_2) == np.argmax(target_pred) )\n",
    "            test_acc = test_error_cnt / float(num_test_images)\n",
    "\n",
    "        print('Iter ' + str(j) + \\\n",
    "              '  Train-Err ' + str(train_err)[0:5] + ' Train-Acc ' + str(train_acc)[0:5] + \\\n",
    "              '  Test-Err ' + str(test_err)[0:5] + ' Test-Acc ' + str(test_acc)[0:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization method 2: Dropout\n",
    "\n",
    "* randomly turn off neurons during training   \n",
    "  = set them to 0 (and usually also their delta during backpropagation, but not necessary)\n",
    "* network trains on **random subsections** of the network   \n",
    "  then sum the total to maintain its expressive power\n",
    "  = go-to state-of-the-art most used regularization technique\n",
    "  = simple and inexpensive\n",
    "* make big network act like little one by randomly training little subsections because **little networks don't overfit**:\n",
    "  * small networks don't have much expressive power\n",
    "  * they have room only to capture the big, obvious, high-level features\n",
    "* Difference between learning with big vs small networks   \n",
    "  = difference between molding with very coarse-grained clay vs very fine-grained clay   \n",
    "  -> coarse-grained can only **_average_** the shapes, ignoring fine creases and corners.\n",
    "* dropout = similar to training for a marathon with weights on your legs (harder to train), but taking off the weights for the big race   \n",
    "  you run quite a bit faster because you trained for something that was much more difficult\n",
    "\n",
    "### Example\n",
    "\n",
    "* new dropout mask for each iteration: random matrix of 0s and 1s\n",
    "* `layer_1 *= dropout_mask * 2`: \n",
    "  * if we turn off half the nodes in `layer_1`, the weighted sum in `layer_2` will be cut in half\n",
    "  * thus, `layer_2` would increase its sensitivity to `layer_1` (like leaning closer to a radio when volume too low)\n",
    "  * but at test time, we no longer use dropout (volume back up to normal), this throws off `layer_2`'s ability to listen to `layer_1`\n",
    "  * we counter this by multiplying `layer_1` by `1 / percentage of turned on nodes` (here: 1/0.5, which equals to 2)\n",
    "  * this way, the volume of `layer_1` is the same between training and testing\n",
    "* In the previous example, the network peaked at 89.4% test accuracy then falled down to 77.1% when training finishes   \n",
    "  and it predicted perfectly on training data, but poorly on test data\n",
    "  * with dropout, it peaks at 89.3% then drops to only 83.3% when training peaks   \n",
    "    = doesn't overfit as badly as before\n",
    "  * test accuracy keeps going up and down\n",
    "  * note: dropout slows down training accuracy, which previously went straight to 100%   \n",
    "    twice more iterations (1500 vs 750) and still training not finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0  Train-Err 0.885 Train-Acc 0.289  Test-Err 0.718 Test-Acc 0.570\n",
      "Iter 50  Train-Err 0.462 Train-Acc 0.742  Test-Err 0.430 Test-Acc 0.888\n",
      "Iter 100  Train-Err 0.452 Train-Acc 0.769  Test-Err 0.433 Test-Acc 0.880\n",
      "Iter 150  Train-Err 0.457 Train-Acc 0.783  Test-Err 0.446 Test-Acc 0.870\n",
      "Iter 200  Train-Err 0.442 Train-Acc 0.796  Test-Err 0.436 Test-Acc 0.882\n",
      "Iter 250  Train-Err 0.433 Train-Acc 0.789  Test-Err 0.421 Test-Acc 0.884\n",
      "Iter 300  Train-Err 0.407 Train-Acc 0.804  Test-Err 0.434 Test-Acc 0.875\n",
      "Iter 350  Train-Err 0.418 Train-Acc 0.79  Test-Err 0.426 Test-Acc 0.893\n",
      "Iter 400  Train-Err 0.386 Train-Acc 0.835  Test-Err 0.426 Test-Acc 0.883\n",
      "Iter 450  Train-Err 0.398 Train-Acc 0.823  Test-Err 0.425 Test-Acc 0.881\n",
      "Iter 500  Train-Err 0.380 Train-Acc 0.829  Test-Err 0.424 Test-Acc 0.878\n",
      "Iter 550  Train-Err 0.390 Train-Acc 0.819  Test-Err 0.441 Test-Acc 0.866\n",
      "Iter 600  Train-Err 0.360 Train-Acc 0.84  Test-Err 0.430 Test-Acc 0.865\n",
      "Iter 650  Train-Err 0.369 Train-Acc 0.847  Test-Err 0.438 Test-Acc 0.873\n",
      "Iter 700  Train-Err 0.387 Train-Acc 0.817  Test-Err 0.443 Test-Acc 0.866\n",
      "Iter 750  Train-Err 0.365 Train-Acc 0.845  Test-Err 0.444 Test-Acc 0.859\n",
      "Iter 800  Train-Err 0.351 Train-Acc 0.864  Test-Err 0.455 Test-Acc 0.863\n",
      "Iter 850  Train-Err 0.372 Train-Acc 0.839  Test-Err 0.450 Test-Acc 0.860\n",
      "Iter 900  Train-Err 0.347 Train-Acc 0.846  Test-Err 0.459 Test-Acc 0.853\n",
      "Iter 950  Train-Err 0.349 Train-Acc 0.849  Test-Err 0.455 Test-Acc 0.845\n",
      "Iter 1000  Train-Err 0.350 Train-Acc 0.851  Test-Err 0.467 Test-Acc 0.851\n",
      "Iter 1050  Train-Err 0.335 Train-Acc 0.853  Test-Err 0.488 Test-Acc 0.851\n",
      "Iter 1100  Train-Err 0.353 Train-Acc 0.85  Test-Err 0.469 Test-Acc 0.847\n",
      "Iter 1150  Train-Err 0.359 Train-Acc 0.832  Test-Err 0.479 Test-Acc 0.838\n",
      "Iter 1200  Train-Err 0.341 Train-Acc 0.858  Test-Err 0.472 Test-Acc 0.834\n",
      "Iter 1250  Train-Err 0.349 Train-Acc 0.854  Test-Err 0.474 Test-Acc 0.836\n",
      "Iter 1300  Train-Err 0.337 Train-Acc 0.833  Test-Err 0.481 Test-Acc 0.834\n",
      "Iter 1350  Train-Err 0.346 Train-Acc 0.855  Test-Err 0.487 Test-Acc 0.834\n",
      "Iter 1400  Train-Err 0.338 Train-Acc 0.867  Test-Err 0.490 Test-Acc 0.833\n",
      "Iter 1450  Train-Err 0.318 Train-Acc 0.863  Test-Err 0.508 Test-Acc 0.835\n",
      "Iter 1500  Train-Err 0.334 Train-Acc 0.846  Test-Err 0.502 Test-Acc 0.833\n"
     ]
    }
   ],
   "source": [
    "for j in range(iterations):\n",
    "    \n",
    "    #-- Predicting on training data\n",
    "        \n",
    "    train_error, train_correct_cnt = 0.0, 0\n",
    "    \n",
    "    for i in range(num_train_images):\n",
    "        \n",
    "        target_pred = train_labels[i:i+1]\n",
    "        \n",
    "        layer_0 = train_images[i:i+1]\n",
    "        layer_1 = relu(np.dot(layer_0, weights_0_1))\n",
    "        dropout_mask = np.random.randint(2, size=layer_1.shape) # new dropout mask: random matrix of 0s and 1s\n",
    "        layer_1 *= dropout_mask *2                              # turn off some of layer_1's nodes\n",
    "        layer_2 = np.dot(layer_1, weights_1_2)\n",
    "        \n",
    "        train_error += np.sum((target_pred - layer_2) ** 2)\n",
    "        train_err = train_error / float(num_train_images)\n",
    "        \n",
    "        train_correct_cnt += int( np.argmax(layer_2) == np.argmax(target_pred) )\n",
    "        train_acc = train_correct_cnt / float(num_train_images)\n",
    "        \n",
    "        layer_2_delta = target_pred - layer_2\n",
    "        layer_1_delta = layer_2_delta.dot(weights_1_2.T) * relu2deriv(layer_1)\n",
    "        layer_1_delta *= dropout_mask                             # turn off layer_1's deltas\n",
    "        \n",
    "        weights_1_2 += alpha * layer_1.T.dot(layer_2_delta)\n",
    "        weights_0_1 += alpha * layer_0.T.dot(layer_1_delta)\n",
    "    \n",
    "    #-- Predicting on test data\n",
    "    \n",
    "    if (j % 50 == 0) or (j == iterations - 1):\n",
    "        \n",
    "        test_error, test_error_cnt = 0.0, 0\n",
    "\n",
    "        for k in range(num_test_images):\n",
    "            \n",
    "            target_pred = test_labels[k:k+1]\n",
    "\n",
    "            layer_0 = test_images[k:k+1]\n",
    "            layer_1 = relu(np.dot(layer_0, weights_0_1))\n",
    "            layer_2 = np.dot(layer_1, weights_1_2)\n",
    "\n",
    "            test_error += np.sum((target_pred - layer_2) ** 2)\n",
    "            test_err = test_error / float(num_test_images)\n",
    "            \n",
    "            test_correct_cnt += int( np.argmax(layer_2) == np.argmax(target_pred) )\n",
    "            test_acc = test_correct_cnt / float(num_test_images)\n",
    "\n",
    "        print('Iter ' + str(j) + \\\n",
    "              '  Train-Err ' + str(train_err)[0:5] + ' Train-Acc ' + str(train_acc)[0:5] + \\\n",
    "              '  Test-Err ' + str(test_err)[0:5] + ' Test-Acc ' + str(test_acc)[0:5])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "* subset of methods to help the nn learn true signal and ignore noise\n",
    "* by increasing the difficulty for a model to learn the fine-grained details\n",
    "* a form of training a bunch of networks and averaging them (called **_ensemble learning_**)\n",
    "* since no 2 nns learn the same (see previous chapter), no 2 nns overfit the same:   \n",
    "  = each network inevitably makes different mistakes, resulting in different updates   \n",
    "  = **_each network overfits to different noise_**\n",
    "* all networks **start by learning broad** features (general features)   \n",
    "  before learning about noise (fine-grained features   \n",
    "<br/>  \n",
    "\n",
    "If you train 100 different networks (all initialized randomly):\n",
    "* they will each latch on to **_different noise_**\n",
    "* but **_similar broad signal_**\n",
    "* so by allowing them to vote equally, their noise would tend to **_cancel out_**, revealing only what they all learned in common: **_the signal_**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BATCH GRADIENT DESCENT\n",
    "\n",
    "* **Increasing speed training and the rate of convergence**   \n",
    "<br/>   \n",
    "\n",
    "* training runs much faster: `np.dot` now performs batched dot products (100 at a time)   \n",
    "  CPU architectures are much faster at performing batched dot products\n",
    "* alpha is 20 times larger (0.005 vs 0.001)   \n",
    "  because training takes an average noisy signal (average weight change over 100 training examples), it can take **bigger steps**\n",
    "* batch size: pick numbers randomly until you find a `batch_size`/`alpha` pair that works well   \n",
    "<br/>   \n",
    "\n",
    "* Note: with batched gradient descent, training accuracy has smoother trend than with stochastic GD (1 training example at a time)   \n",
    "  this is an effect of taking an average weight update consistently   \n",
    "  individual training example are very noisy in terms of the weight updates they generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0  Train-Err 82.46 Train-Acc 0.118  Test-Err 0.818 Test-Acc 0.128\n",
      "Iter 50  Train-Err 16.21 Train-Acc 0.218  Test-Err 0.269 Test-Acc 0.289\n",
      "Iter 100  Train-Err 8.870 Train-Acc 0.286  Test-Err 0.187 Test-Acc 0.435\n",
      "Iter 150  Train-Err 10.66 Train-Acc 0.294  Test-Err 0.163 Test-Acc 0.486\n",
      "Iter 200  Train-Err 10.42 Train-Acc 0.288  Test-Err 0.157 Test-Acc 0.506\n",
      "Iter 250  Train-Err 11.97 Train-Acc 0.326  Test-Err 0.160 Test-Acc 0.517\n",
      "Iter 300  Train-Err 11.67 Train-Acc 0.327  Test-Err 0.164 Test-Acc 0.525\n",
      "Iter 350  Train-Err 14.47 Train-Acc 0.33  Test-Err 0.168 Test-Acc 0.530\n",
      "Iter 400  Train-Err 13.74 Train-Acc 0.337  Test-Err 0.174 Test-Acc 0.545\n",
      "Iter 450  Train-Err 13.81 Train-Acc 0.355  Test-Err 0.181 Test-Acc 0.555\n",
      "Iter 500  Train-Err 14.97 Train-Acc 0.376  Test-Err 0.186 Test-Acc 0.564\n",
      "Iter 550  Train-Err 15.76 Train-Acc 0.363  Test-Err 0.199 Test-Acc 0.572\n",
      "Iter 600  Train-Err 16.33 Train-Acc 0.369  Test-Err 0.204 Test-Acc 0.579\n",
      "Iter 650  Train-Err 15.96 Train-Acc 0.369  Test-Err 0.214 Test-Acc 0.585\n",
      "Iter 700  Train-Err 14.81 Train-Acc 0.399  Test-Err 0.218 Test-Acc 0.588\n",
      "Iter 750  Train-Err 16.45 Train-Acc 0.39  Test-Err 0.224 Test-Acc 0.591\n",
      "Iter 800  Train-Err 18.03 Train-Acc 0.37  Test-Err 0.227 Test-Acc 0.594\n",
      "Iter 850  Train-Err 17.57 Train-Acc 0.401  Test-Err 0.229 Test-Acc 0.596\n",
      "Iter 900  Train-Err 15.55 Train-Acc 0.4  Test-Err 0.231 Test-Acc 0.597\n",
      "Iter 950  Train-Err 17.13 Train-Acc 0.385  Test-Err 0.232 Test-Acc 0.599\n",
      "Iter 1000  Train-Err 15.20 Train-Acc 0.423  Test-Err 0.235 Test-Acc 0.602\n",
      "Iter 1050  Train-Err 17.12 Train-Acc 0.408  Test-Err 0.234 Test-Acc 0.602\n",
      "Iter 1100  Train-Err 18.46 Train-Acc 0.399  Test-Err 0.235 Test-Acc 0.603\n",
      "Iter 1150  Train-Err 16.77 Train-Acc 0.426  Test-Err 0.235 Test-Acc 0.612\n",
      "Iter 1200  Train-Err 18.57 Train-Acc 0.402  Test-Err 0.235 Test-Acc 0.613\n",
      "Iter 1250  Train-Err 16.91 Train-Acc 0.419  Test-Err 0.235 Test-Acc 0.618\n",
      "Iter 1300  Train-Err 19.91 Train-Acc 0.396  Test-Err 0.237 Test-Acc 0.621\n",
      "Iter 1350  Train-Err 17.59 Train-Acc 0.41  Test-Err 0.239 Test-Acc 0.628\n",
      "Iter 1400  Train-Err 16.96 Train-Acc 0.448  Test-Err 0.237 Test-Acc 0.635\n",
      "Iter 1450  Train-Err 19.23 Train-Acc 0.414  Test-Err 0.239 Test-Acc 0.630\n",
      "Iter 1500  Train-Err 18.66 Train-Acc 0.425  Test-Err 0.239 Test-Acc 0.627\n",
      "Iter 1550  Train-Err 17.02 Train-Acc 0.426  Test-Err 0.241 Test-Acc 0.640\n",
      "Iter 1600  Train-Err 18.13 Train-Acc 0.417  Test-Err 0.240 Test-Acc 0.630\n",
      "Iter 1650  Train-Err 18.28 Train-Acc 0.424  Test-Err 0.240 Test-Acc 0.645\n",
      "Iter 1700  Train-Err 17.60 Train-Acc 0.416  Test-Err 0.241 Test-Acc 0.632\n",
      "Iter 1750  Train-Err 18.16 Train-Acc 0.431  Test-Err 0.238 Test-Acc 0.634\n",
      "Iter 1800  Train-Err 19.58 Train-Acc 0.41  Test-Err 0.241 Test-Acc 0.634\n",
      "Iter 1850  Train-Err 17.08 Train-Acc 0.424  Test-Err 0.241 Test-Acc 0.635\n",
      "Iter 1900  Train-Err 19.68 Train-Acc 0.42  Test-Err 0.238 Test-Acc 0.641\n",
      "Iter 1950  Train-Err 20.42 Train-Acc 0.425  Test-Err 0.240 Test-Acc 0.632\n",
      "Iter 2000  Train-Err 20.97 Train-Acc 0.419  Test-Err 0.239 Test-Acc 0.654\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(1)\n",
    "relu = lambda x: (x >= 0) * x  # x if x>0, 0 otherwise\n",
    "relu2deriv = lambda x: x >= 0  # 1 if input>0, 0 otherwise\n",
    "\n",
    "alpha = 0.005\n",
    "iterations = 2001\n",
    "hidden_size = 10\n",
    "batch_size = 100\n",
    "\n",
    "pixels_per_image = 784\n",
    "num_labels = 10\n",
    "num_train_images = len(train_images)\n",
    "num_test_images = len(test_images)\n",
    "\n",
    "weights_0_1 = 0.2 * np.random.random((pixels_per_image, hidden_size)) - 0.1\n",
    "weights_1_2 = 0.2 * np.random.random((hidden_size, num_labels)) - 0.1\n",
    "\n",
    "for j in range(iterations):\n",
    "    \n",
    "    train_error, train_correct_cnt = (0.0, 0)\n",
    "    \n",
    "    for i in range(num_train_images // batch_size):\n",
    "        \n",
    "        batch_start, batch_end = (i * batch_size), ((i+1) * batch_size)\n",
    "                \n",
    "        layer_0 = train_images[batch_start:batch_end]\n",
    "        layer_1 = relu(np.dot(layer_0, weights_0_1))\n",
    "        dropout_mask = np.random.randint(2, size=layer_1.shape)\n",
    "        layer_1 *= dropout_mask * 2\n",
    "        layer_2 = np.dot(layer_1, weights_1_2)\n",
    "        \n",
    "        train_error += np.sum(train_labels[batch_start:batch_end] - layer_2) ** 2\n",
    "        train_err = train_error / float(num_train_images)\n",
    "        \n",
    "        for k in range(batch_size):            \n",
    "            train_correct_cnt += int( np.argmax(layer_2[k:k+1]) == \\\n",
    "                                      np.argmax(train_labels[batch_start+k:batch_start+k+1]) )\n",
    "        train_acc = train_correct_cnt / float(num_train_images)\n",
    "            \n",
    "        layer_2_delta = (train_labels[batch_start:batch_end] - layer_2) / batch_size\n",
    "        layer_1_delta = layer_2_delta.dot(weights_1_2.T) * relu2deriv(layer_1)\n",
    "        layer_1_delta *= dropout_mask\n",
    "            \n",
    "        weights_1_2 += alpha * layer_1.T.dot(layer_2_delta)\n",
    "        weights_0_1 += alpha * layer_0.T.dot(layer_1_delta)\n",
    "        \n",
    "    \n",
    "    if (j % 50 == 0):\n",
    "        test_error, test_correct_cnt = (0.0, 0)\n",
    "        \n",
    "        for m in range(num_test_images):            \n",
    "            layer_0 = test_images[m:m+1]\n",
    "            layer_1 = relu(np.dot(layer_0, weights_0_1))\n",
    "            layer_2 = np.dot(layer_1, weights_1_2)\n",
    "            \n",
    "            test_error += np.sum(test_labels[m:m+1] - layer_2) ** 2\n",
    "            test_err = test_error / float(num_test_images)\n",
    "            \n",
    "            test_correct_cnt += int( np.argmax(layer_2) == np.argmax(test_labels[m:m+1]) )\n",
    "            test_acc = test_correct_cnt / float(num_test_images)\n",
    "\n",
    "        print('Iter ' + str(j) + \\\n",
    "              '  Train-Err ' + str(train_err)[0:5] + ' Train-Acc ' + str(train_acc)[0:5] + \\\n",
    "              '  Test-Err ' + str(test_err)[0:5] + ' Test-Acc ' + str(test_acc)[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
