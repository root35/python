{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural networks\n",
    "\n",
    "* NNs find and create correlation.  \n",
    "<br/>  \n",
    "* NNs seek to find direct and indirect correlation between input layer and output layer.  \n",
    "<br/>  \n",
    "* NNs are just a series of matrices connected by layers.  \n",
    "<br/>  \n",
    "* NNs always start out randomly:\n",
    "  * so every network learns differently (different weight configs, iterations, speed...)\n",
    "  * so every network overfits differently, to diffenret noise (see next chapter)   \n",
    "<br/>  \n",
    "* NNs learn by trial and error (after starting randomly)  \n",
    "<br/>  \n",
    "* NNs always start by learning the broader features (general features)   \n",
    "  before learning about noise (fine-grained features  \n",
    "<br/>  \n",
    "* If you train 100 different networks (all initialized randomly):\n",
    "  * they will each latch on to **_different noise_**\n",
    "  * but **_similar broad signal_**  \n",
    "<br/>  \n",
    "\n",
    "<p style=\"background:#DDEEEE;padding: 15px;\">\n",
    "<b>Learning</b> = Adjust each weight in the <b>correct direction</b> by the <b>correct amount</b> so `error` reduces to 0\n",
    "</p>\n",
    "\n",
    "### Local correlation\n",
    "\n",
    "A set of weights optimizes to learn how to correlate its input layer with what the output layer says it should be.  \n",
    "\n",
    "### Global correlation\n",
    "\n",
    "* What an earlier layer says it should be can be determined by taking what a later layer says it should be and multiplying it by the weights in between them\n",
    "* This way, later layers can tell earlier layers what kind of signal they need, to ultimately find correlation with the output. \"Hey, need higher signal!\" or \"Hey, need lower signal!\"\n",
    "* This *crosscommunication* is called **backpropagation**.  \n",
    "<br/>  \n",
    "\n",
    "* `weights` are matrices, `layers` are vectors.   \n",
    "* dimensions of matrices depend on the previous and next layers:\n",
    "  * `layer_1` = nb. features * nb. labels\n",
    "  * last layer = 1 * nb. labels      \n",
    "<br/>     \n",
    "\n",
    "* The entire neural network from *chapter 4* can be represented in one expression: \n",
    "```\n",
    "layer_2 = relu(layer_0.dot(weights_0_1)).dot(weights_1_2)\n",
    "```\n",
    "> $l2 = relu(l0 W0) W1$   \n",
    "\n",
    "where $l0$ is `layer_0`, $W0$ is `weights_0_1` (the first weights matrix), and $l2$ is the nn's output.\n",
    "<br/>    \n",
    "\n",
    "### Architecture\n",
    "\n",
    "* The configuration of layers between the first and last layers have a strong impact on whether the network is successful in finding correlation, and/or how fast if finds it.   \n",
    "* Much of the research in NNs is about finding new architectures that can find correlation faster and generalize better to unseen data.  \n",
    "* Create **precisely focused** architectures:\n",
    "  * that maximize the network's ability to focus on the areas where meaningful correlation exists\n",
    "  * and minimize the network's ability to focus on the areas that contain noise.\n",
    "\n",
    "<p style=\"background:#DDEEEE;padding: 15px;\">\n",
    "Good architectures <b>channel signal so that correlation is easy to discover</b>.   \n",
    "<br/>    \n",
    "Great architectures also <b>filter noise to help prevent overfitting</b>.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
