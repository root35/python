{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes from:** Duygu ALTINOK - *Mastering spaCy*   \n",
    "\n",
    "[Section 1](#section1): Getting started with spaCy\n",
    "\n",
    "- [Chapter 1](#chapter1) – Getting started with spaCy\n",
    "- [Chapter 2](#chapter2) – Core operations with spaCy\n",
    "\n",
    "[Section 2](#section2): spaCy features\n",
    "\n",
    "- [Chapter 3](#chapter3) – Linguistic features\n",
    "- [Chapter 4](#chapter4) – Rule-based matching\n",
    "- [Chapter 5](#chapter5) – Word Vectors and Semantic Similarity\n",
    "- [Chapter 6](#chapter6) – Putting everything toghether: Semantic Parsing with spaCy\n",
    "\n",
    "[Section 3](#section3): Machine Learning with spaCy\n",
    "\n",
    "- [Chapter 7](#chapter7) – Customizing spaCy models\n",
    "- [Chapter 8](#chapter8) – Text Classification with spaCy\n",
    "- [Chapter 9](#chapter9) – spaCy and transformers\n",
    "- [Chapter 10](#chapter10) – Putting everything together: Designing your chatbot with spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a class=\"anchor\" id=\"section1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:13px; font-weight:bold; background:#eebbcc;padding: 15px;\">SECTION 1 – Getting started with spaCy</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"chapter1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:34px; font-weight:bold; background:#DDEEEE;padding: 15px;\">Chapter 1 – Getting started with spaCy</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chatbots:\n",
    "- Entity Extraction\n",
    "- Intent recognition\n",
    "- Context handling\n",
    "- (= text classification)\n",
    "\n",
    "Entity Linking = \n",
    "- 'Diana Spence' <-> 'Lady Diana'\n",
    "- Semantic relations + General knowledge (See: Semantic Web on Wikipedia)\n",
    "- available in Spacy\n",
    "\n",
    "Spacy and Deep Learning:\n",
    "- ML library `Thinc`\n",
    "- Wrappers for: PyTorch, TensorFlow, MXNet, and Hugging Face transformers\n",
    "- **46** state-of-the-art models for **16** languages\n",
    "\n",
    "Pretrained models:\n",
    "- `fr_core_web_sm`\n",
    "- `fr`: language code\n",
    "- `core`: model capability\n",
    "- `web`: corpus type (`news`, `twitter`...)\n",
    "- `sm`: model size, large `lg`, medium `md`, small `sm`\n",
    "- **Important**: match model genre to your text type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install spacy\n",
    "\n",
    "# if you have multiple Python versions \n",
    "# and you want to use spacy with a specific version\n",
    "pip3.5 install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check your spacy version\n",
    "python -m spacy info\n",
    "\n",
    "# upgrade spacy\n",
    "pip install -U spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install with conda\n",
    "conda install -c conda-forge spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install spacy on macOS/OS X\n",
    "# - install Xcode IDE\n",
    "# - then install command-line development tools:\n",
    "xcode-select -install\n",
    "# - then install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install other Python modules:\n",
    "! pip install numpy\n",
    "! pip install scikit-learn\n",
    "! pip install matplotlib\n",
    "! pip install pandas\n",
    "\n",
    "! pip install tensorflow   # TensorFlow >= 2.2.0\n",
    "\n",
    "# Install Jupyter notebooks: https://jupyter.org/install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install language models\n",
    "\n",
    "Spacy's `download` command: \n",
    "- selects and downloads the most compatible version of this model for you local spacy version\n",
    "- deploys `pip` behind the scenes,    \n",
    "  and `pip` installs the package and places it in your `site-packages` directory (just like any other Python package)\n",
    "\n",
    "See book (p. 22, zoom lecture 33%): download a model via `pip` and `import` it as a module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python -m spacy download fr_core_web_md\n",
    "\n",
    "# download a specific version\n",
    "python -m spacy download fr_core_web_md-2.0.0 --direct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "nlp = spacy.load('fr_core_web_md')  # then load the package\n",
    "doc = nlp('Hello world')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization tool\n",
    "\n",
    "See interactive demos:\n",
    "- POS tags + Syntactic dependencies: https://explosion.ai/demos/displacy\n",
    "- Named entities: https://explosion.ai/demos/displacy-ent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "nlp = spacy.load('fr_core_web_md')\n",
    "doc = nlp('Hello world')\n",
    "\n",
    "# start the displacy web server:\n",
    "displacy.serve(doc, style='dep')\n",
    "\n",
    "# response with a link: http://0.0.0.0:5000\n",
    "# = local address where displacy renders your graphics\n",
    "# see p. 28 how to use another port\n",
    "# click the link and navigate to the web page (localhost)\n",
    "# Ctrl+C to shut down the displacy server and go back to Python shell\n",
    "\n",
    "# style=ent for named entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see book p. 29:\n",
    "#   displacy in Jupyter Notebook\n",
    "#   displacy.render(doc, style='dep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see book p. 30:\n",
    "#   export displacy renders in image files with python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install other tools\n",
    "\n",
    "- Data annotation tools\n",
    "  - Prodigy: https://prodi.gy/demo\n",
    "    - annotate entities\n",
    "  - Brat: https://brat.nlplab.org/introduction.html\n",
    "    - demo website: https://brat.nlplab.org/examples.html   \n",
    "    - Brat can also annotate relations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a class=\"anchor\" id=\"chapter2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:34px; font-weight:bold; background:#DDEEEE;padding: 15px;\">Chapter 2 – Core operations with spaCy</h1>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing steps\n",
    "\n",
    "1. We create a spaCy pipeline object: `Language`, output of `spacy.load`\n",
    "  \n",
    "  \n",
    "2. We apply the `nlp` pipeline on a text:\n",
    "  - tokenizer -> `Doc` tagger -> `Doc` - parser -> `Doc` -> entity recognizer -> `Doc`\n",
    "  - each component returns a `Doc` and passes it to the next component   \n",
    "  <img src=\"images/Capture%20d%E2%80%99e%CC%81cran%202022-07-12%20a%CC%80%2020.42.08.png\" width=\"600\"> \n",
    "\n",
    "### Components\n",
    "\n",
    "Each correspond to a spaCy class   \n",
    "  <img src=\"images/Capture%20d%E2%80%99e%CC%81cran%202022-07-12%20a%CC%80%2020.48.22.png\" width=\"600\">\n",
    "\n",
    "### Containers\n",
    "\n",
    "Classes contain information about text data   \n",
    "  <img src=\"images/Capture%20d%E2%80%99e%CC%81cran%202022-07-12%20a%CC%80%2020.51.04.png\" width=\"600\">\n",
    "  <img src=\"images/Capture%20d%E2%80%99e%CC%81cran%202022-07-12%20a%CC%80%2020.52.39.png\" width=\"600\">\n",
    "\n",
    "### Global architecture\n",
    "\n",
    "- Processing pipeline **Components** (actions)\n",
    "- Data **Containers** (inputs and outputs)\n",
    "  <img src=\"images/Capture%20d%E2%80%99e%CC%81cran%202022-07-12%20a%CC%80%2020.54.18.png\" width=\"600\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('fr_core_web_md')\n",
    "doc = nlp('Bonjour le monde !')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Tokenizer\n",
    "  - [`Token`](https://spacy.io/api/token) -> `[token.text for token in doc]`\n",
    "  - Based on language-specific rules\n",
    "  - Can be customized\n",
    "  - Debugging the tokenizer\n",
    "  \n",
    "  \n",
    "2. Sentence segmentation\n",
    "  - `doc.sents` -> `Token.is_sent_start`\n",
    "  - Done by the dependency parser\n",
    "  \n",
    "  \n",
    "3. Lemmatization\n",
    "  - `token.lemma_`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Container classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[`Doc`](https://spacy.io/api/doc)**\n",
    "  - `doc.text`: Unicode representation of the text\n",
    "  - `doc.sents`: sentences\n",
    "  - `doc.ents`: named entities\n",
    "  - `doc.lang`: language id  \n",
    "    `doc.lang_`: language Unicode string\n",
    "  \n",
    "  \n",
    "**[`Token`](https://spacy.io/api/token)**\n",
    "  - `token.is_sent_start`: sentence start\n",
    "  - `token.is_stop`: is a stop word\n",
    "  - `token.lemma` + `token.lemma_`\n",
    "  - `token.ent_type_`\n",
    "  - `token.dep_` + `token.head_`\n",
    "  - `token.is_oov`: out of vocabulary\n",
    "  \n",
    "  \n",
    "**[`Span`](https://spacy.io/api/span)**\n",
    "\n",
    "Use: `dir(doc)`, `dir(token)`, `dir(span)` to see all object attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a class=\"anchor\" id=\"section2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:13px; font-weight:bold; background:#eebbcc;padding: 15px;\">SECTION 2 – spaCy features</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a class=\"anchor\" id=\"chapter3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:34px; font-weight:bold; background:#DDEEEE;padding: 15px;\">Chapter 3 – Linguistic features</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `token.pos` (int)   \n",
    "  `token.pos_`: Unicode universal tags\n",
    "- `token.tag` (int)   \n",
    "  `token.tag_`: fine-grained tags\n",
    "- `spacy.explain(tag_name)`\n",
    "- `lang/<language_code>/tag_map.py` under each language submodule\n",
    "  \n",
    "  \n",
    "- Verb, Noun, Pronoun, Determiner, Adjective, Adverb, Preposition, Conjunction, Interjection\n",
    "- Same language can support different tagsets\n",
    "- See:\n",
    "  - http://partofspeech.org/,\n",
    "  - [The eight parts of speech](http://www.butte.edu/departments/cas/tipsheets/grammar/parts_of_speech.html)\n",
    "  \n",
    "  \n",
    "- POS taggers: sequential models, **Seq2seq**, spaCy uses an **LSTM** variation  \n",
    "  (see state-of-the-art of POS Tagging on ACL website)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Sense Disambiguation (WSD)\n",
    "\n",
    "- Sometimes, POS tags can help identify a specific sense of a word\n",
    "- Example: \n",
    "  - beat – strike someone – V\n",
    "  - beat – defeat someone – V\n",
    "  - beat – rythm in music and poetry – N\n",
    "  - beat – bird wing movement – N\n",
    "  - beat – completely exhausted – ADVJ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural Language Understanding (NLU)\n",
    "\n",
    "- Verb **tense** and **aspect** can help identify intent\n",
    "- Example:\n",
    "  - I flew to Rome 3 days ago. I still didn't get the bill, please send it ASAP.\n",
    "  - I need to fly to Rome\n",
    "  - I will fly to Rome next week. Check availabilities please.\n",
    "  - I'm flying to Rome next week. Check flights on next Tuesday."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numbers, Symboles and Punctuations\n",
    "\n",
    "- `NUM`, `SYM`, `PUNCT` (cf. book p. 78 fine-grained punctuation tags)\n",
    "- Can be used in rule-based matching to recognize financial info..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependency Parsing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `ROOT`: sentence head\n",
    "- `token.dep_`: dependency label in Unicode / `token.dep`: int\n",
    "- `token.head_`: syntactic head\n",
    "- `token.children`: syntactic subtree\n",
    "  \n",
    "  \n",
    "- spaCy's English dependency labels:\n",
    "  <img src=\"images/Capture%20d%E2%80%99e%CC%81cran%202022-07-12%20a%CC%80%2023.03.06.png\" width=\"450\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- One of the key components for understanding text topic (financial, medical, movies...)   \n",
    "  named entities usually belong to a **semantic category**   \n",
    "  Example: *BTS* = music, *Salma Hayek* = movies\n",
    "  \n",
    "  \n",
    "- `doc.ents`: list of `Span` objects\n",
    "- `token.ent_type` (int)   \n",
    "  `token.ent_type_` (Unicode string) (empty if token is not an entity)   \n",
    "  `spacy.explain(token.ent_type_)`\n",
    "  <img src=\"images/Capture%20d%E2%80%99e%CC%81cran%202022-07-12%20a%CC%80%2023.08.37.png\" width=\"450\">\n",
    "  \n",
    "  \n",
    "- State of the art:\n",
    "  - First modern NER tagger was a CRF (Conditional Random Field)   \n",
    "    See implementation details [here](https://homepages.inf.ed.ac.uk/csutton/publications/crftutv2.pdf.)\n",
    "  - Current state-of-the art: LSTM (+ CRF)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging and splitting NERs\n",
    "\n",
    "- multiword expressions, multiword named entities, typos\n",
    "- `doc.retokenize`: tool for merging and splitting the spans\n",
    "  - See p. 101, zoom lecture 33%\n",
    "  - assigning new linguistic features to the merge/split spans (syntactic, pos...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a class=\"anchor\" id=\"chapter4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:34px; font-weight:bold; background:#DDEEEE;padding: 15px;\">Chapter 4 – Rule-based matching</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Entities **specific to your domain** (times, dates, phone numbers, IBAN, account numbers...)\n",
    "  \n",
    "  \n",
    "- Can be recognized with rules   \n",
    "  without having to train statistical models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Token-based matching"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `Matcher` object:\n",
    "  - Based on morphological features, POS tags, regex and other spaCy features\n",
    "  - Can be applied to `Doc` and `Span`\n",
    "  - Attributes the `Matcher` recognizes:    \n",
    "    Examples for each in the book!\n",
    "    <img src=\"images/Capture%20d%E2%80%99e%CC%81cran%202022-07-12%20a%CC%80%2023.28.36.png\" width=\"700\">\n",
    "  - Extended syntax:\n",
    "    <img src=\"images/Capture%20d%E2%80%99e%CC%81cran%202022-07-12%20a%CC%80%2023.31.46.png\" width=\"650\">\n",
    "  - Regex-like operators:\n",
    "    <img src=\"images/Capture%20d%E2%80%99e%CC%81cran%202022-07-12%20a%CC%80%2023.33.08.png\" width=\"400\">\n",
    "  - Regular expressions support: [regex101](https://regex101.com)\n",
    "    <img src=\"images/Capture%20d%E2%80%99e%CC%81cran%202022-07-12%20a%CC%80%2023.36.11.png\" width=\"400\">\n",
    "\n",
    "\n",
    "- spaCy's `Matcher` [demo page](https://explosion.ai/demos/matcher):  \n",
    "  <img src=\"images/Capture%20d%E2%80%99e%CC%81cran%202022-07-12%20a%CC%80%2023.36.59.png\" width=\"500\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `PhraseMatcher`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `PhraseMatcher` class scans long dictionaries\n",
    "- When you have a long list of domain-specific phrases (legal, medical...)\n",
    "- `Matcher` not handy: too manual\n",
    "- See usage p. 123 (zoom lecture 42%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `EntityRuler`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Component used to add rules on top of the statistical model   \n",
    "  = even more powerful NER model\n",
    "- it's not a `matcher`, it's a pipeline component: `nlp.add_pipe`\n",
    "- appends its matches to `doc.ents`\n",
    "- see usage in the book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining spaCy models and matchers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples of NER extraction models:\n",
    "- IBAN and account numbers\n",
    "- Phone numbers\n",
    "- Mentions in online comments\n",
    "- Hashtags and emojis\n",
    "- Expanding NERs (add title to person names...)\n",
    "- Combining linguistic features (POS tags, dependencies) and named entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a class=\"anchor\" id=\"chapter5\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:34px; font-weight:bold; background:#DDEEEE;padding: 15px;\">Chapter 5 – Word Vectors and Semantic Similarity</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Similar words occur in similar contexts*\n",
    "\n",
    "- **distributional semantics**: information about the context of the target word\n",
    "- **semantic similarity** methods   \n",
    "  word vector computations: distance calculation, analogy calculation, visualization\n",
    "- **text vectorization**\n",
    "  \n",
    "  \n",
    "- Technical requirements: `NumPy`, `scikit-learn`, `Matplotlib`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sentence can be represented as a `(N, V)` matrix (`N` number of words, `V` vocabulary size)\n",
    "  \n",
    "  \n",
    "- Simplest possible implementation\n",
    "  - **one-hot encoding**\n",
    "  - very sparse vectors: only one '1' and all others are '0's\n",
    "  - solution: see next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word vectors\n",
    "\n",
    "- fixed-size, dense, real-valued\n",
    "- vector = learned representation of the text    \n",
    "    semantically similar words have similar vectors\n",
    "- = **distributional semantics**\n",
    "- ex: Glove\n",
    "- [word vector visualizer at TensorFlow](https://projector.tensorflow.org/)\n",
    "  \n",
    "  \n",
    "- can capture synonyms, antonyms, semantic categories (animals, places, plantes...)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analogies and vector operations\n",
    "\n",
    "- addition, subtraction\n",
    "- word analogy = semantic relationship between pairs of words (synonyms, antonyms, wholepart relations)\n",
    "  - ex: Queen - woman VS King - man\n",
    "  \n",
    "- Example operation: `king - man + woman = queen`\n",
    "  - subtract `man` and add `woman`\n",
    "  \n",
    "<img src=\"images/Capture%20d%E2%80%99e%CC%81cran%202022-07-13%20a%CC%80%2001.44.57.png\" width=\"500\">\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How word vectors are produced\n",
    "\n",
    "Most popular pre-trained vectors and how they are produced:\n",
    "\n",
    "- **word2vec**: Google\n",
    "  - download [here](https://developer.syn.co.in/tutorial/bot/oscova/pretrained-vectors.html#word2vec-and-glove-models)\n",
    "  - read details about [algorithm and data preparation steps](https://jalammar.github.io/illustrated-word2vec/)\n",
    "  \n",
    "  \n",
    "- **Glove**: Stanford NLP group\n",
    "  - depends on Singular Value Decomposition (SVD) applied to word co-occurrences matrix\n",
    "  - [comprehensive guide](https://www.youtube.com/watch?v=Fn_U2OG1uqI)\n",
    "  - [download pretrained vectors](https://nlp.stanford.edu/projects/glove/)\n",
    "  \n",
    "  \n",
    "- **FastText**: Facebook Research\n",
    "  - similar to word2vec but offers more: also predicts vectors for each subword\n",
    "    more robust for mispelled words, rare words, words not in a proper lexicon\n",
    "  - 157 languages\n",
    "  - [download](https://fasttext.cc/docs/en/crawl-vectors.html)\n",
    "  \n",
    "  \n",
    "Most trained on **huge corpus** such as Wikipedia, news or Twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spaCy's pretrained vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Part of many of spaCy's language models\n",
    "- `sm` (small) models: \n",
    "  - no word vectors included, context-sensitive tensors instead\n",
    "  - can still make semantic calculations, but not as accurate as word vector computations\n",
    "  \n",
    "  \n",
    "- `token.vector`:\n",
    "  - returns a NumPy `ndarray`\n",
    "  - can be used with NumPy methods\n",
    "  \n",
    "  \n",
    "- `doc.vector` & `span.vector`:\n",
    "  - vector is an average of its word vectors\n",
    "  - `doc[1:3].vector`\n",
    "  \n",
    "  \n",
    "- Only words in the model's vocabulary have vectors: `token.has_vector` VS `token.is_oov`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `similarity` method\n",
    "\n",
    "- In spaCy, every Container object has a `similarity()` method\n",
    "- to calculate semantic similarity with other Container objects\n",
    "- by comparing their word vectors\n",
    "- even if different types of Containers\n",
    "  - ex: compare `Token` and `Doc`, `Doc` and `Span`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = nlp('I visited England.')\n",
    "doc2 = nlp('I went to London.')\n",
    "\n",
    "doc1[1:3].similarity(doc2[1:4])    # compare Span objects\n",
    "doc1[2].similarity(doc2[3])        # compare Token objects\n",
    "\n",
    "doc1.similarity(doc1)              # compare object to itself -> returns 1.0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Visualization** of words similarity:\n",
    "  - See code with `matplotlib` (p. 152, zoom lecture 49%)\n",
    "  - 2 word groups\n",
    "\n",
    "<img src=\"images/Capture%20d%E2%80%99e%CC%81cran%202022-07-13%20a%CC%80%2002.13.53.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using third party word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Import a third-party vector package into spaCy\n",
    "- See book: example with `fastText`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced semantic similarity methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding semantic similarity\n",
    "\n",
    "- What do the similarity scores mean?\n",
    "- Differences between distance metrics: **Euclidean** VS **Cosine**\n",
    "  - Vector orientation\n",
    "  - Vector magnitude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorizing text using semantic similarity\n",
    "\n",
    "- Categorize into different topics, categories\n",
    "- Or spot relevant texts\n",
    "  \n",
    "  \n",
    "- Example: \n",
    "  - eCommerce, search comments about 'perfume'\n",
    "  - compare 'perfume' vector with comment vectors\n",
    "  - problem: texts can be very long\n",
    "  - **solution**: extract key phrases in the sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: Extracting key phrases\n",
    "\n",
    "- Extract only important words and phrases\n",
    "- And compare them to the search key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: Extracting and comparing named entities\n",
    "\n",
    "- Extract only proper nouns\n",
    "- Can help determine sentence topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a class=\"anchor\" id=\"chapter6\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:34px; font-weight:bold; background:#DDEEEE;padding: 15px;\">Chapter 6 – Putting everything together: Semantic Parsing with spaCy</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLU system:\n",
    "  \n",
    "  \n",
    "1. NER: \n",
    "  - method 1: spaCy Matcher\n",
    "  - method 2: walking the dependency tree\n",
    "  \n",
    "  \n",
    "2. Determine intent: based on syntactic relations\n",
    "  - method 1: extract verb and direct objects\n",
    "  - method 2: walking the dependency tree, recognizing multiple intents\n",
    "  \n",
    "  \n",
    "3. Keywords matching: semantic similarity\n",
    "  - method 1: compare keywords to synonyms list, to detect semantic similarity\n",
    "  - method 2: compare keywords using vector-based semantic similarity methods\n",
    "  \n",
    "  \n",
    "- Technical requirements: `pandas`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting named entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get to know your corpus\n",
    "\n",
    "- Named Entities play a key role in understanding a user utterance\n",
    "  \n",
    "  \n",
    "- **Your corpus**:\n",
    "  - What kind of utterances? (long texts, short)\n",
    "  - What kinds of entities? (times and dates, people names, city, country names, organizations...)\n",
    "  - How is punctuation? (correctly punctuated, no punctuation...)\n",
    "  - How are grammatical rules followed? (capitalization correct, misspelled words, correct syntax)\n",
    "  - More: number of utterances per intent\n",
    "  - See book for code example\n",
    "  \n",
    "  \n",
    "- Integrate observations from the corpus into your code\n",
    "  \n",
    "  \n",
    "- **Example:** ATIS dataset (benchmark dataset for intent classification):\n",
    "  - book a flight\n",
    "  - get info about a flight: flight costs, destinations, timetables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract entities with `Matcher`\n",
    "\n",
    "- See book for code example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract entities using the dependency tree\n",
    "\n",
    "- example: 'to' + GPE\n",
    "- See book for code example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using dependency relations for intent recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Each intent = VERB + OBJECT\n",
    "- E.g. *book flight, purchase meal*\n",
    "  \n",
    "  \n",
    "1. Extract transitive verbs and their direct objects from utterances\n",
    "2. Detect intent:\n",
    "  - by finding synonyms of verbs and their direct objects\n",
    "    - e.g. *book a flight*\n",
    "  - by using wordlists: sometimes intent info not just in the verb phras\n",
    "    - e.g. *make a reservation for a flight*, *would like to*, *need to*\n",
    "  - by using semantic similarity methods\n",
    "  - multiple intent: conjunctions, more than one verb in the sentence\n",
    "  \n",
    "  \n",
    "`findFlight`, `bookFlight`, `cancelFlight`, `bookMeal`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic similarity methods for semantic parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Users use a fairly wide set of phrases and expressions for each intent\n",
    "\n",
    "\n",
    "- Chatbot building platforms:\n",
    "  - RASA (https://rasa.com/) \n",
    "  - Dialogflow (https://dialogflow.cloud.google.com/)\n",
    "  \n",
    "  \n",
    "- 2 ways to recognize semantic similarity:\n",
    "  - with a synonyms dictionary for semantic similarity\n",
    "  - with word-vector based semantic similarity methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using synonyms list for semantic similarity\n",
    "\n",
    "- Semantic groups: different verbs express the same action\n",
    "  - E.g. landing, arriving, flying to / departing, leaving, flying from\n",
    "  - book, make a reservation, buy, reserve\n",
    "   \n",
    "   \n",
    "- In most cases: intent = TRANSITIVE VERB + DIRECT OBJECT\n",
    "  - Check if verb and nouns are synonyms between 2 utterances\n",
    "  \n",
    "  \n",
    "- Each synonym sets = **synset** = set of synonyms for our domain\n",
    " - language general synonyms (e.g. *plane*, *ariplane*)\n",
    " - domain-specific synonyms (e.g. *buy*, *book*)\n",
    " \n",
    " \n",
    "- Synonym lists are very handy for very specific domains   \n",
    "  - synonyms list fairly small\n",
    "  - but can become inefficient for big synsets   \n",
    "    have to look up the whole table each time\n",
    "    \n",
    "    \n",
    "**SEE:** book for example code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using word vectors to recognize semantic similarity\n",
    "\n",
    "- synonym lists are very efficient for very specific domains   \n",
    "  synonyms list fairly small\n",
    "- calculate similarity between verb's vectors\n",
    "  \n",
    "  \n",
    "**SEE:** book for example code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SEE:** book for example code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a class=\"anchor\" id=\"section3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:13px; font-weight:bold; background:#eebbcc;padding: 15px;\">SECTION 3 – Machine Learning with spaCy</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a class=\"anchor\" id=\"chapter7\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:34px; font-weight:bold; background:#DDEEEE;padding: 15px;\">Chapter 7 – Customizing spaCy models</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- train, store and use custom pipeline components\n",
    "- when we should perform custom model training\n",
    "  \n",
    "  \n",
    "#### Fundamental steps of model training:\n",
    "1. **Collect** data\n",
    "  \n",
    "  \n",
    "2. **Preprocess** data into a format recognized by spaCy\n",
    "  \n",
    "  \n",
    "3. **Label** your own data   \n",
    "   using a data annotation tool **Prodigy**\n",
    "   \n",
    "   \n",
    "4. **Model**: build a statistical component:\n",
    "   - either update an existing statistical pipeline component with your own data    \n",
    "     an example with spaCy's NER component\n",
    "   - or create a statistical pipeline component from scratch,   \n",
    "     with your own data and labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started with data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Customize statistical models for our own custom domain and data\n",
    "- spaCy models are good for general NLP: understanding sentence syntax, extracting some entities\n",
    "- but the pretrained models didn't see some very specific domains during training\n",
    "  \n",
    "  \n",
    "- Examples: \n",
    "  - Twitter texts, with hashtags, mentions and emoticons, phrases instead of full sentences\n",
    "    - spaCy's POS tagger wouldn't perform as well, since trained of grammatically correct sentences\n",
    "  - Medical domain\n",
    "    - many domain-specific named entities (drugs, diseases, chemical compound names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When should you do custom training?\n",
    "  - Do spaCy models perform well enough on your data?\n",
    "  - Does your domain include many labels that are absent in spaCy models?\n",
    "  - Is there a pre-trained model/application in GitHub or elsewhere already?   \n",
    "    (don't reinvent the wheel!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do spaCy models perform well enough on your data?\n",
    "\n",
    "No need for new model from scratch if:\n",
    "- above **0.75** accuracy?   \n",
    "  -> train the NER model further to recognize entities as you want\n",
    "- you need 1 or 2 new entity types?   \n",
    "  -> add them to the model with `EntityRuler`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Does your domain include many new labels?\n",
    "\n",
    "- Ex: medical domain, very specialized and long list of entities   \n",
    "  -> custom model training: update existing one or new from scratch\n",
    "  \n",
    "  \n",
    "1. Collect data:\n",
    "  - how much you need:\n",
    "    - depends on complexity of task and domain\n",
    "    - start with an acceptable amount, train your model, and see how it performs\n",
    "    - then add more data if necessary, and retrain\n",
    "    \n",
    "    \n",
    "2. Annotate data: in spaCy input format\n",
    "  \n",
    "  \n",
    "3. Update training of an existing model:   \n",
    "   - if entities in the existing model but bad performance on your data   \n",
    "     -> update model with your own data\n",
    "   - if entities not in the model at all   \n",
    "     -> train custom model from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotating and preparing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect:\n",
    "  - collect user logs\n",
    "  - save `JSON` format (spaCy input format)\n",
    "  \n",
    "  \n",
    "Annotate:\n",
    "  - intents\n",
    "  - entities\n",
    "  - POS tags\n",
    "  - etc.\n",
    "  - see book: `JSON` annotated example\n",
    "  - Use Prodigy (spaCy's tool, not free) or Brat (free)\n",
    "  \n",
    "  \n",
    "spaCy training data format:\n",
    "  - annotate raw data\n",
    "  - then convert each utterance into an `Example` object\n",
    "  - see book: example code\n",
    "  - (different training format for the dependency parser)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Updating an existing pipeline component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Train spaCy's NER model\n",
    "  - with our own examples from **navigation** domain\n",
    "  - because it doesn't label some of them with the correct entity type\n",
    "  - we want to recognize some locations (street names, district names, 'home', 'work', 'office'...)\n",
    "  \n",
    "  \n",
    "#### 3 steps:\n",
    "  - \"code block\":  \n",
    "    - disable other statistical components in the pipeline (including POS tagger and dependency parser)   \n",
    "    - to train **only** the intended component\n",
    "  - feed our domain examples to the training procedure\n",
    "  - evaluate the new NER model\n",
    "  \n",
    "  \n",
    "- also:\n",
    "  - save updated NER model to disk\n",
    "  - load updated NER model when need to use it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Disable other statistical models\n",
    "\n",
    "- see book for disabling code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Model training procedure\n",
    "\n",
    "- spaCy's NER model is a neural network model:\n",
    "  - need to configure some **parameters** \n",
    "  - and provide **training examples\n",
    "  \n",
    "  \n",
    "- Each prediction of the neural network is a **sum of its weight** values;   \n",
    "  hence, the training procedure **adjusts** the weights of the neural network with our examples\n",
    "  \n",
    "  \n",
    "#### `epochs` parameter:\n",
    "\n",
    "- Shuffling:\n",
    "  - we go over the training *several times*\n",
    "  - because showing each training example once is not enough\n",
    "  - at each iteration, we shuffle the training data, so that their order does not matter\n",
    "  - shuffling helps training the neural network thoroughly\n",
    "  \n",
    "  \n",
    "- In each `epoch`, training code updates the weights of the nn with a small number\n",
    "  - then a **loss** value is calculated\n",
    "  - by comparing the actual label with the nn's current output\n",
    "  - then **optimizer** function updates nn's weight\n",
    "  - with respect to this loss value\n",
    "  - see book for code example: \n",
    "    - with SGD as optimizer (Stochastic Gradient Descent, iterative algorithm, used to minimize a function)\n",
    "    - starts from a random point on the loss function,   \n",
    "      and travels down its slope in steps,   \n",
    "      until it reaches the lowest point of that function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Evaluate updated NER model\n",
    "\n",
    "- Test if model recognizes our domain utterances,\n",
    "- and didn't just memorize them\n",
    "See book for code example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Save and load custom models\n",
    "\n",
    "See book for code example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a pipeline component from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Start with a small dataset to understand the training procedure\n",
    "- Then work with real-world dataset\n",
    "See book for code example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a class=\"anchor\" id=\"chapter8\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:34px; font-weight:bold; background:#DDEEEE;padding: 15px;\">Chapter 8 – Text Classification with spaCy</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The basics of text classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Text classification = \n",
    "  - supervised machine learning task\n",
    "  - assign set of predefined class labels to texts\n",
    "  - training dataset = text-class label pairs\n",
    "  \n",
    "  \n",
    "- Classes: defined based on your data\n",
    "  - e.g. customer reviews, classes = [positive, negative, neutral]\n",
    "  \n",
    "  \n",
    "- Class types:\n",
    "  - class labels: **categorical** (strings) or **numerical** (numbers)\n",
    "  \n",
    "  \n",
    "- Three categories of text classification depending on number of classes:\n",
    "  - **Binary**: 2 classes\n",
    "  - **Multiclass**: more than 2 classes, mutually exclusive (each text only one class)\n",
    "  - **Multilabel**: more than 2 classes, each text can be assigned more than one   \n",
    "    (e.g. levels of toxicity, insult+threat+obscene)\n",
    "  \n",
    "  \n",
    "- Used to **understand customer intent**\n",
    "  \n",
    "  \n",
    "- Most common types of text classification and their use cases:\n",
    "  - Topic detection\n",
    "  - Language detection\n",
    "  - Sentiment analysis (see example image)\n",
    "  \n",
    "<img src=\"images/Capture%20d%E2%80%99e%CC%81cran%202022-07-13%20a%CC%80%2010.16.08.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the spaCy text classifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- spaCy's text classifier: `TextCategorizer` class, based on a neural network\n",
    "- Optional and trainable pipeline component, with text-label pairs as training dataset\n",
    "  \n",
    "  \n",
    "- First, add `TextCategorizer` to the NLP pipeline\n",
    "  - comes after the essential components\n",
    "- Then do the **training** procedure\n",
    "\n",
    "<img src=\"images/Capture%20d%E2%80%99e%CC%81cran%202022-07-13%20a%CC%80%2010.26.20.png\" width=\"450\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting to know the `TextCategorizer` class\n",
    "\n",
    "- Available as:\n",
    "  - **single-label** classifier (predicts one class per example),\n",
    "  - or **multilabel** classifier (predicts more than one class per example)\n",
    "  \n",
    "  \n",
    "- Classifier's parameters:\n",
    "  - **threshold** value: class assigned to a text if its probability is higher than the threshold (0.5, or more for more confidence)\n",
    "  - **model name**\n",
    "  \n",
    "  \n",
    "See book for example code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatting training data for the `TextCategorizer`\n",
    "\n",
    "- E.g. label is `sentiment`, values: `0` or `1`\n",
    "- See book for training dataset format, how to add labels, and code example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the training loop\n",
    "\n",
    "- First, disable other pipe components, so that only new text classifier is trained\n",
    "- Then ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the new component\n",
    "\n",
    "- See book for example code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training `TextCategorizer` for multilabel classification\n",
    "\n",
    "- See book for example code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis with spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Exploring the dataset: plotting the class distribution, class imbalance!, etc.\n",
    "- Training the `TextClassifier` component\n",
    "See book for example code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text classification with spaCy and Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Blending **spaCy** with **TensorFlow** and its high-level API **Keras**\n",
    "  \n",
    "  \n",
    "- **Keras** :\n",
    "  - high-level deep learning API that can run on top of ML libraries such as TensorfFlow, Theano and CNTK\n",
    "  - Very popular in research and development world because:\n",
    "    - supports rapid prototyping\n",
    "    - provides user-friendly API to neural network architectures\n",
    "    \n",
    "    \n",
    "- **TensorFlow 2** integrates **tf.keras** high-level API\n",
    "  - developers can take advantage of Keras' user-friendliness + TensorFlow's low-level methods\n",
    "    \n",
    "    \n",
    "- Layers:\n",
    "  - **input layer**: first one\n",
    "  - **hidden layers**\n",
    "  - **output layer**: last one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a layer?\n",
    "\n",
    "- Each layer transforms the input vectors   \n",
    "  and feed them of the next layer   \n",
    "  to get a final vector\n",
    "  \n",
    "  \n",
    "- Keras provides all sorts of layers:\n",
    "  - **input** layers:\n",
    "    - send input data to the rest of the network\n",
    "  - dense layers: \n",
    "    - transform the input of a given shape to the output shape we want   \n",
    "    - e.g. 5-dimensional input into 1-dimensional input\n",
    "  - **recurrent** layers: \n",
    "    - RNN, GRU, LSTM cells fully supported in Keras\n",
    "  - **dropout** layers: \n",
    "    - dropout is a technique to prevent overfitting (when neurons memorize data instead of learning it)\n",
    "    - dropout layers randomly select a given number of neurons   \n",
    "      and set their weights to zero for the forward and backward passes   \n",
    "      for one iterations\n",
    "      usually placed after dense layers\n",
    "  - **embedding** layers\n",
    "  - **activation** layers\n",
    "  - and so on\n",
    "  \n",
    "  \n",
    "- See book for example code"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential modeling with LSTMs\n",
    "\n",
    "- LSTM = an RNN variation\n",
    "  \n",
    "  \n",
    "- RNN: \n",
    "  - can process sequential data in steps\n",
    "  - and capture info about the past sequence of elements,\n",
    "  - by holding a **memory**, called **hidden state** (see figure)\n",
    "  - in text data, inputs and outputs are not independent of each other\n",
    "  - words depend on neighbor words\n",
    "  - e.g. machine translation, word translation depends on what we predicted before\n",
    "  \n",
    "<img src=\"images/Capture%20d%E2%80%99e%CC%81cran%202022-07-13%20a%CC%80%2014.59.33.png\" width=\"450\">\n",
    "  \n",
    "  \n",
    "- **RNN** issues:\n",
    "  - they forget some data back in the sequence\n",
    "  - they have numerical stability issues due to chain multiplications, called **vanishing** and **exploding gradients** (see *Colah's* blog)\n",
    "  \n",
    "  \n",
    "- **LSTMs** where invented to fix some computational problems of RNNs:\n",
    "  - LSTMs cells (see figure)\n",
    "    - are slightly more complicated that RNN cells\n",
    "    - but the logic of the computation is the same:\n",
    "      - we feed one input word at each time step: `xi`\n",
    "      - and LSTM outputs an output value at each time step: `hi`\n",
    "      - input steps and output steps are the same as RNN counterparts\n",
    "  - strong support in Keras for RNN variations: GRU and LSTM, + simple API for training RNNs\n",
    "  - RNN variations: \n",
    "    - crucial for NLP,\n",
    "    - language data is sequential by nature\n",
    "      - e.g. text is a sequence of words or characters, speech is a sequence of sounds\n",
    "\n",
    "<img src=\"images/Capture%20d%E2%80%99e%CC%81cran%202022-07-13%20a%CC%80%2015.04.19.png\" width=\"450\">\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Tokenizer\n",
    "\n",
    "- Step 1:\n",
    "  - Tokenize each sentence   \n",
    "  - and turn sentences into a sequence of words\n",
    "- Step 2:\n",
    "  - Create a vocabulary from the set of words   \n",
    "  - words that are supposed to be recognized by the nn\n",
    "- Step 3:\n",
    "  - Vocab should assign an ID to each word\n",
    "- Step 4:\n",
    "  - Map word IDs to word vectors\n",
    "  \n",
    "  \n",
    "- See book for example code"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding words\n",
    "\n",
    "- We can now transform words into vectors\n",
    "- **Embedding table**: a lookup table\n",
    "  - each row: a word's word vector\n",
    "  - row index: the word's ID\n",
    "  \n",
    "  \n",
    "1. word -> word-ID\n",
    "2. word-ID -> word vector\n",
    "\n",
    "<img src=\"images/Capture%20d%E2%80%99e%CC%81cran%202022-07-13%20a%CC%80%2015.27.49.png\" width=\"450\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network architecture for Text Classification\n",
    "\n",
    "- Step 1:\n",
    "  - preprocess, tokenize, pad the sentences\n",
    "  - output: list of sequences\n",
    "  \n",
    "  \n",
    "- Step 2:\n",
    "  - feed list of sequences to the neural network, throught the input layer\n",
    "  \n",
    "  \n",
    "- Step 3:\n",
    "  - vectorize each word by looking up its word ID in the embedding layer\n",
    "  - a sentence is now a sequence of word vectors, each corresponding to a word\n",
    "  \n",
    "  \n",
    "- Step 4:\n",
    "  - feed the sequence of word vectors to LSTM\n",
    "  \n",
    "  \n",
    "- Step 5: \n",
    "  - squash the LSTM output with a sigmoid layer\n",
    "  - output: class probabilities\n",
    "  \n",
    "  \n",
    "See book for code example:\n",
    "- Dataset\n",
    "- Data and vocabulary preparation\n",
    "- input layer\n",
    "- embedding layer\n",
    "- LSTM layer\n",
    "- compiling the model\n",
    "- fitting the model and experiment evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a class=\"anchor\" id=\"chapter9\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:34px; font-weight:bold; background:#DDEEEE;padding: 15px;\">Chapter 9 – spaCy and transformers</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Latest hot topic in NLP\n",
    "- How to use them with TensorFlow and spaCy\n",
    "  \n",
    "  \n",
    "**1.** Transformers and transfer learning    \n",
    "**2.** Understanding **BERT** (Bidirectinoal Encoder Representations from Transformers), architecture details used for transformers    \n",
    "**3.** How **BERT Tokenizer** and **WordPiece** algorithms work    \n",
    "**4.** How to quickly get started with pre-trained transformer models from **HuggingFace** library    \n",
    "**5.** Transformers with TensorFlow and Keras: practice fine-tuning HuggingFace   \n",
    "**6.** Transformers and spaCy: how spaCy integrates transformer models as pre-trained pipelines    \n",
    "  \n",
    "  \n",
    "- Build state-of-the-art NLP models with just a few lines of code\n",
    "- with the power of Transformer models and transfer learning\n",
    "   \n",
    "   \n",
    "#### Technical requirements\n",
    "\n",
    "- Install `transformers` and `tensorflow` Python libraries\n",
    "\n",
    "```\n",
    "pip install transformers\n",
    "pip install \"tensorflow>=2.0.0\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers and Transfer Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **2017**: NLP milestone, release of research paper *Attention is all you need*, by Vaswani et al.\n",
    "- introduced a new machine learning idea and architecture: transformers\n",
    "- transformers aim to solve sequential modeling tasks,   \n",
    "  targets some problems introduced by LSTM architecture\n",
    "   \n",
    "   \n",
    "- **Transfer learning**:\n",
    "  - import knowledge from pre-trained word vectors or pre-trained statistical models\n",
    "  - Glove and FastText word vectors are already trained on the Wikipedia corpus\n",
    "  - we used them directly for our semantic similarity calculations\n",
    "  \n",
    "  \n",
    "- Transformers offer thousands of pre-trained models to perform NLP tasks (text classification, summarization, question answering, machine translation, nlg...)\n",
    "  - in more than 100 languages\n",
    "  - aim to make state-of-the-art NLP accessible to everyone\n",
    "  - select a model suitable for your task\n",
    "  \n",
    "  <img src=\"images/Capture%20d%E2%80%99e%CC%81cran%202022-07-13%20a%CC%80%2015.53.21.png\" width=\"550\">\n",
    "  \n",
    "  \n",
    "- LSTM:\n",
    "  - has difficulties with learning statistical dependencies in long text\n",
    "    - as the steps pass, LSTM forgets about earlier time steps\n",
    "  - is sequential: process one word at each time step\n",
    "    - parallelizing is not possible\n",
    "    - performance bottleneck\n",
    "    \n",
    "    \n",
    "- Transformers address these problems:\n",
    "  - by not using recurrent layers at all\n",
    "  - architecture in **2 parts**:\n",
    "    - **Encoder** input (left in the figure)\n",
    "    - **Decoder** output (right in the figure)\n",
    "    - **Multi-Head Attention** bock\n",
    "    - **Self-attention** mechanism\n",
    "    - etc.\n",
    "    <img src=\"images/Capture%20d%E2%80%99e%CC%81cran%202022-07-13%20a%CC%80%2015.57.27.png\" width=\"450\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Bidirectional**: \n",
    "  - training on text data is bi-directional\n",
    "  - meaning each input sentence is processed from left to right + right to left\n",
    "  \n",
    "  \n",
    "- **Encoder**:\n",
    "  - encodes the input sentence\n",
    "  \n",
    "  \n",
    "- **Representations**:\n",
    "  - a word vector\n",
    "  \n",
    "  \n",
    "- **Transformers**:\n",
    "  - architecture is transformer-based\n",
    "  \n",
    "  \n",
    "- **Input**: a sentence\n",
    "- **Output**: a sequence of word vectors, contextual (word vector assigned to a word based on the input sentence)\n",
    "  \n",
    "  \n",
    "ETC. ... ... see book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers and TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers and spaCy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- spacy v3.0 new feature: Transformer-based pipelines\n",
    "\n",
    "    <img src=\"images/Capture%20d%E2%80%99e%CC%81cran%202022-07-13%20a%CC%80%2016.04.19.png\" width=\"450\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a class=\"anchor\" id=\"chapter10\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:34px; font-weight:bold; background:#DDEEEE;padding: 15px;\">Chapter 10 – Putting it all together: Designing your chatbot with spaCy</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- entity extraction\n",
    "- intent recognition\n",
    "- context handling\n",
    "- syntactic and semantic parsing\n",
    "- text classification\n",
    "  \n",
    "  \n",
    "1. Explore **dataset** used to collect linguistic information about utterances\n",
    "2. Perform **NER** by combining spaCy **NER model** and spaCy's **`Matcher`**\n",
    "3. Perform **intent recognition**: 2 different techniques:\n",
    "    - pattern-based method\n",
    "    - statistical text classification with TensorFlow and Keras:    \n",
    "      train a character-level LSTM to classify utterance intents\n",
    "4. Sentence- and dialog-level semantics:\n",
    "    - anaphora resolution\n",
    "    - grammatical question types\n",
    "    - differentiating subjects from objects\n",
    "    \n",
    "    \n",
    "Chapter goal: design a real **chatbot NLU pipeline**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Conversational AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intent recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
