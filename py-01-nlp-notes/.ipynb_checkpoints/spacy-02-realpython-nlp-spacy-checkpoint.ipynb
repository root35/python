{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reference**: [NLP with Spacy](https://realpython.com/natural-language-processing-spacy-python/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:21px; font-weight:bold; background:#eeeeee;padding: 15px;\">Contents</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Install Spacy](#section1)\n",
    "- [Read input](#section2)\n",
    "- [Sentence detection](#section3)\n",
    "- [Tokenization](#section4)\n",
    "- [Lemmatization](#section5)\n",
    "- [Word frenquency](#section6)\n",
    "- [Part-of-speech Tagging](#section7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a class=\"anchor\" id=\"section1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:30px; font-weight:bold; background:#DDEEEE;padding: 15px;\">Install Spacy</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a virtual environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "$ python3 -m venv myenv\n",
    "$ source ./myenv/bin/activate\n",
    "!pip install spacy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/Capture%20d%E2%80%99e%CC%81cran%202022-07-10%20a%CC%80%2022.53.13.png\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download models and data per language\n",
    "\n",
    "- French Models: [fr_core_news_sm](https://spacy.io/models/fr)\n",
    "- Install [Spacy transformers](https://spacy.io/universe/project/spacy-transformers)\n",
    "- Select models: [Spacy Trained Models and Pipelines](https://spacy.io/models)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/Capture%20d%E2%80%99e%CC%81cran%202022-07-10%20a%CC%80%2022.57.15.png\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download fr_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple cherche à acheter une start-up anglaise pour 1 milliard de dollars\n",
      "Apple                 NOUN     ROOT\n",
      "cherche               VERB     amod\n",
      "à                     ADP      mark\n",
      "acheter               VERB     xcomp\n",
      "une                   DET      det\n",
      "start                 NOUN     obj\n",
      "-                     PUNCT    punct\n",
      "up                    DET      det\n",
      "anglaise              NOUN     ROOT\n",
      "pour                  ADP      case\n",
      "1                     NUM      nummod\n",
      "milliard              NOUN     nmod\n",
      "de                    ADP      case\n",
      "dollars               NOUN     nmod\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.lang.fr.examples import sentences\n",
    "\n",
    "nlp = spacy.load('fr_core_news_sm')\n",
    "doc = nlp(sentences[0])\n",
    "print(doc.text)\n",
    "for token in doc:\n",
    "    print(f'{token.text:<21} {token.pos_:<8} {token.dep_}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/Capture%20d%E2%80%99e%CC%81cran%202022-07-10%20a%CC%80%2021.53.48.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a class=\"anchor\" id=\"section2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:30px; font-weight:bold; background:#DDEEEE;padding: 15px;\">Read input</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Read a String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ce', 'tutoriel', 'Spacy', 'explique', 'comment', 'faire', 'du', 'Traitement', 'Automatique', 'du', 'Langage', 'Naturel', '.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('fr_core_news_sm')\n",
    "\n",
    "intro_text = ('Ce tutoriel Spacy explique comment faire'\n",
    "              ' du Traitement Automatique du Langage Naturel.')\n",
    "intro_doc = nlp(intro_text)\n",
    "print([token.text for token in intro_doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ce', 'tutoriel', 'Spacy', 'explique', 'comment', 'faire', '\\n', 'du', 'Traitement', 'Automatique', 'du', 'Langage', 'Naturel', '.', '\\n', 'Est', '-ce', 'que', 'tu', 'peux', '...', 'Laisse', 'tomber', '.', '\\n', \"J'\", 'ai', 'oublié', 'ce', 'que', 'je', 'voulais', 'dire', '!', '\\n']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('fr_core_news_sm')\n",
    "\n",
    "intro_filename = 'test_files/introduction.txt'\n",
    "intro_text = open(intro_file_name).read()\n",
    "intro_doc = nlp(intro_file_text)\n",
    "print([token.text for token in intro_file_doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a class=\"anchor\" id=\"section3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:30px; font-weight:bold; background:#DDEEEE;padding: 15px;\">Sentence detection</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ce tutoriel Spacy explique comment faire\n",
      "du Traitement Automatique du Langage Naturel.\n",
      "\n",
      "Est-ce que tu peux ...\n",
      "Laisse tomber.\n",
      "\n",
      "J'ai oublié ce que je voulais dire !\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentences = list(intro_doc.sents)\n",
    "for sentence in sentences:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Custom pipeline component: sentence delimiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Create new component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.language import Language\n",
    "\n",
    "@Language.component(\"set_custom_boundary\")\n",
    "def set_custom_boundary(doc):\n",
    "    ''' Recognize '...' as sentence delimiter.'''\n",
    "    for token in doc[:-1]:\n",
    "        if token.text == '...':\n",
    "            doc[token.i+1].is_sent_start = True\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Add it to pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.set_custom_boundary(doc)>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_nlp = spacy.load('fr_core_news_sm')\n",
    "custom_nlp.add_pipe('set_custom_boundary', before='parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Use it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ce tutoriel Spacy explique comment faire\n",
      "du Traitement Automatique du Langage Naturel.\n",
      "\n",
      "Est-ce que tu peux ...\n",
      "Laisse tomber.\n",
      "\n",
      "J'ai oublié ce que je voulais dire !\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "intro_filename = 'test_files/introduction.txt'\n",
    "intro_text = open(intro_file_name).read()\n",
    "intro_doc = nlp(intro_file_text)\n",
    "intro_sentences = list(intro_doc.sents)\n",
    "\n",
    "for sentence in sentences:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a class=\"anchor\" id=\"section4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:30px; font-weight:bold; background:#DDEEEE;padding: 15px;\">Tokenization</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0    Ce                                \n",
      "1    3    tutoriel                          \n",
      "2    12   Spacy                             \n",
      "3    18   explique                          \n",
      "4    27   comment                           \n",
      "5    35   faire                             \n",
      "6    40   \n",
      "                                 \n",
      "7    41   du                                \n",
      "8    44   Traitement                        \n",
      "9    55   Automatique                       \n",
      "10   67   du                                \n"
     ]
    }
   ],
   "source": [
    "for token in intro_doc[:11]:\n",
    "    print(f'{token.i:<5}{token.idx:<5}{token.text:<34}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Token` attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See: [`Token` attributes](https://spacy.io/api/token#attributes)\n",
    "\n",
    "- `token.i`: sentence index\n",
    "- `token.idx`: token index in dictionary\n",
    "  \n",
    "  \n",
    "- `token.text_with_ws`: token with trailing space if present\n",
    "- `token.is_alpha`: consist of alphabetic characters\n",
    "- `token.is_punct`:\n",
    "- `token.is_space`:\n",
    "- `token.is_stop`: is in the stop words list\n",
    "  \n",
    "  \n",
    "- `token.shape_`: show orthographic features.  \n",
    "  alphabetic characters replaced by `x`   \n",
    "  numeric characters replaced by `X`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Customize the `nlp.tokenizer`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass various parameters to the `Tokenizer` class:\n",
    "\n",
    "- `nlp.vocab`: \n",
    "  - Storage container for special cases\n",
    "  - Ex: contractions, emoticons\n",
    "  \n",
    "  \n",
    "- `prefix_search`:\n",
    "  - Function used to handle preceding punctuation\n",
    "  - Ex: opening parentheses\n",
    "  \n",
    "  \n",
    "- `suffix_search`:\n",
    "  - Function used to handle succeeding punctuation\n",
    "  - Ex: closing parentheses\n",
    "  \n",
    "  \n",
    "- `infix_finditer`:\n",
    "  - Function used to handle non-whitespace separators\n",
    "  - Ex: hyphens\n",
    "  \n",
    "  \n",
    "- `token_match`:\n",
    "  - Optional `Boolean` function used to match strings that should never be split.   \n",
    "  - Ex: entities like URLs or numbers   \n",
    "  - **Overrides** the previous rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See example code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Usually removed because they distort the word frequency analysis\n",
    "- Stop words in French:   \n",
    "  `/Users/macbook/anaconda3/lib/python3.7/site-packages/spacy/lang/fr/stop_words.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "507\n",
      "['quoique', 'six', 'differentes', 'peux', 'te', 'parfois', 'celui-la', 'qu’', 'se', 'façon', 'les']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "spacy_stopwords = spacy.lang.fr.stop_words.STOP_WORDS\n",
    "print(len(spacy_stopwords))\n",
    "print(list(spacy_stopwords)[:11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple                 NOUN     ROOT\n",
      "cherche               VERB     amod\n",
      "acheter               VERB     xcomp\n",
      "start                 NOUN     obj\n",
      "-                     PUNCT    punct\n",
      "up                    DET      det\n",
      "anglaise              NOUN     ROOT\n",
      "1                     NUM      nummod\n",
      "milliard              NOUN     nmod\n",
      "dollars               NOUN     nmod\n",
      "\n",
      "Found 4 stop words in this document.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.lang.fr.examples import sentences\n",
    "\n",
    "nlp = spacy.load('fr_core_news_sm')\n",
    "doc = nlp(sentences[0])\n",
    "\n",
    "for token in doc:\n",
    "    if not token.is_stop:\n",
    "        print(f'{token.text:<21} {token.pos_:<8} {token.dep_}')\n",
    "\n",
    "doc_stop_words = [token for token in doc if token.is_stop]\n",
    "print(f'\\nFound {len(doc_stop_words)} stop words in this document.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a class=\"anchor\" id=\"section5\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:30px; font-weight:bold; background:#DDEEEE;padding: 15px;\">Lemmatization</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " d\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a class=\"anchor\" id=\"section6\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:30px; font-weight:bold; background:#DDEEEE;padding: 15px;\">Word frequency</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " d\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a class=\"anchor\" id=\"section7\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:30px; font-weight:bold; background:#DDEEEE;padding: 15px;\">Part-of-speech Tagging</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " d\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
