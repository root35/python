{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RECURRENT LAYERS\n",
    "\n",
    "* How to create embeddings that convey the meaning of variable-length phrases and sentences\n",
    "* Different sentences have different-length vectors\n",
    "-> makes vector comparison tricky   \n",
    "\n",
    "# Averaging word embeddings\n",
    "\n",
    "* very effective and powerful for capturing complex relationships between words\n",
    "* when we average word embeddings, average shapes/curves remain\n",
    "* similar words have similarities to their shapes (see image in the book)   \n",
    "<br/>   \n",
    "\n",
    "* In the following example, the reviews' vector representations capture statistical information such that positive and negative embeddings cluster together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THIS CODE AT THE END OF THE PREVIOUS CHAPTER'S NOTEBOOK\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "norms = np.sum(weights_0_1 * weights_0_1, axis=1)\n",
    "norms.resize(norms.shape[0], 1)\n",
    "normed_weights = weights_0_1 * norms\n",
    "\n",
    "def make_sentence_vector(words):\n",
    "    indices = list(map(lambda x: vocab_index.get(x), \\\n",
    "              filter(lambda x: x in vocab_index, words)))\n",
    "    return np.mean(normed_weights[indices], axis=0)\n",
    "\n",
    "reviews2vectors = list()\n",
    "for review in tokens:\n",
    "    reviews2vectors.append(make_sentence_vector(review))\n",
    "reviews2vectors = np.array(reviews2vectors)\n",
    "\n",
    "def most_similar_reviews(review):\n",
    "    v = make_sentence_vectors(review)\n",
    "    \n",
    "    scores = Counter()\n",
    "    for index, value in enumerate(reviews2vectors.dot(v)):\n",
    "        scores[index] = value\n",
    "    \n",
    "    most_similar = list()\n",
    "    for index, score in scores.most_common(3):\n",
    "        most_similar.append(raw_reviews[index][0:80])\n",
    "    \n",
    "    return most_similar\n",
    "\n",
    "print(most_similar_reviews(['boring', 'awful']))\n",
    "print()\n",
    "print(most_similar_reviews(['nice', 'good']))\n",
    "\n",
    "# OUTPUT:\n",
    "#\n",
    "#  ['comment this movie is impossible  is terrible  very improbable  bad interpretati', \n",
    "#   'horrible waste of time   bad acting  plot  directing  this is the most boring mo',\n",
    "#   'this movie stinks  the stench resembles bad cowpies that sat in the sun too long']\n",
    "#\n",
    "#  ['this is actually one of my favorite films  i would recommend that everyone watch',\n",
    "#   'this movie is terrible but it has some good effects ', \n",
    "#   'malcolm mcdowell has not had too many good movies lately and this is no differen']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How the network uses embeddings\n",
    "\n",
    "* It detects the curves that have correlation with a target label\n",
    "* **Bag of words**: sentence embeddings are a **sum or average** of the characteristics of their words\n",
    "* **Pros**:\n",
    "  * if a sequence has repeating patterns, the sentence vector retains the most dominant patterns accross the word vectors being summed\n",
    "* **Cons**:\n",
    "  * if a sequence is too long, the sentence vector will average out to a **straight line** (vector of near-0s)\n",
    "  * **order** becomes irrelevant (ex. \"Yankees defeat Red Sox\" vs \"Red Sox defeat Yankees\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN: Learning word order in embeddings\n",
    "\n",
    "* **Identity matrix**: a square matrix of 0s with 1s on the diagonal\n",
    "```\n",
    "[1, 0, 0] \n",
    "[0, 1, 0] \n",
    "[0, 0, 1]\n",
    "```\n",
    "* Multiplied with any vector, it returns the original vector   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "this  = np.array([ 2,  4,  6])\n",
    "movie = np.array([10, 10, 10])\n",
    "rocks = np.array([ 1,  1,  1])\n",
    "\n",
    "identity = np.eye(3)\n",
    "\n",
    "print(identity)\n",
    "print(this + movie + rocks)\n",
    "print( (this.dot(identity) + movie).dot(identity) + rocks )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Instead of directly summing or averaging, we add a step after each sum/average:\n",
    "```this movie rocks```\n",
    "\n",
    "  * multiply the `this` vector by the matrix\n",
    "  * add the output to the `movie` vector\n",
    "  * multiply the output by the matrix\n",
    "  * add the output to the `rocks` vector\n",
    "\n",
    "```\n",
    "(this.dot(identity) + movie).dot(identity) + rocks\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning transition matrices\n",
    "\n",
    "* Which matrix to use as transition? The network learns it:\n",
    "  * learn useful word vectors\n",
    "  * learn useful modifications to the transition matrices\n",
    "* The method:\n",
    "  * creating a sentence embedding\n",
    "  * using it to predict\n",
    "  * then modifying the parts that formed the sentence embedding to make this prediction more accurate\n",
    "* Transition matrix starts as an identity matrix, then it is modified:\n",
    "  * during training, we backpropagate gradients into it and update it to help the network make better predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3 sets of weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x_):\n",
    "    x = np.atleast_2d(x_)\n",
    "    temp = np.exp(x)\n",
    "    return temp / np.sum(temp, axis=1, keepdims=True)\n",
    "\n",
    "# Dictionary of word embeddings\n",
    "word_vects = {}\n",
    "word_vects['yankees'] = np.array([[0., 0., 0.]])\n",
    "word_vects['bears']   = np.array([[0., 0., 0.]])\n",
    "word_vects['braves']  = np.array([[0., 0., 0.]])\n",
    "word_vects['red']     = np.array([[0., 0., 0.]])\n",
    "word_vects['sox']     = np.array([[0., 0., 0.]])\n",
    "word_vects['lose']    = np.array([[0., 0., 0.]])\n",
    "word_vects['defeat']  = np.array([[0., 0., 0.]])\n",
    "word_vects['beat']    = np.array([[0., 0., 0.]])\n",
    "word_vects['tie']     = np.array([[0., 0., 0.]])\n",
    "\n",
    "# Identity matrix (transition weights)\n",
    "identity = np.eye(3)\n",
    "\n",
    "# Classification layer:\n",
    "# a weight matrix to predict the next word given a sentence vector of length 3\n",
    "sent2output = np.random.rand(3, len(word_vects))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a sentence embedding\n",
    "layer_0 = word_vects['red']\n",
    "layer_1 = layer_0.dot(identity) + word_vects['sox']\n",
    "layer_2 = layer_1.dot(identity) + word_vects['defeat']\n",
    "\n",
    "# Predicting over all vocabulary\n",
    "pred = softmax(layer_2.dot(sent2output))\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backpropagation\n",
    "\n",
    "* We generate `layer_2_delta` by backpropagating twice:\n",
    "  * once accross the identity matrix to create `layer_1_delta`\n",
    "  * and again to `word_vects[defeat]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The target: the one-hot vector for 'yankees'\n",
    "y = np.array([1, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "\n",
    "# Generating layer_2_delta\n",
    "\n",
    "pred_delta    = pred - y\n",
    "layer_2_delta = pred_delta.dot(sent2output.T)  # sent2output: weight matrix\n",
    "\n",
    "# Backpropagating layer_2_delta accross word_vects\n",
    "\n",
    "defeat_delta  = layer_2_delta * 1   # can ignore the '1' (see chap. 11)\n",
    "layer_1_delta = layer_2_delta.dot(identity.T)\n",
    "sox_delta     = layer_1_delta * 1   # idem\n",
    "layer_0_delta = layer_1_delta.dot(identity.T)\n",
    "\n",
    "alpha = 0.01\n",
    "word_vects['red']    -= alpha * layer_0_delta\n",
    "word_vects['sox']    -= alpha * sox_delta\n",
    "word_vects['defeat'] -= alpha * defeat_delta\n",
    "\n",
    "# Backpropagating layer_2_delta accross the identity matrix\n",
    "\n",
    "identity    -= alpha * np.outer(layer_0, layer_1_delta)\n",
    "identity    -= alpha * np.outer(layer_1, layer_2_delta)\n",
    "sent2output -= alpha * np.outer(layer_2, pred_delta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training on a toy corpus: Babi dataset\n",
    "\n",
    "* synthetically generated question-answer corpus to teach machines how to answer simple questions about an environment\n",
    "* contains a variety of simple statements and questions\n",
    "* each question is followed by the correct answer\n",
    "* we train the network to finish each sentence when given one or more starting words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the terminal\n",
    "# wget http://www.thespermwhale.com/jaseweston/babi/tasks_1-20_v1-1.tar.gz\n",
    "# tar -xvf tasks_1-20_v1-1.tar.gz\n",
    "\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "with open('/Users/macbook/code/_dataset_thespermwhale/tasksv11/en/qa1_single-supporting-fact_train.txt', 'r') as f:\n",
    "    raw = f.readlines().strip().lower().replace('\\n', '')\n",
    "\n",
    "sentences = list()\n",
    "for line in raw[0:1000]:\n",
    "    sentences.append(line.split(' ')[1:])\n",
    "\n",
    "for sentence in sentences[0:3]:\n",
    "    print(sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import re\n",
    "import itertools\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading and encoding data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab 44 Sentences 100\n"
     ]
    }
   ],
   "source": [
    "with open('/Users/macbook/code/_dataset_thespermwhale/tasksv11/en/qa1_single-supporting-fact_train.txt', 'r') as f:\n",
    "    raw = f.readlines()\n",
    "\n",
    "sentences = list()\n",
    "for line in raw[0:100]:\n",
    "    sentences.append(line.strip().lower().replace('\\n', '').split(' ')[1:])\n",
    "        \n",
    "vocab = set()\n",
    "for sentence in sentences:\n",
    "    for word in sentence:\n",
    "        vocab.add(word)\n",
    "vocab = list(vocab)\n",
    "\n",
    "vocab_index = {}\n",
    "for index, word in enumerate(vocab):\n",
    "    vocab_index[word] = index\n",
    "\n",
    "print(f'Vocab {len(vocab_index)} Sentences {len(sentences)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Network parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence2sequence(sentence):\n",
    "    sequence = np.array([vocab_index.get(word) for word in sentence if len(word) > 0], dtype=int)\n",
    "    return sequence\n",
    "\n",
    "def softmax(x):\n",
    "    '''Used to predict'''\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "np.random.seed(1)\n",
    "alpha = 0.001\n",
    "iterations = 30000\n",
    "embed_size = 10\n",
    "\n",
    "word_embeddings = (np.random.rand(len(vocab), embed_size) - 0.5) * 0.1  # word embeddings\n",
    "sentence_embedding = np.zeros(embed_size)     # sentence embeddings\n",
    "recurrent_embedding = np.eye(embed_size)   # recurrent embedding: embedding -> embedding (transition matrix)\n",
    "decoder = (np.random.rand(embed_size, len(vocab)) - 0.5) * 0.1  # weight matrix: embedding -> output weights\n",
    "one_hot = np.eye(len(vocab))     # utility matrix: one-hot lookups (for the loss function)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PREDICT: Forward propagation and prediction with arbitraty length\n",
    "\n",
    "* same procedure as before: summing embeddings by using an identity matrix called `recurrent` (initialized to all 0s)\n",
    "* only difference: `layers` is a new way to forward propagate \n",
    "  * we can't use static layers anymore\n",
    "  * we need more forward propagations if the length of `sent` is larger\n",
    "  * we append new layers to the list based on the number of forward propagations needed\n",
    "* instead of predicting only the last word, we make a prediction `layer[pred]` avec every step, based on the embedding generated by the previous words\n",
    "  * more efficient than doing forward propagation from the beginning for each new prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(sequence):\n",
    "    \n",
    "    layers = list()  # one per word in the sequence: 'pred' + 'hidden'\n",
    "    layer = {}\n",
    "    layer['hidden'] = sentence_embedding\n",
    "    layers.append(layer)\n",
    "    #print('START:\\n', layer['hidden'])\n",
    "    \n",
    "    preds = list()  # forward propagation\n",
    "    loss = 0\n",
    "    \n",
    "    for target_i in range(len(sequence)):\n",
    "        layer = {}\n",
    "        \n",
    "        # predict next word using softmax: hidden * weights\n",
    "        layer['pred'] = softmax(layers[-1]['hidden'].dot(decoder))\n",
    "        \n",
    "        loss += -np.log(layer['pred'][sequence[target_i]])\n",
    "        \n",
    "        # generates the next hidden state\n",
    "        layer['hidden'] = layers[-1]['hidden'].dot(recurrent_embedding) \\\n",
    "                        + word_embeddings[sequence[target_i]]\n",
    "        \n",
    "        layers.append(layer)\n",
    "        \n",
    "     #   print(f'TOKEN: {sequence[target_i]}')\n",
    "        for key, value in layer.items():\n",
    "            print(key, ':\\n', value)\n",
    "     #   print(f'LOSS: {loss}')\n",
    "    \n",
    "    return layers, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### COMPARE: Backpropagation with arbitrary length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagation(layers):\n",
    "    \n",
    "    for layer_idx in reversed(range(len(layers))):  # backpropagates\n",
    "        layer = layers[layer_idx]\n",
    "        target = sequence[layer_idx-1]\n",
    "        print('-----', layer_idx, target)\n",
    "        \n",
    "        if (layer_idx > 0):\n",
    "            layer['output_delta'] = layer['pred'] - one_hot[target]\n",
    "            print('output_delta:', layer['output_delta'].shape, len(vocab_index))\n",
    "            print(layer['output_delta'])\n",
    "            \n",
    "            new_hidden_delta = layer['output_delta'].dot(decoder.transpose())\n",
    "            print('new_hidden_delta:', new_hidden_delta.shape)\n",
    "            print(new_hidden_delta)\n",
    "            \n",
    "            if (layer_idx == len(layers)-1):  # last layer: no backpropagation\n",
    "                layer['hidden_delta'] = new_hidden_delta\n",
    "            else:\n",
    "                layer['hidden_delta'] = new_hidden_delta \\\n",
    "                                        + layers[layer_idx+1]['hidden_delta'] \\\n",
    "                                                    .dot(recurrent_embedding.transpose())\n",
    "            print('hidden_delta:', layer['hidden_delta'].shape)\n",
    "            print(layer['hidden_delta'])\n",
    "                \n",
    "        else:  # first layer\n",
    "            layer['hidden_delta'] = layers[layer_idx+1]['hidden_delta'] \\\n",
    "                                                    .dot(recurrent_embedding.transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LEARN: update weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_update(sequences, layers, sentence_embedding, decoder, word_embeddings, recurrent_embedding):\n",
    "    \n",
    "    sentence_embedding -= layers[0]['hidden_delta'] * alpha / float(len(sequence))  # updating weights\n",
    "    \n",
    "    for layer_idx, layer in enumerate(layers[1:]):\n",
    "        print('---', layer_idx)\n",
    "        \n",
    "        decoder -= np.outer(layers[layer_idx]['hidden'], \\\n",
    "                            layer['output_delta']) \\\n",
    "                 * alpha / float(len(sequence))\n",
    "        print('decoder:', decoder.shape)\n",
    "        print(decoder)\n",
    "        \n",
    "        embed_idx = sequence[layer_idx]\n",
    "        word_embeddings[embed_idx] -= layers[layer_idx]['hidden_delta'] \\\n",
    "                                      * alpha / float(len(sequence)) \n",
    "        print('word_embedding:', word_embeddings.shape, word_embeddings[embed_idx].shape)\n",
    "        print(word_embeddings[embed_idx].shape)\n",
    "        \n",
    "        recurrent_embedding -= np.outer(layers[layer_idx]['hidden'], \\\n",
    "                                     layer['hidden_delta']) \\\n",
    "                               * alpha / float(len(sequence))\n",
    "        print('recurrent_embedding:', recurrent_embedding.shape)\n",
    "        print(recurrent_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training\n",
    "\n",
    "* **perplexity**: \n",
    "  * represents the **difference between two probability distributions**\n",
    "  * in this example, the perfect probability distribution is 100% to the correct term and 0% elsewhere\n",
    "  * probability of the correct label (word), passed through a log function, negated, and exponentiated ($e^x$)\n",
    "  * **high** when probability distributions don't match, **low** (close to 1) when they do match   \n",
    "  * decreasing perplexity is a good thing\n",
    "  * means the network is learning to predict probabilities that match the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iteration in range(iterations):  # forward\n",
    "    \n",
    "    sequence = sentence2sequence(sentences[iteration % len(sentences)][1:])\n",
    "    print('----------------------------')\n",
    "    print('SEQUENCE: ', sequence)\n",
    "    \n",
    "    print('-- @predict() --')\n",
    "    layers, loss = predict(sequence)                                                          # predict\n",
    "    print('-- @backpropagation() --')\n",
    "    backpropagation(layers)                                                                   # compare\n",
    "    print('-- @weight_update() --')\n",
    "    weight_update(sequence, layers, sentence_embedding, decoder, word_embeddings, recurrent_embedding)   # learn\n",
    "    \n",
    "    if (iteration % 5000 == 0) or (iteration == iterations-1):\n",
    "        perplexity = np.exp(loss / len(sequence))\n",
    "        print(f'{iteration:5} Perplexity {perplexity:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_index = 4\n",
    "l, _ = predict(sentence2sequence(sentences[seq_index]))\n",
    "print(sentences[seq_index])\n",
    "\n",
    "for i, each_layer in enumerate(l[1:-1]):\n",
    "    input = sentences[seq_index][i]\n",
    "    target = sentences[seq_index][i+1]\n",
    "    pred = vocab[each_layer['pred'].argmax()]\n",
    "    print(f'Prev input: {input}' + (' ' * (12 - len(input)))  + \\\n",
    "          f' Target: {target}'   + (' ' * (15 - len(target))) + \\\n",
    "          f' Pred: {pred}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
