{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACTIVATION FUNCTIONS\n",
    "\n",
    "# What is an activation function?\n",
    "\n",
    "* It's a function applied to neurons during predictino (ex. `relu`)\n",
    "* A small list of activations **account for the vast majority of activation needs**,   \n",
    "  improvements on them have been minute in most cases\n",
    "* An infinite number of functions could be used as activations,   \n",
    "  but not any function can be used as activation\n",
    "\n",
    "### Several constraints on what makes a function an activation function:\n",
    "\n",
    "**1.** Must be **continuous and infinite** in domain:   \n",
    "   = has an output number for **_any_** input, no input x for which we can't compute an output (y)   \n",
    "<br/>   \n",
    "\n",
    "**2.** Must be **monotonic**, never changing direction:   \n",
    "   = is 1:1, i.e. either **_always increasing_** or **_always decreasing_**   \n",
    "   = can never have the same output value for multiple input values   \n",
    "   = can make learning harder is there are multiple right answers, multiple possible perfect weight configurations   \n",
    "   SEE convex vs non-convex optimization      \n",
    "<br/>   \n",
    "\n",
    "**3.** Must be **nonlinear** (they squiggle and turn)   \n",
    "<br/>   \n",
    "\n",
    "**4.** Should be **efficiently computable** (and their derivatives):   \n",
    "  = you'll be calling it a lot (sometimes billions of times)   \n",
    "  = ex. `relu` is the most popular, it is ridiculously easy to compute at the expense of its expressiveness \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hidden layer activation functions\n",
    "\n",
    "### sigmoid\n",
    "\n",
    "* smoothly squishes the infinite amount of input to an output **between 0 and 1**\n",
    "* you can interpret the output of an individual neuron as a probability\n",
    "* thus people use this nonlinearity both in hidden and output layers\n",
    "\n",
    "### tanh\n",
    "\n",
    "* better than `sigmoid` for hidden layers\n",
    "* smoothly squishes the infinite amount of input to an output **between -1 and 1**\n",
    "* gives varying degrees of positive correlation (see selective correlation),\n",
    "* so it can also throw in some *negative correlation*:\n",
    "  * not useful for output layers unless predictions between -1 and 1\n",
    "  * powerful for hidden layers\n",
    "* outperforms `sigmoid` on many problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output layer activation functions\n",
    "\n",
    "* different than what's best for hidden layers, especially for classification\n",
    "* 3 major types of output layers\n",
    "\n",
    "### no activation: when predicting raw data values\n",
    "\n",
    "* most straightforward but least common type of output layer\n",
    "* range of output values is something other than a probability   \n",
    "  ex. predicting average temperature in Paris given temperature in surrounding cities\n",
    "* focus on ensuring that output nonlinearity can predict the right answers\n",
    "* `sigmoid`or `tanh` would be inappropriate: forces predictions between 0 and 1, or -1 and 1   \n",
    "  range of temperature values can be higher or lower\n",
    "\n",
    "### sigmoid: when predicting unrelated yes/no probabilities\n",
    "\n",
    "* multiple binary probabilities in one network (see \"GD with multiple outputs\")\n",
    "* often when predicting one label, the network will learn something useful to one of the other labels\n",
    "* `sigmoid` more appropriate because it models individual probabilities separately for each output node\n",
    "\n",
    "### softmax: when predicting which-one probabilities\n",
    "\n",
    "* by far the most common type of output layer: predicting a single label out of many\n",
    "* could use `sigmoid` and declare that highest output probability is the most likely, will work reasonably well\n",
    "* but it's far better with an activation function that models the idea that   \n",
    "  \"the more likely it's one label, the less likely it's any of the other labels\"\n",
    "* example:\n",
    "  * raw dot product values: `0, 0, 0, 0, 0, 0, 0, 0, 0, 100` (mnist, output label is '9')\n",
    "  * with `sigmoid`: `.50, .50, .50, .50, .50, .50, .50, .50, .50, .99`   \n",
    "    network seems less sur that it's a 9, seems to think there's a 50% chance it could be any of the other digits\n",
    "  * with `tanh`: `0, 0, 0, 0, 0, 0, 0, 0, 0, 1`   \n",
    "    9 is the highest, and the network doesn't even suspect it's any of the other possible digits   \n",
    "* this is a flaw of `sigmoid` that can have serious effects during backpropagation   \n",
    "  example: mean squared error calculated on `sigmoid`'s output is `.25, .25, .25, .25, .25, .25, .25, .25, .25, .00`   \n",
    "  weights would be massively updated even though the network predicted perfectly!   \n",
    "  why? because for `sigmoid` to reach 0 error, it doesn't just have to predict the highest positive number for the true output, it also has to predict 0 everywhere else.      \n",
    "<br/>   \n",
    "\n",
    "* `softmax` = \"which digits seems like the best fit for this input?\"\n",
    "* `sigmoid` = \"you better believe that it's only digit 9 and doesn't have anything in common with the other digits\"\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "## Different inputs share characteristics\n",
    "\n",
    "* different input nodes can have overlapping features (ex. digits '2' and '3' both have a top curve)\n",
    "* as a general rule, similar inputs create similar outputs\n",
    "* training with `sigmoid` would penalize the network for trying to predict a 2 based on this input (top curve),  \n",
    "  because by doing so, it would be looking for the same input it does for 3s   \n",
    "  when input would be 3, the 2 would get some probability (because of that top curve), and vice versa\n",
    "* side effect: lots of images share pixels in the middle of images, so network tries to focus on the edges\n",
    "* these can be the best **_individual indicators_** for a label\n",
    "* but the best overall is a network that sees **_the entire shape_** for what it is\n",
    "* individual indicators can be accidentally triggered by a different input\n",
    "* network isn't learning the true essence of a label,   \n",
    "  because it needs to learn 2 and *not 1*, *not 3*, *not 4*, etc.   \n",
    "<br/>   \n",
    "\n",
    "* Good activation doesn't penalize labels that are similar\n",
    "* it pays attention to all information that can be indicative of any potential input\n",
    "* `softmax` works better both in theory and practice\n",
    "\n",
    "## softmax\n",
    "\n",
    "* raises each input value $x$ exponentially ($e^x$) and then divides by the layer's sum\n",
    "* notice: turns every prediction into a positive number,   \n",
    "  **negative numbers** turn into **very small positive numbers** (between 0 and 1),   \n",
    "  and **big numbers** turn into **very big numbers**\n",
    "* next step: compute sum of the layer and divide each node's value by that sum,   \n",
    "  which makes every number 0 except the value for the predicted label\n",
    "* with `softmax`, the higher the network predicts one label, the lower it predicts all others   \n",
    "  = increase the **_sharpness of attenuation_**: encourages the network to predict one output with very high probability\n",
    "* to adjust how aggressively it does this, use numbers slightly higher of lower than $e$ when exponentiating:\n",
    "  * lower numbers -> lower attenuation\n",
    "  * higer numbers -> higher attenuation   \n",
    "  * most people just stick with $e$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation installation instructions\n",
    "\n",
    "* Adding an activation function to a layer in forward propagation is relatively straightforward\n",
    "* But properly compensating for the activation function in backpropagation is a bit more nuanced   \n",
    "\n",
    "### In forward propagation\n",
    "\n",
    "* Apply the function to each node **in the input** of the layer.\n",
    "* The **_input to a layer_** is the value before nonlinearity:   \n",
    "  in this example, the input to `layer_1` is `np.dot(layer_0, weights_0_1)`, not `layer_0` (the previous layer)\n",
    "```\n",
    "layer_0 = test_images[m:m+1]\n",
    "layer_1 = relu(np.dot(layer_0, weights_0_1))\n",
    "layer_2 = np.dot(layer_1, weights_1_2)\n",
    "```\n",
    "\n",
    "### In backpropagation\n",
    "\n",
    "* When you backpropagate, in order to generate `layer_1_delta`:\n",
    "  * multiply the backpropagated `delta` from `layer_2`: `layer_2_delta.dot(weights_1_2_.T)`\n",
    "  * by the slope of `relu` *at the point predicted in forward propagation*: `relu2deriv(layer_1)`\n",
    "  ```\n",
    "  layer_1_delta = layer_2_delta.dot(weights_1_2.T) * relu2deriv(layer_1)\n",
    "  ```\n",
    "  * `relu2deriv` takes the output of `relu` and calculates the slope of `relu` at that point   \n",
    "<br/>   \n",
    "\n",
    "* The slope is an indicator of how much a *tiny* change to the input affects the output   \n",
    "  You wan to modify the incoming `delta` (from the following layer)   \n",
    "  to take into account whether a weight update before this node would have any effect)\n",
    "* This step encourages the network to leave weights alone if adjusting them will have little to no effect   \n",
    "  by multiplying it by the slope (it's no different from `sigmoid`)   \n",
    "<br/>   \n",
    "\n",
    "#### Multiplying the backpropagated delta by the layer's slope\n",
    "\n",
    "* `delta` on a neuron tells the weights whether they should move or not:   \n",
    "  if moving them has no effect, they should be left alone\n",
    "* that's what `relu` does: either turn on or turn off\n",
    "* `sigmoid`: more nuanced\n",
    "  * it's sensitivity to change in the input slowly increases as **input approaches 0 from either direction** (negative or positive) (see the sigmoid curve)\n",
    "  * and **very positive and very positive** inputs approach a slope of very near 0\n",
    "* so small changes to the incoming weights become **_less relevant_** to the neuron's error at this training example   \n",
    "  = many hidden nodes are irrelevant to the accurate prediction of a particular label,   \n",
    "  but we shouldn't mess with their weights because we could corrupt their usefulness elsewhere (predicting another digit)\n",
    "* inversely, it also creates **_stickiness_**: \n",
    "  * weights previously updated a lot in one direction (for similar trng examples) confidently predict a high or low value\n",
    "  * these nonlinearities help make it harder for occasional erroneous training examples to corrupt intelligence that has been reinforced many times\n",
    "    \n",
    "#### Converting output to slope (derivative)\n",
    "\n",
    "* most great activations can convert their output to their slope\n",
    "* adding an activation to a layer (`relu`) changes how to compute `delta` for that layer (`relu2deriv`)\n",
    "* unlike in calculus, most great activation functions use the **_output_** of the layer (at forward propagation) to compute the derivative  \n",
    "<br/>  \n",
    "* Functions and their derivatives:  \n",
    "  * `input`: Numpy `ndarray`, the input layer\n",
    "  * `output`: prediction\n",
    "  * `deriv`: derivative of the vector of activation derivatives correponding to the derivative of the activation at each node\n",
    "  * `target_pred`: vector of expected values (typically for the correct label position, 0 everywhere else)   \n",
    "<br/>   \n",
    "  \n",
    "<table style=\"width: 80%; align:center;\">\n",
    "    <tr>\n",
    "        <th style=\"width: 20%; text-align: center;\">Function</th>\n",
    "        <th style=\"width: 40%; text-align: center;\">Forward propagation</th>\n",
    "        <th style=\"width: 40%; text-align: center;\">Backpropagation delta</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align: center\">relu</td>\n",
    "        <td>\n",
    "            <p>ones_and_zeros = input > 0</p>\n",
    "            <p>output = input * ones_and_zeros</p>\n",
    "        </td>\n",
    "        <td>\n",
    "            <p>mask = output > 0</p>\n",
    "            <p>deriv = output * mask</p>\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align: center\">sigmoid</td>\n",
    "        <td>\n",
    "            <p>output = 1 / (1 + np.exp(-input))</p>\n",
    "        </td>\n",
    "        <td>\n",
    "            <p>deriv = output * (1 - output)</p>\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align: center\">tanh</td>\n",
    "        <td>\n",
    "            <p>output = np.tanh(input</p>\n",
    "        </td>\n",
    "        <td>\n",
    "            <p>deriv = 1 - (output ** 2)</p>\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align: center\">softmax</td>\n",
    "        <td>\n",
    "            <p>temp = np.exp(intput)</p>\n",
    "            <p>output /= np.sum(temp)</p>\n",
    "        </td>\n",
    "        <td>\n",
    "            <p>temp = output - target_pred</p>\n",
    "            <p>output = temp / len(target_pred)</p>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example code\n",
    "\n",
    "* `tanh` should be better for hidden layers   \n",
    "  `softmax` should be better for output layers\n",
    "* but things are not always as simple   \n",
    "\n",
    "Adjustments to tune the network properly with these activations:\n",
    "* for `tanh`: reduce the std dev of incoming weights:\n",
    "  * when initializing weights, `np.random.random` creates a matrix with numbers between 0 and 1\n",
    "  * by multiplying by 0.2 and subtracting 0.1, we **rescale** the random range between -0.1 and 0.1\n",
    "  * worked great for `relu` but less optimal for `tanh`:\n",
    "    * `tanh` needs narrower random initialization\n",
    "    * so we adjust it to be between -0.01 and 0.01\n",
    "\n",
    "* Removed `error` calculation because `softmax` is best used with the **_cross entropy_** error function   \n",
    "  (seen later in the book)\n",
    "* Much higher `alpha` was required to reach a good score within 300 iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "from keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, y_train = x_train[0:1000], y_train[0:1000]\n",
    "\n",
    "# IMAGES\n",
    "train_images = x_train.reshape(1000, 28*28) / 255\n",
    "test_images = x_test.reshape(len(x_test), 28*28) / 255\n",
    "\n",
    "# LABELS: one_hot vectors\n",
    "# label '4' = [0, 0, 0, 0, 1, 0, 0...]\n",
    "train_labels = np.zeros((len(y_train), 10))\n",
    "for i,l in enumerate(y_train):\n",
    "    train_labels[i][l] = 1\n",
    "\n",
    "test_labels = np.zeros((len(y_test), 10))\n",
    "for i,l in enumerate(y_test):\n",
    "    test_labels[i][l] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Network parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "tanh = lambda x: np.tanh(x)\n",
    "tanh2deriv = lambda output: 1 - (output ** 2)\n",
    "def softmax(x):\n",
    "    temp = np.exp(x)\n",
    "    return temp / np.sum(temp, axis=1, keepdims=True)\n",
    "\n",
    "alpha = 2\n",
    "iterations = 701\n",
    "hidden_size = 100\n",
    "batch_size = 100\n",
    "\n",
    "pixels_per_image = 784\n",
    "num_labels = 10\n",
    "num_train_images = len(train_images)\n",
    "num_test_images = len(test_images)\n",
    "\n",
    "weights_0_1 = 0.02 * np.random.random((pixels_per_image, hidden_size)) - 0.01\n",
    "weights_1_2 = 0.2 * np.random.random((hidden_size, num_labels)) - 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning and predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0 Train-Acc 0.156 Test-Acc 0.394\n",
      "Iter 20 Train-Acc 0.732 Test-Acc 0.702\n",
      "Iter 40 Train-Acc 0.794 Test-Acc 0.766\n",
      "Iter 60 Train-Acc 0.849 Test-Acc 0.810\n",
      "Iter 80 Train-Acc 0.867 Test-Acc 0.831\n",
      "Iter 100 Train-Acc 0.883 Test-Acc 0.840\n",
      "Iter 120 Train-Acc 0.901 Test-Acc 0.848\n",
      "Iter 140 Train-Acc 0.905 Test-Acc 0.852\n",
      "Iter 160 Train-Acc 0.925 Test-Acc 0.857\n",
      "Iter 180 Train-Acc 0.933 Test-Acc 0.861\n",
      "Iter 200 Train-Acc 0.926 Test-Acc 0.864\n",
      "Iter 220 Train-Acc 0.93 Test-Acc 0.866\n",
      "Iter 240 Train-Acc 0.938 Test-Acc 0.868\n",
      "Iter 260 Train-Acc 0.945 Test-Acc 0.868\n",
      "Iter 280 Train-Acc 0.949 Test-Acc 0.869\n",
      "Iter 300 Train-Acc 0.95 Test-Acc 0.871\n",
      "Iter 320 Train-Acc 0.95 Test-Acc 0.872\n",
      "Iter 340 Train-Acc 0.952 Test-Acc 0.873\n",
      "Iter 360 Train-Acc 0.948 Test-Acc 0.872\n",
      "Iter 380 Train-Acc 0.958 Test-Acc 0.874\n",
      "Iter 400 Train-Acc 0.951 Test-Acc 0.873\n",
      "Iter 420 Train-Acc 0.958 Test-Acc 0.875\n",
      "Iter 440 Train-Acc 0.955 Test-Acc 0.877\n",
      "Iter 460 Train-Acc 0.962 Test-Acc 0.878\n",
      "Iter 480 Train-Acc 0.958 Test-Acc 0.878\n",
      "Iter 500 Train-Acc 0.953 Test-Acc 0.878\n",
      "Iter 520 Train-Acc 0.949 Test-Acc 0.878\n",
      "Iter 540 Train-Acc 0.956 Test-Acc 0.878\n",
      "Iter 560 Train-Acc 0.959 Test-Acc 0.878\n",
      "Iter 580 Train-Acc 0.973 Test-Acc 0.878\n",
      "Iter 600 Train-Acc 0.958 Test-Acc 0.877\n",
      "Iter 620 Train-Acc 0.956 Test-Acc 0.878\n",
      "Iter 640 Train-Acc 0.954 Test-Acc 0.878\n",
      "Iter 660 Train-Acc 0.961 Test-Acc 0.878\n",
      "Iter 680 Train-Acc 0.958 Test-Acc 0.878\n",
      "Iter 700 Train-Acc 0.955 Test-Acc 0.878\n"
     ]
    }
   ],
   "source": [
    "for j in range(iterations):\n",
    "    \n",
    "    # Predicting on training data\n",
    "    \n",
    "    train_correct_cnt = 0\n",
    "    \n",
    "    for i in range(int(num_train_images / batch_size)):\n",
    "        batch_start, batch_end = (i * batch_size), ((i+1) * batch_size)\n",
    "        \n",
    "        layer_0 = train_images[batch_start:batch_end]\n",
    "        layer_1 = tanh(np.dot(layer_0, weights_0_1))\n",
    "        dropout_mask = np.random.randint(2, size=layer_1.shape)\n",
    "        layer_1 *= dropout_mask * 2\n",
    "        layer_2 = softmax(np.dot(layer_1, weights_1_2))\n",
    "        \n",
    "        for k in range(batch_size):\n",
    "            train_correct_cnt += int( np.argmax(layer_2[k:k+1]) == \\\n",
    "                                      np.argmax(train_labels[batch_start+k:batch_start+k+1]) )\n",
    "        train_acc = train_correct_cnt / float(num_train_images)\n",
    "            \n",
    "        layer_2_delta = (train_labels[batch_start:batch_end] - layer_2) / (batch_size * layer_2.shape[0])\n",
    "        layer_1_delta = layer_2_delta.dot(weights_1_2.T) * tanh2deriv(layer_1)\n",
    "        layer_1_delta *= dropout_mask\n",
    "            \n",
    "        weights_1_2 += alpha * layer_1.T.dot(layer_2_delta)\n",
    "        weights_0_1 += alpha * layer_0.T.dot(layer_1_delta)\n",
    "        \n",
    "    # Predicting on test data\n",
    "    \n",
    "    test_correct_cnt = 0\n",
    "    \n",
    "    for m in range(num_test_images):\n",
    "        layer_0 = test_images[m:m+1]\n",
    "        layer_1 = tanh(np.dot(layer_0, weights_0_1))\n",
    "        layer_2 = np.dot(layer_1, weights_1_2)\n",
    "        \n",
    "        test_correct_cnt += int( np.argmax(layer_2) == np.argmax(test_labels[m:m+1]) )\n",
    "        test_acc = test_correct_cnt / float(num_test_images)\n",
    "        \n",
    "    if (j % 20 == 0) or (j == iterations-1):\n",
    "        print('Iter ' + str(j) + ' Train-Acc ' + str(train_acc)[0:5] + ' Test-Acc ' + str(test_acc)[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/device:CPU:0', '/device:XLA_CPU:0']\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_devices():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos]\n",
    "\n",
    "print(get_available_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
