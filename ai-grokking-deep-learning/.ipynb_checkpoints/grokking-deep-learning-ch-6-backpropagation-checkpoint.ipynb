{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BACKPROPAGATION\n",
    "\n",
    "## Full, batch and stochastic gradient descent\n",
    "\n",
    "* **Stochastic**: makes a prediction and updates weights one training example at a time\n",
    "* **Full**: makes a prediction and updates weights after all training examples,    \n",
    "calculate it as `weight_delta`, the average of all `weight_deltas`\n",
    "* **Batch**: makes a prediction and updates weights after `batch_size` training examples   \n",
    "`batch_size` usually between 8 and 256\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- 0 -- 0 --\n",
      "Error      0.040\n",
      "Prediction -0.200\n",
      "Delta      -0.200\n",
      "Weights    ['0.520', '0.480', '-0.680']\n",
      "-- 0 -- 1 --\n",
      "Error      1.440\n",
      "Prediction -0.200\n",
      "Delta      -1.200\n",
      "Weights    ['0.520', '0.600', '-0.560']\n",
      "-- 0 -- 2 --\n",
      "Error      0.314\n",
      "Prediction -0.560\n",
      "Delta      -0.560\n",
      "Weights    ['0.520', '0.600', '-0.504']\n",
      "-- 0 -- 3 --\n",
      "Error      0.147\n",
      "Prediction 0.616\n",
      "Delta      -0.384\n",
      "Weights    ['0.558', '0.638', '-0.466']\n",
      "-- 0 -- 4 --\n",
      "Error      0.684\n",
      "Prediction 0.173\n",
      "Delta      -0.827\n",
      "Weights    ['0.558', '0.721', '-0.383']\n",
      "-- 0 -- 5 --\n",
      "Error      0.031\n",
      "Prediction 0.176\n",
      "Delta      0.176\n",
      "Weights    ['0.541', '0.721', '-0.400']\n",
      "-- 10 -- 0 --\n",
      "Error      0.001\n",
      "Prediction 0.037\n",
      "Delta      0.037\n",
      "Weights    ['0.166', '1.058', '-0.136']\n",
      "-- 10 -- 1 --\n",
      "Error      0.006\n",
      "Prediction 0.922\n",
      "Delta      -0.078\n",
      "Weights    ['0.166', '1.066', '-0.128']\n",
      "-- 10 -- 2 --\n",
      "Error      0.016\n",
      "Prediction -0.128\n",
      "Delta      -0.128\n",
      "Weights    ['0.166', '1.066', '-0.116']\n",
      "-- 10 -- 3 --\n",
      "Error      0.014\n",
      "Prediction 1.117\n",
      "Delta      0.117\n",
      "Weights    ['0.154', '1.054', '-0.127']\n",
      "-- 10 -- 4 --\n",
      "Error      0.005\n",
      "Prediction 0.927\n",
      "Delta      -0.073\n",
      "Weights    ['0.154', '1.062', '-0.120']\n",
      "-- 10 -- 5 --\n",
      "Error      0.001\n",
      "Prediction 0.034\n",
      "Delta      0.034\n",
      "Weights    ['0.151', '1.062', '-0.123']\n",
      "-- 20 -- 0 --\n",
      "Error      0.000\n",
      "Prediction -0.003\n",
      "Delta      -0.003\n",
      "Weights    ['0.063', '1.051', '-0.065']\n",
      "-- 20 -- 1 --\n",
      "Error      0.000\n",
      "Prediction 0.986\n",
      "Delta      -0.014\n",
      "Weights    ['0.063', '1.052', '-0.064']\n",
      "-- 20 -- 2 --\n",
      "Error      0.004\n",
      "Prediction -0.064\n",
      "Delta      -0.064\n",
      "Weights    ['0.063', '1.052', '-0.057']\n",
      "-- 20 -- 3 --\n",
      "Error      0.003\n",
      "Prediction 1.058\n",
      "Delta      0.058\n",
      "Weights    ['0.057', '1.046', '-0.063']\n",
      "-- 20 -- 4 --\n",
      "Error      0.000\n",
      "Prediction 0.983\n",
      "Delta      -0.017\n",
      "Weights    ['0.057', '1.048', '-0.061']\n",
      "-- 20 -- 5 --\n",
      "Error      0.000\n",
      "Prediction -0.004\n",
      "Delta      -0.004\n",
      "Weights    ['0.057', '1.048', '-0.061']\n",
      "-- 30 -- 0 --\n",
      "Error      0.000\n",
      "Prediction -0.004\n",
      "Delta      -0.004\n",
      "Weights    ['0.029', '1.027', '-0.032']\n",
      "-- 30 -- 1 --\n",
      "Error      0.000\n",
      "Prediction 0.995\n",
      "Delta      -0.005\n",
      "Weights    ['0.029', '1.028', '-0.031']\n",
      "-- 30 -- 2 --\n",
      "Error      0.001\n",
      "Prediction -0.031\n",
      "Delta      -0.031\n",
      "Weights    ['0.029', '1.028', '-0.028']\n",
      "-- 30 -- 3 --\n",
      "Error      0.001\n",
      "Prediction 1.028\n",
      "Delta      0.028\n",
      "Weights    ['0.026', '1.025', '-0.031']\n",
      "-- 30 -- 4 --\n",
      "Error      0.000\n",
      "Prediction 0.994\n",
      "Delta      -0.006\n",
      "Weights    ['0.026', '1.026', '-0.031']\n",
      "-- 30 -- 5 --\n",
      "Error      0.000\n",
      "Prediction -0.004\n",
      "Delta      -0.004\n",
      "Weights    ['0.027', '1.026', '-0.030']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "weights = np.array([0.5, 0.48, -0.7])\n",
    "alpha = 0.1\n",
    "\n",
    "# input: training data\n",
    "streetlights = np.array( [[1, 0, 1],\n",
    "                          [0, 1, 1],\n",
    "                          [0, 0, 1],\n",
    "                          [1, 1, 1],\n",
    "                          [0, 1, 1],\n",
    "                          [1, 0, 1]] )\n",
    "\n",
    "# input: training labels\n",
    "walk_vs_stop = np.array([0, 1, 0, 1, 1, 0])\n",
    "\n",
    "# test input\n",
    "input = streetlights[0]         # [1, 0, 1]\n",
    "target_pred = walk_vs_stop[0]   # 0\n",
    "\n",
    "for iteration in range(40):\n",
    "    error_for_all_lights = 0\n",
    "    \n",
    "    # Stochastic gradient descent:\n",
    "    # prediction & weight update for each training example\n",
    "    for row_index in range(len(walk_vs_stop)):\n",
    "        \n",
    "        # current training example (example + label)\n",
    "        input = streetlights[row_index]\n",
    "        target_pred = walk_vs_stop[row_index]\n",
    "        \n",
    "        pred = input.dot(weights)\n",
    "        \n",
    "        error = (pred - target_pred) ** 2\n",
    "        error_for_all_lights += error\n",
    "        \n",
    "        delta = pred - target_pred\n",
    "        \n",
    "        weights = weights - (alpha * (input * delta))\n",
    "        \n",
    "        if (iteration % 10 == 0):\n",
    "            print(f'-- {iteration} -- {row_index} --')\n",
    "            print(f'Error      {error:.3f}')\n",
    "            print(f'Prediction {pred:.3f}')\n",
    "            print(f'Delta      {delta:.3f}')\n",
    "            print(f\"Weights    {[f'{w:.3f}' for w in weights]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does the network identify correlation?\n",
    "\n",
    "* NN identifies **_correlation_** between the output and some of the input,   \n",
    "and **_randomness_** between the output and the rest of the inputs\n",
    "* each example/batch/iteration asserts either **_up pressure_** or **_down pressure_** on the weights\n",
    "* pressure comes from the data:\n",
    "  * each node **_independently_** tries to correctly predict the output\n",
    "  * only **_cross communication_** between nodes occurs in that all weights mush share the same error measure\n",
    "  * weight update = multiplying this shared error measure by each respective input\n",
    "  * each weight is trying to compensate for error\n",
    "* error attribution: given a shared error, the nn needs to figure out:\n",
    "  * which weights contributed = **_relevant nodes for prediction_** -> up pressure\n",
    "  * which didn't = **_irrelevant nodes_** -> down pressure\n",
    "  \n",
    "  \n",
    "The prediction is a **weighted sum of the input**\n",
    "  \n",
    "### Edge case: Overfitting\n",
    "\n",
    "* If a particular configuration of weights **_accidentally_** creates perfect correlation between prediction and output dataset (`error == 0`) without giving the heaviest weights to the best inputs, the NN will **_stop learning_**\n",
    "* NNs can find many different configurations that will correctly predict for a subset of training data\n",
    "\n",
    "### Edge case: Conflicting pressure\n",
    "\n",
    "* As other nodes learn, they absorb some of the error; they **absorb part of the correlation**.\n",
    "* They cause the network to predict with **moderate** correlative power, which reduces the error.\n",
    "* The other weights then only try to adjust their weights to correctly predict **what's left**.\n",
    "\n",
    "**Regularization**\n",
    "* Forces weights with conflicting pressure to move toward 0\n",
    "* If a weight has equal pressure upward and downward, it **_shouldn't stay on_** = noise = useless\n",
    "* So only weights with strong correlation should stay on\n",
    "* Side effects = **faster training** (fewer iterations)\n",
    "\n",
    "### Edge case: Data doesn't have correlation\n",
    "\n",
    "* All inputs have conflicting pressure (equal pressure between up and down)\n",
    "* Create correlation with an intermediate network = **backpropagation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "* Add an intermediate network to create correlation: intermediate dataset that has correlation with output:\n",
    "  * `layer_0` input\n",
    "  * `layer_1` intermediate network (new input)\n",
    "  * `layer_2` prediction   \n",
    "<br/>       \n",
    "   \n",
    "* If `layer_2`is too high by *x* amount, how do we know which values at `layer_1` contributed to the error?\n",
    "  * the ones with **_higher weights_** among `weights_1_2`\n",
    "  * `weights_1_2` exactly describe:\n",
    "    * how much each `layer_1` node contributed to `layer_2` prediction\n",
    "    * in other words, how much each contributed to `layer_2` error   \n",
    "<br/>       \n",
    "   \n",
    "* We use the `delta` at `layer_2` to figure out the `delta` at `layer_1` = `delta * weights_1_2`   \n",
    "\n",
    "  = multiply a node's `delta` by its input `value`, then adjust its `weight` by that much (scaled with `alpha`)      \n",
    "  = If you want this node to be *x* amount higher:\n",
    "    * then each of these previous nodes needs to be `x * weights_1_2` amount higher/lower,   \n",
    "    * because these weights were amplifying the prediction by `weights_1_2` times    \n",
    "  \n",
    "  = Prediction logic in reverse   \n",
    "\n",
    "### Linear vs nonlinear\n",
    "\n",
    "* The intermediate layer:\n",
    "  * Each node in the intermediate layer has a certain amount of correlation with each input node\n",
    "  * If the weight from an input to the middle layer is 1.0, then it subscribes to 100% for that node's movement\n",
    "  * So it that node goes up by 0.3, the middle node will follow\n",
    "  * If the weight from an input node is 0.5, each node in the middle layer subscribes to 50% of that input node's movement\n",
    "  * only way for in middle node to escape the correlation of a particular input node is to take correlation from another input node    \n",
    "  = Each middle node subscribes to a little correlation from each input node   \n",
    "* We need the middle layer to **selectively correlate** with the input nodes = **_conditional correlation_**  \n",
    "  * middle nodes either correlate by `weight`% or not at all   \n",
    "   \n",
    "<p style=\"background:#DDEEEE;padding: 15px;\">\n",
    "<b>Nonlinearity</b> = if the node is negative, set if to 0\n",
    "<br/>\n",
    "By turning off any middle node whenever it would be negative, you allow the network to subscribe to correlation from the most important inputs.\n",
    "<br/>\n",
    "This type of nonlinearity is the simplest, it's called <b>relu</b>\n",
    "</p>\n",
    "\n",
    "Adjusting the weights to reduce the error over a series of training examples ultimately searches for correlation between the input and the output layers. If no correlation exists, then the error will never reach 0.\n",
    "\n",
    "Neural networks **search for correlation** between input and output **by adjusting weights**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "Error 1.414\n",
      "Prediction [[0.]]\n",
      "----\n",
      "Error 0.597\n",
      "Prediction [[0.27767701]]\n",
      "----\n",
      "Error 0.326\n",
      "Prediction [[0.19402478]]\n",
      "----\n",
      "Error 0.067\n",
      "Prediction [[0.04904907]]\n",
      "----\n",
      "Error 0.005\n",
      "Prediction [[0.0030369]]\n",
      "----\n",
      "Error 0.000\n",
      "Prediction [[0.]]\n",
      "----\n",
      "Error 0.000\n",
      "Prediction [[0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "def relu(x):\n",
    "    return (x > 0) * x\n",
    "\n",
    "def relu2deriv(output):\n",
    "    # returns 1 if true, otherwise 0\n",
    "    return output > 0\n",
    "\n",
    "# training data\n",
    "streetlights = np.array( [[1, 0, 1],\n",
    "                          [0, 1, 1],\n",
    "                          [0, 0, 1],\n",
    "                          [1, 1, 1]] )\n",
    "\n",
    "# training labels\n",
    "walk_vs_stop = np.array([1, 1, 0, 0]).T\n",
    "\n",
    "# learning parameters\n",
    "alpha = 0.2\n",
    "hidden_size = 4\n",
    "weights_0_1 = 2 * np.random.random((3, hidden_size)) - 1  # 3*4 matrix (features*labels)\n",
    "weights_1_2 = 2 * np.random.random((hidden_size, 1)) - 1  # 4*1 vector (labels)\n",
    "# dimensions are based on previous and next layers\n",
    "\n",
    "for iteration in range(61):\n",
    "    layer_2_error = 0\n",
    "    for i in range(len(streetlights)):\n",
    "        layer_0 = streetlights[i:i+1]\n",
    "        layer_1 = relu(np.dot(layer_0, weights_0_1))\n",
    "        layer_2 = np.dot(layer_1, weights_1_2)\n",
    "        \n",
    "        layer_2_error += np.sum((layer_2 - walk_vs_stop[i:i+1]) ** 2)\n",
    "        \n",
    "        layer_2_delta = layer_2 - walk_vs_stop[i:i+1]\n",
    "        layer_1_delta = layer_2_delta.dot(weights_1_2.T) * relu2deriv(layer_1)\n",
    "        # layer_1_delta = backpropagation: compute layer_1 delta given layer_2 delta\n",
    "        # relu2deriv(layer_1) = \n",
    "        #   if relu set the output of a layer_1 node to 0,\n",
    "        #   then that node didn't contribute to the error, \n",
    "        #   so it shouldn't have any impact on the weight update either,\n",
    "        #   so we set the delta of that node to 0.\n",
    "        \n",
    "        weights_1_2 -= alpha * layer_1.T.dot(layer_2_delta)\n",
    "        weights_0_1 -= alpha * layer_0.T.dot(layer_1_delta)\n",
    "    \n",
    "    if iteration % 10 == 0:\n",
    "        print('----')\n",
    "        print(f'Error {layer_2_error:.3f}')\n",
    "        print(f\"Prediction {layer_2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation =\n",
    "* Once you know how much the final prediction should move up or down (`delta`), you need to figure out how much each middle (`layer_1`) node should move up or down = **_intermediate predictions_**.\n",
    "* If `relu` set the output of a `layer_1` node to 0, then it didn't contribute to the `error`, so we set the `delta` of this node to 0 (`relu2deriv`).\n",
    "* Once you have the `delta` at `layer_1`, you can calculate a weight update:\n",
    "  * for each weight, multiply its input value by its output `delta`\n",
    "  * then increase the `weight` value by that much   \n",
    "<br/>   \n",
    "  \n",
    "* When there is no direct correlation between input and output, the intermediate layers try to **identify/create configurations of features** that may or may not correlate with the output (ex. for output=cat, an ear, cat eyes, cat hair)\n",
    "* The presense of many such configurations with give the final layer the information (correlation) it needs to correctly predict the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
