{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STRUCTURING MACHINE LEARNING PROJECTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><span style=\"background:yellow;padding: 15px;\">Orthogonalization</span></h1>\n",
    "<br/>\n",
    "    \n",
    "- = Know exactly **what to change** in order to achieve **what effect**\n",
    "- **orthogonal** = at 90° to each other\n",
    "- Example:\n",
    "  - old TV set with a lot of knobs to adjust the picture\n",
    "  - each knob does only one thing and does not affect other knobs   \n",
    "  -> ***orthogonal controls*** = at 90° to each other   \n",
    "  -> much easier to adjust one feature and then another separately\n",
    "  - same with cars: stearing wheel, accelerators, breaks, each has only one effect (speed, angle...)\n",
    "  - imagine if stearing wheel changed 0.3*angle - 0.8*speed!   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Machine Learning chain of assumptions*\n",
    "   \n",
    "- Each step has its own knobs to adjust   \n",
    "  => improving nns = diagnose what exactly is the bottleneck to your system's performance\n",
    "  \n",
    "   \n",
    "1. Fit model on **training set** until does best on cost function -> achieve ***human-level performance***\n",
    "  - train a bigger network\n",
    "  - switch to better optimization algorithm (Adam...)\n",
    "  - etc.\n",
    "     \n",
    "     \n",
    "2. Adjust model on **dev set** until does best on cost function\n",
    "    - regularization\n",
    "    - get a bigger training set  \n",
    "    \n",
    "    \n",
    "3. Hope model will do well on **test set** -> cost function\n",
    "    - get a bigger dev set, because if well on dev set but not test set, then model overtuned to dev set  \n",
    "    \n",
    "    \n",
    "4. Hope model will do well in **production**\n",
    "    - change the dev set -> dev set distribution not set correctly\n",
    "    - change the cost function -> cost function not measuring the right thing\n",
    "    \n",
    "    \n",
    "- **Note**. Andrew Ng: better not to use early stopping because not orthogonal, affects both how well you fit the training set and dev set (improves its performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Using a single evaluation metric*\n",
    "\n",
    "Makes it easier at the beginning of the project\n",
    "   \n",
    "   \n",
    "### Satisficing and Optimizing metric\n",
    "\n",
    "- **Optimizing**: metric we want to optimize\n",
    "- **Satisficing**: we want to reduce it to a certain threshold, under that we don't care\n",
    "- If you have N metrics:\n",
    "  - choose **one** as optimizing -> maximize it\n",
    "  - the others are satisficing -> as long as they reach a threshold, you don't care how they do\n",
    "   \n",
    "##### Example\n",
    "\n",
    "- **Precision**: of images recognized as cats, what percentage actually are cats?\n",
    "- **Recall**: of all images of cats, what percentage are actually recognized as such?   \n",
    "  \n",
    "-> If one model does better on precision while the other is better on recall, which one is better?    \n",
    "-> 2 evaluation metrics makes it difficult to quickly pick one   \n",
    "  \n",
    "  \n",
    "- **F1 score**: ***harmonic mean*** of precision and recall. `2 / (1/P + 1/R)`    \n",
    "  \n",
    "-> Well defined dev set + single evaluation metric => **speed up iterating**   \n",
    "\n",
    "##### Example\n",
    "\n",
    "- Idem if accuracy + running time -> combine them into single metric: `accuracy - 0.5*running_time`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Train/Dev/Test set distributions*\n",
    "\n",
    "- Choose **dev** and **test** sets:\n",
    "  - with **same distribution**\n",
    "  - that reflect **real world data**\n",
    "  - that you consider **important to do well** on\n",
    "  \n",
    "  \n",
    "<table align=left style=\"border:1px solid black\">\n",
    "    <tr>\n",
    "        <td style=\"text-align:left; width: 320px;\">Setting up <b>dev set + evaluation metric</b></td>\n",
    "        <td style=\"text-align:left; width: 350px;\">= <b>defining</b> what target you want to hit</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align:left; width: 320px;\">Setting up <b>dev set + test set</b> to same distribution</td>\n",
    "        <td style=\"text-align:left; width: 350px;\">= <b>aiming</b> at that target</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align:left; width: 320px;\">Setting up <b>training set</b></td>\n",
    "        <td style=\"text-align:left; width: 350px;\">= how well you will <b>hit</b> that target</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Size of the dev/test sets*\n",
    "\n",
    "- Choose test set big enough to give **high confidence** in overall system performance\n",
    "  -> Allows you to evaluate how good your final system is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *When to change dev/test sets and metrics*\n",
    "\n",
    "- metric does not match humal evaluation   \n",
    "see details in video\n",
    "\n",
    "#### Orthogonalization\n",
    "- First define your target: evaluation metric\n",
    "- Then worry about how well to do on this metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Why human-level performance?*\n",
    "\n",
    "- Before reaching human performance (ex 92%), **progress is fast**\n",
    "- Then progress **slows down** when it surpasses human performance\n",
    "- Ultimately reaches **optimum** level of performance (ex 97%) and **plateaus**\n",
    "  = **Bayes Optimal Error**: cannot be surpassed by any function mapping x -> y   \n",
    "  ex. noisy audio, blurry image\n",
    "  \n",
    "\n",
    "#### Why compare to human level performance?\n",
    "\n",
    "- Humans quite good at a lot of tasks.\n",
    "- So long as ML worse than humans, you can:\n",
    "  - get labeled data from humans\n",
    "  - gain insight from manual error analysis: why did a person get this right when system is wrong?\n",
    "  - better analysis of bias and variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Avoidable bias*\n",
    "  \n",
    "- Depending on what we estimate human error to be, we focus on different solutions\n",
    "  \n",
    "\n",
    "<table align=left style=\"border:1px solid black\">\n",
    "    <tr>\n",
    "        <td style=\"text-align:left; width:200px;\"></td>\n",
    "        <td style=\"text-align:left; width:200px;\"></td>\n",
    "        <th style=\"text-align:left; width:300px;\">Problem</th>\n",
    "        <th style=\"text-align:left; width:300px;\">Solution</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th style=\"text-align:left;\">Humans</th>\n",
    "        <td style=\"text-align:left; color:red;\"><b>1%</b></td>\n",
    "        <td rowspan=3 style=\"text-align:left;\">\n",
    "            huge gap between human and training shows that system is not fitting well on training set\n",
    "        </td>\n",
    "        <td rowspan=3 style=\"text-align:left;\">\n",
    "            <b>focus on bias</b>: deeper network, more hidden units, etc.\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th style=\"text-align:left;\">Training error</th>\n",
    "        <td style=\"text-align:left; color:red;\"><b>8%</b></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th style=\"text-align:left;\">Humans</th>\n",
    "        <td style=\"text-align:left;\">10%</td>\n",
    "    </tr>\n",
    "</table>   \n",
    "     \n",
    "<BR/>    \n",
    "\n",
    "<table align=left style=\"border:1px solid black\">\n",
    "    <tr>\n",
    "        <td style=\"text-align:left; width:200px;\"></td>\n",
    "        <td style=\"text-align:left; width:200px;\"></td>\n",
    "        <th style=\"text-align:left; width:300px;\">Problem</th>\n",
    "        <th style=\"text-align:left; width:300px;\">Solution</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th style=\"text-align:left;\">Humans</th>\n",
    "        <td style=\"text-align:left;\"><b>7.5%</b></td>\n",
    "        <td rowspan=3 style=\"text-align:left;\">\n",
    "            images so blurry even human cannot recognize\n",
    "        </td>\n",
    "        <td rowspan=3 style=\"text-align:left;\">\n",
    "            <b>focus on variance</b>. regularization, etc.\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th style=\"text-align:left;\">Training error</th>\n",
    "        <td style=\"text-align:left; color:red;\"><b>8%</b></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th style=\"text-align:left;\">Humans</th>\n",
    "        <td style=\"text-align:left; color:red;\"><b>10%</b></td>\n",
    "    </tr>\n",
    "</table>     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Understanding human-level performance*\n",
    "\n",
    "cf. details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Surpassing human-level performance*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Improving your model performance*\n",
    "\n",
    "**= Reducing (avoidable) bias and variance**\n",
    "- Different tools for each problem\n",
    "\n",
    "### Avoidable bias\n",
    "- train bigger model\n",
    "- train longer/better optimization algorithms (Momentum, RMSPorp, Adam)\n",
    "- improve NN architecture/hyperparameters search (RNN, CNN, different activations...)\n",
    "\n",
    "### Variance\n",
    "- get more data (helps generalize better to dev set)\n",
    "- regularization (L2, dropout, data augmentation)\n",
    "- improve NN architecture/hyperparameters search (RNN, CNN, different activations...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Error analysis*\n",
    "\n",
    "### Error analysis\n",
    "\n",
    "- manually examine errors made by the system to gain insights on what to do next\n",
    "\n",
    "### Clean up incorrectly labeled data\n",
    "\n",
    "- if you correct incorrectly labeled data in the dev set\n",
    "  - you should also do it for the test, so that they stay the same distribution\n",
    "  - you don't need to also do it for training data\n",
    "\n",
    "### Build your first system quickly, then interate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Mismatched training and dev/test set*\n",
    "\n",
    "### Training and testing on different distributions\n",
    "\n",
    "### Bias and variance with mismatched data distributions\n",
    "\n",
    "<img src=\"images/course2improvingnnsP2.png\" width=\"950\">\n",
    "\n",
    "### Addressing data mismatch\n",
    "\n",
    "- Carry out error analysis to try to understand difference between training and dev/test sets\n",
    "  - ex: cause noisy in-car audio\n",
    "- Make training data more similar; or collect data more similar to dev/test sets\n",
    "  - ex: find more audio with in-car noise - or synthesise clean audio with few recorder in-car noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Learning from multiple tasks: transfer learning*\n",
    "\n",
    "- Tasks A and B have the same input x\n",
    "- You have a lot more data for Task A than for Task B\n",
    "- Low level features from A could be helpful for learning B\n",
    "  \n",
    "\n",
    "- **few** data for a **specific** task, but **lots** of data for a similar more **generic** task\n",
    "- **few** data for a **general** task, but **lots** of data for a similar more **specific** task    \n",
    "  -> more data is always better\n",
    "  - Example:\n",
    "    - Low-level features (lines, dots, curves, small parts of objects)    \n",
    "      = useful info about how images look like in general\n",
    "    - can be helpful for **any** image related task\n",
    "   \n",
    "   \n",
    "- **Example application**:\n",
    "  - you train a nn for speech recognition (general task)\n",
    "  - now you want to use it for wake-word detection (specific task)\n",
    "  - **Process**:\n",
    "    1. remove the output layer   \n",
    "    2. replace it with a new output node, or with multiple new layers, specific to your task   \n",
    "    3. depending on how much data you have, you might \n",
    "        - retrain only the latest layers, \n",
    "        - or also retrain previous layers from the general task\n",
    "\n",
    "<img src=\"images/transfer-learning.png\" width=\"650\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Learning from multiple tasks: multi-task learning*\n",
    "\n",
    "- Training on set of tasks that could benefit from having shared low-level features\n",
    "- Output = vector of classes instead of only 1 class\n",
    "  - ex: [has_red_light, has_pedestrian, has_stop_sign...]\n",
    "- Usually same amount of data available for each task   \n",
    "  -> combining datasets for each task => bigger dataset\n",
    "- Only cases when multi-task learning not better than individual tasks -> network not big or deep enough\n",
    "- If some examples are missing some features, you can compute the cost such that it is not influenced by the fact that some entries haven't been labeled\n",
    "  \n",
    "  \n",
    "- **Example**: autonomous driving, simultaneously recognizing signs, pedestrians, cars, etc.\n",
    "  - each image labeled [1 0 0 1 0] means contains stop sign and red traffic light\n",
    "  - it's ok if some features are not labeled for some examples [1 ? 0 1 ?]\n",
    "  - 100.000 labeled images + 900.000 found online (not same distribution)\n",
    "    - training set = 80.000 of your labeled images + 900.000 internet images\n",
    "    - dev and test set = remaining 20.000 of your labeled images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *End-to-end learning*\n",
    "\n",
    "= multiple processing steps combined in a single one\n",
    "  \n",
    "- **traditional approach**: multi-steps\n",
    "  - speech recognition: audio -> features -> phonemes -> words -> transcript  \n",
    "- **deep learning approach**: single step from input to output\n",
    "  - speech recognition: audio -> transcript\n",
    "  - sometimes intermediate steps  \n",
    "- might need **lot of data** before it works well\n",
    "  - few data -> hand-designed components very useful\n",
    "  \n",
    "  \n",
    "- **Example**:\n",
    "  - MT for pairs of languages with lots of aligned data available\n",
    "  - English -> text analysis ... -> French **VS** English -> French\n",
    "\n",
    "\n",
    "- **Pros and cons**:\n",
    "  - **Pros**:\n",
    "    - let the data speak: more machine reasoning that human reasoning\n",
    "    - less hand-designing of components needed\n",
    "  - **Cons**:\n",
    "    - May need large amount of data\n",
    "    - Excludes potentially useful hand-designed components\n",
    "   \n",
    "   \n",
    "### Key question\n",
    "\n",
    "Do you have sufficient data to learn a function of the complexity needed to map directly x to y ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
