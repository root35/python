{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Latest innovations: speech recognition, machine translation, transformers>embeddings  \n",
    "\n",
    "# 1. BUILDING\n",
    "\n",
    "### Shallow NN\n",
    "1 hidden layer (logistic regression)   \n",
    "\n",
    "### Deep NN\n",
    "more than 1 hidden layers  \n",
    "first layers: learn low level features (common to all related tasks) -> simple tasks   \n",
    "last layers: learn high level features -> complex tasks  \n",
    "ex: audio signal -> phonemes -> words -> sentences   \n",
    "\n",
    "<img src=\"images/nn-basic-architecture.png\" width=\"650\">\n",
    "\n",
    "\n",
    "\n",
    "### Standard NNs \n",
    "\n",
    "### Convolutional NNs  \n",
    "\n",
    "### Recurrent NNs  \n",
    "\n",
    "### LSTM\n",
    "\n",
    "- Long Short-Term Memory\n",
    "- Type of RNN model that avoids the vanishing gradient problem by adding 'forget' gates.\n",
    "\n",
    "\n",
    "### Structured data\n",
    "supervised lrng  \n",
    "\n",
    "### Unstructured data\n",
    "audio, img  \n",
    "\n",
    "\n",
    "### Logistic Regression\n",
    "binary classification   \n",
    "\n",
    "\n",
    "### Linear Regression  \n",
    "\n",
    "\n",
    "### Deep Learning\n",
    "- Learn the parameters that minimize the error \n",
    "\n",
    "\n",
    "### Training in Deep Learning  \n",
    "  Adjust the parameters (w, b...) in order to minimize the cost function  \n",
    "- Adjust each weight in the **correct direction** by the **correct amount** so **error reduces to 0**\n",
    "\n",
    "\n",
    "### Parameters     \n",
    "  - weight (w)  \n",
    "  - bias (b)  \n",
    "\n",
    "\n",
    "### Hyperparameters   \n",
    "  (see course 2 week 3 Tuning process: order of importance)\n",
    "  they control the parameters and determine their final value   \n",
    "  - learning rate (`alpha`)   \n",
    "    - if alpha too large, we may overshoot the optimal value  \n",
    "    - if alpha too small, too many iterations to converge  \n",
    "    tuning = plot the curve with various values  \n",
    "    **See** ai specialization course 1 'w2-002-logistic-regression-with-nn'  \n",
    "  - regularization parameter (`lambda`)  \n",
    "    tuning = use the dev set   \n",
    "  - number of iterations   \n",
    "  - number of hidden layers\n",
    "  - number of hidden units\n",
    "  - choice of activation functions\n",
    "  - momentum (`beta`)\n",
    "  - mini-batch size\n",
    "  - `beta1`, `beta2`, `epsilon` in Adam optim algo\n",
    "  - learning rate decay\n",
    "\n",
    "\n",
    "### Learning rate\n",
    "- indicate at which pace the weights get updated \n",
    "- Often noted `α` or sometimes `η`, indicates at which pace the weights get updated.\n",
    "- Can be fixed or adaptively changed\n",
    "- Current most popular method: called Adam, a method that adapts the learning rate.\n",
    "- methods:\n",
    "  - Adam (most popular)\n",
    "  - Momentum\n",
    "  - RMSProp\n",
    "  \n",
    "<img src=\"images/optimization-adaptive-learning-rates.png\" width=\"650\">\n",
    "\n",
    "\n",
    "### Hyperparameters tuning     \n",
    "deep learning = empirical process   \n",
    "trying different values of hyperparameters and plotting (nb iterations * cost J) to find the best curve    \n",
    "best values for a given problem can change in time (cpu, data update...)   \n",
    "\n",
    "\n",
    "### Loss function: \n",
    "cost for one example  \n",
    "- **L1** loss function  \n",
    "- **L2** loss function \n",
    " \n",
    "### Cost function: \n",
    "  average loss functions for all examples = `J(w,b)` surface of the curve  \n",
    "\n",
    "<img src=\"images/loss-function-cross-entropy.png\" width=\"650\">\n",
    "\n",
    "\n",
    "\n",
    "### Gradient Descent algorithm: \n",
    "update w and b  \n",
    "\n",
    "\n",
    "###  Derivative/Slope \n",
    "  (partial derivative = J function of 2+ args): height/width   \n",
    "  for any value of a, if you nudge it by a certain amount, the slope will be ~\n",
    "  Derivatives at each step are used to compute derivatives at following steps  \n",
    "\n",
    "\n",
    "### Computation Graph: \n",
    "  organizes the computations:  \n",
    "  1. Initialize parameters / Define hyperparameters: nn architecture  \n",
    "  2. Loop for num_iterations:   \n",
    "    a. Forward propagation: compute output (linear->activation)   \n",
    "    b. Compute cost function   \n",
    "    c. Backward propagation: compute gradients/derivatives using cost   \n",
    "    d. Update parameters: gradient descent, using parameters, and grads from backprop    \n",
    "  4. Predict: use trained parameters to predict labels\n",
    "  **See** ai specialization course 1 'w2-002-logistic-regression-with-nn' + exos c1w4\n",
    "\n",
    "\n",
    "### Chain rule  \n",
    "  in backpropagation, if we change a, we change z, so we change J  \n",
    "\n",
    "\n",
    "### Backpropagation  \n",
    "[coursera ai: backpropagation](https://www.coursera.org/learn/neural-networks-deep-learning/lecture/6dDj7/backpropagation-intuition-optional).  \n",
    "  compute the derivative of the final output with respect to some other variable usually called ´dvar´ in code  \n",
    "  \n",
    "<img src=\"images/backpropagation.png\" width=\"650\">\n",
    "\n",
    "\n",
    "### Updating weights\n",
    "\n",
    "<img src=\"images/updating-weights.png\" width=\"650\">\n",
    "\n",
    "\n",
    "### Vectorization  \n",
    "  whenever possible, avoid explicit ´for´ loops  \n",
    "\n",
    "\n",
    "### Broadcasting in Python  \n",
    "  https://numpy.org/doc/stable/user/basics.broadcasting.html  \n",
    "\n",
    "\n",
    "### Softmax   \n",
    "  normalizing function used when algorithm needs to classify two or more classes    \n",
    "\n",
    "\n",
    "### Hidden\n",
    "not observed, not seen in the training set\n",
    "\n",
    "\n",
    "### Activation functions\n",
    "[coursera ai: activation functions](https://www.coursera.org/learn/neural-networks-deep-learning/lecture/4dDC1/activation-functions)    \n",
    "[coursera ai: nns-non-linear-activation-functions](https://www.coursera.org/learn/neural-networks-deep-learning/lecture/OASKH/why-do-you-need-non-linear-activation-functions): why do nns need non-linear activation functions   \n",
    "   -> because the nn would just be outputing a linear function of the input   \n",
    "   = standard linear regression.   \n",
    "linear activation function: g(z) = z -> outputs the input  \n",
    "[coursera ai: derivatives of activation functions](https://www.coursera.org/learn/neural-networks-deep-learning/lecture/qcG1j/derivatives-of-activation-functions)   \n",
    "  -> output values based on possible input values (max, min, 0)     \n",
    "    \n",
    "can be different for different layers in the same nn\n",
    "- **sigmoid**: [0..1] -> only better for binary classification (0 or 1)  \n",
    "- **tanh**: hyperbolic tangent [-1..1] -> 0 mean. always better for hidden layers. makes learning easier for the next layer  \n",
    "- **relu**: [1 or 0 (or 0,00000000000x)] *most used* -> faster learning of all  \n",
    "- **leaky relu**\n",
    "\n",
    "<img src=\"images/activation-functions.png\" width=\"650\">\n",
    "\n",
    "\n",
    "### Initialization\n",
    "[coursera ai: random initialization](https://www.coursera.org/learn/neural-networks-deep-learning/lecture/XtFPI/random-initialization).   \n",
    "- **weights**: do not initialize with zeros -> all hidden units are symmetric, they all compute the same function. initialize with random values instead.   \n",
    "`np.random.randn((2,2)) * 0.01`   # '* 0.01' initializes to very small values   \n",
    "if weights too large, GD will be very slow (values in the flat parts of the activation functions)  \n",
    "- **bias**: can be initialized with zeros, as long as weights are note   \n",
    "  \n",
    "  \n",
    "### Overfitting\n",
    "\n",
    "- If a network is overfitting, you can augment the loss function by :\n",
    "    - choosing simpler nonlinearities\n",
    "    - smaller layer sizes\n",
    "    - shallower architectures\n",
    "    - larger datasets\n",
    "    - or more aggressive regularization techniques\n",
    "- All have a similar effect of the loss function and similar consequence on the behavior of the network\n",
    "   \n",
    "   \n",
    "   \n",
    "### Underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get your dimensions right\n",
    "\n",
    "[coursera ai: getting your dimensions right](https://www.coursera.org/learn/neural-networks-deep-learning/lecture/Rz47X/getting-your-matrix-dimensions-right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5,)\n",
      "[-1.48936219  1.13770586  2.16944225 -1.43955269  0.58857812]\n",
      "10.637790162716158\n",
      "\n",
      "[[ 0.72143301]\n",
      " [ 1.3055879 ]\n",
      " [ 1.46287306]\n",
      " [-0.44561694]\n",
      " [-0.31813234]]\n",
      "[[ 0.72143301  1.3055879   1.46287306 -0.44561694 -0.31813234]]\n",
      "[[ 0.52046558  0.9418942   1.05536491 -0.32148277 -0.22951117]\n",
      " [ 0.9418942   1.70455976  1.90990937 -0.58179209 -0.41534974]\n",
      " [ 1.05536491  1.90990937  2.13999759 -0.65188102 -0.46538723]\n",
      " [-0.32148277 -0.58179209 -0.65188102  0.19857446  0.14176516]\n",
      " [-0.22951117 -0.41534974 -0.46538723  0.14176516  0.10120819]]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-b22947bce8e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# A trick when you want to flatten a matrix X of shape (a,b,c,d)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# to a matrix X_flatten of shape (b ∗ c ∗ d, a) is to use:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mX_flatten\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m## Many software bugs in deep learning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# --- Rank 1 arrays (value,)\n",
    "a = np.random.randn(5)\n",
    "print(a.shape)\n",
    "print(a.T)                # a and a.T are the same\n",
    "print(np.dot(a, a.T))     # value\n",
    "\n",
    "# --- Column vectors: (value, 1) - Row vectors: (1, value)\n",
    "print(\"\")\n",
    "a = np.random.randn(5,1)  # 5*1 matrix\n",
    "print(a)\n",
    "print(a.T)                # inversed shape\n",
    "print(np.dot(a, a.T))     # vector\n",
    "assert(a.shape == (5,1))  # can help as DOCUMENTATION\n",
    "\n",
    "# --- ALWAYS use column or row vectors, not rank 1 arrays\n",
    "# OR reshape:\n",
    "a.reshape((5,1))\n",
    "\n",
    "# A trick when you want to flatten a matrix X of shape (a,b,c,d) \n",
    "# to a matrix X_flatten of shape (b ∗ c ∗ d, a) is to use:\n",
    "X_flatten = X.reshape(X.shape[0], -1).T\n",
    "\n",
    "## Many software bugs in deep learning \n",
    "## come from having matrix/vector dimensions that don't fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best NN perfs = scale\n",
    "  scale algorithms: very large NN (lots of hidden layers, of parameters, of connections)  \n",
    "+ scale data: very large amount of data  \n",
    "+ scale computation: CPU, GPU  \n",
    "ex: switch from sigmoid (lrng becomes really slow at extremities) to relu  \n",
    " -> GD works much faster  \n",
    " (Neural Networks and Deep Learning - Semaine 1 - Why is Deep Learning taking off?)  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
