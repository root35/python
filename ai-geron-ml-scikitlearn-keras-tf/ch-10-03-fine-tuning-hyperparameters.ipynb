{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINE TUNING NEURAL NETWORKS HYPERPARAMETERS\n",
    "\n",
    "* Flexibility = also one the main drawbacks: many params to tweak\n",
    "* Looking for the best params combinations:\n",
    "  * try many combinations manually (K-fold cross-validation)\n",
    "  * or use `GridSearchCV` and `RandomizedSearchCV` to explore the hyperparameter space   \n",
    "    wrap model in objects that mimic regular Scikit-Learn regressors\n",
    "  \n",
    "  \n",
    "# Simple method\n",
    "\n",
    "#### STEP 1\n",
    "Create a function that will build and compile a Keras model, given a set of hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(n_hidden=1, n_neurons=30, learning_rate=3e-3, input_shape=[8]):\n",
    "    \n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.InputLayer(input_shape=input_shape))\n",
    "    for layer in range(n_hidden):\n",
    "        model.add(keras.layers.Dense(n_neurons, activation='relu'))\n",
    "    model.add(keras.layers.Dense(1))\n",
    "    \n",
    "    optimizer = keras.optimizers.SGD(lr=learning_rate)\n",
    "    model.compile(loss='mse', optimizer=optimizer)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 2\n",
    "Create a `KerasRegressor` base on this `build_model()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)\n",
    "# no hyperparams defined here at creation, so will use default values defined in 'build_model()'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 3\n",
    "Train this Scikit-Learn Regressor using its `fit()` method,  \n",
    "then evaluate it using its `score()` method,  \n",
    "then predict using its `predict()` method  \n",
    "  \n",
    "* Notes:  \n",
    "  * Any extra param passed to `fit()` is passed to the underlying Keras model\n",
    "  * The score will be the opposite of the MSE because Scikit-Learn wants scores, not losses (i.e. highers should be better)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_reg.fit(X_train, y_train,\n",
    "              epochs=100,\n",
    "              validation_date=(X_valid, y_valid),\n",
    "              callbacks=[keras.callbacks.EarlyStopping(patience=10)])\n",
    "\n",
    "mse_test = keras_reg.score(X_test, y_test)\n",
    "y_pred = keras_reg.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 4\n",
    "Train and evaluated hundreds of variants and see which one performs better on validation set   \n",
    "Lots of hyperparams -> randomized search preferable to grid search (see chap. 2)\n",
    "\n",
    "* `RandomizedSearchCV()` use K-fold cross validation:\n",
    "  * does not use `X_valid` and `y_valid`\n",
    "  * they are only used for ealy stopping\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import reciprocal\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_distribs = {\n",
    "    'n_hidden'     : [0, 1, 2, 3],\n",
    "    'n_neurons'    : np.arange(1, 100),\n",
    "    'learning_rage': reciprocal(3e-4, 3e-2)\n",
    "}\n",
    "\n",
    "random_search_cv = RandomizedSearchCV(keras_reg, param_distribs, n_iter=10, cv=3)\n",
    "random_search_cv.fit(X_train, y_train,\n",
    "                     epochs=100,\n",
    "                     validation_data=(X_valid, y_valid),\n",
    "                     callbacks=[keras.callbacks.EarlyStopping(patience=10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* When exploration is over, you get access to the best params found, the best score, and the trained Keras model\n",
    "* And you can save the model, evaluate it on test set, and deploy to production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search_cv.best_params_\n",
    "random_search_cv.best_score_\n",
    "model = random_search_cv.best_estimator_.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources for and about hyperparameters optimization\n",
    "\n",
    "### Libraries\n",
    "\n",
    "* Hyperopt\n",
    "* Hyperas, kopt, Talos\n",
    "* Keras Tuner\n",
    "* Scikit-Optimize (skopt)\n",
    "* Spearmint\n",
    "* Hyperband\n",
    "* Sklearn-Deap\n",
    "\n",
    "\n",
    "### Articles\n",
    "\n",
    "* [DeepMind 2017](https://arxiv.org/abs/1711.09846): jointly optimize a population of models and their hyperparameters\n",
    "* [Google's AutoML](https://cloud.google.com/automl): **evolurionary approach** to search for hyperparameters + look for the best network architecture for the problem (cloud service)\n",
    "* [Google's post about evolutionary AutoML](https://ai.googleblog.com/2018/03/using-evolutionary-automl-to-discover.html):  search for best architecture\n",
    "* [Uber's post about Deep Neuroevolution technique](https://eng.uber.com/deep-neuroevolution/): replacing the Gradient Descent!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters tuning\n",
    "\n",
    "SEE: [Leslie Smith 2018 paper](https://arxiv.org/abs/1803.09820)\n",
    "\n",
    "## Number of hidden layers\n",
    "\n",
    "* NN with **one hidden layer** can model the most complex functions if it has **enough neurons**\n",
    "* But **Deep networks** have much more *parameter efficiency* than shallow ones:  \n",
    "  * can model complex functions using exponentially fewer neurons than shallow nets  \n",
    "  * -> can reach much better performance with same amount of training data\n",
    "  \n",
    "  \n",
    "* Real world data often structured in a hierarchical way, deep neural nets take advantage of it:\n",
    "  * **lower** hidden layers model **low-level structures** (line segments of various shapes and orientation)\n",
    "  * **intermediate** hidden layers combine these low-level structures to model **intermediate-level structures** (squares, circles, etc.)\n",
    "  * **highest** hidden layers and **output** layer combine intermediate structures to model **high-level structures** (e.g. faces)  \n",
    "  \n",
    "  \n",
    "* => Hierarchical architecture:\n",
    "    * allows DNNs to **converge faster** and improves their **ability to generalize** to new datasets\n",
    "    * allows **transfer learning**:\n",
    "      * reusing lower layers of a network in a new network with similar task\n",
    "      * weights and biases of new network can be initialized with weights and biases of first network (instead on random values)\n",
    "      * => new network doesn't have to learn low-level structures that occur in most pictures   \n",
    "        only learns the higher-level structures\n",
    "  \n",
    "  \n",
    "* Start with 2 hidden layers and ramp up the number of hidden layers until you start overfitting the training set\n",
    "* We most commonly use parts of a pretrained state-of-the-art network that performs similar tasks:   \n",
    "  => faster training, requires much less data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of neurons per hidden layer\n",
    "\n",
    "* **Input** and **output** layers: number neurons determined by type of input and output of the task   \n",
    "  Ex: MNIST data, 28*28=784 input neurons, 10 output neurons  \n",
    "  \n",
    "  \n",
    "* **Hidden** layers:\n",
    "  * sometimes same number of neurons in all (only one hyperparameter to tune)\n",
    "  * depending on dataset, it can help to make the **first hidden layer bigger** than others\n",
    "  \n",
    "  \n",
    "* You can increase the number of neurons until network starts overfitting\n",
    "* But more efficient: **\"stretch pants\" approach**\n",
    "  * pick a model with more layers and neurons that you actually need\n",
    "  * and use early stopping and other regularization techniques to prevent overfitting\n",
    "  * that way you avoid bottleneck layers that could ruin the model\n",
    "  * advantage: a layer with too few neurons would not have enough representational power to preserve all the useful information from inputs   \n",
    "    lots of neurons -> no information lost   \n",
    "    no matter how big the network, lost information can never be recovered\n",
    "  \n",
    "  \n",
    "=> In general, better to increase the number of layers instead of the number of neurons per layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning rate\n",
    "\n",
    "* **Most important hyperparameter**  \n",
    "<br/>   \n",
    "    \n",
    "* **Optimal** learning rate = about half of the maximum learning rate,  \n",
    "  i.e. learning rate above which the training algorithm diverges (ch. 4)  \n",
    "<br/>   \n",
    "    \n",
    "* **Method:**\n",
    "  * train model for few hundred iterations\n",
    "  * starting with very low lr (e.g. $10^-5$)\n",
    "  * gradually increase it to a very large value (e.g. 10)\n",
    "  * by multiplying the lr by a constant factor at each iteration (e.g. by $exp(log(10^6)/500)$)  \n",
    "    from $10^-5$ to 10 in 500 iterations\n",
    "  * plot the loss as a function of the learning rate (log scale for the lr):\n",
    "    * you should see it dropping first\n",
    "    * but after a while, lr will be too large, so loss will shoot back up\n",
    "    * **optimal** lr will be a bit lower that the turning point\n",
    "  * reinitialize your model and train it normally using the found lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizers\n",
    "\n",
    "Choosing a better optimizer that Mini-batch Gradient Descent (and tune its hyperparameters)  \n",
    "More in ch. 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch size\n",
    "\n",
    "* Can have **significant impact** on model's **performance** and **training time**  \n",
    "<br/>   \n",
    "   \n",
    "* **Large batch sizes:** \n",
    "  * GPUs can process them efficiently (ch. 19), so training with see more instances per second\n",
    "  * can lead to instabilities at the beginning of training, so model may not generalize as well as with small batch sizes\n",
    "  * **strategy:** use large batch size, using learning rate warmup,  \n",
    "    if training unstable or final perf disappointing, reduce batch size\n",
    "  * [training large batches](https://arxiv.org/abs/1705.08741) - [large mini-batch SGD](https://arxiv.org/abs/1706.02677)   \n",
    "<br/>   \n",
    "  \n",
    "* **Small batch sizes:** (2 to 32)\n",
    "  * Yann LeCun tweet: \"Friends don't let friends use mini-batches larger than 32\", citing [paper from 2018](https://arxiv.org/abs/1804.07612)\n",
    "  * = says that small batches lead to better models in less training time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation function\n",
    "\n",
    "* **Hidden layers**: `ReLU` good default\n",
    "* **Output layers**: depends on the task\n",
    "  * **For regression**:\n",
    "    * None if output value can be any range (house price)\n",
    "    * ReLU or softplus if positive outputs\n",
    "    * logistic or tanh if output within a range\n",
    "  * **For classification**:\n",
    "    * binary classification: single output, between 0 and 1 (estimated probability of positive class), logistic activation\n",
    "    * multilabel binary classification: multiple output neurons, don't necessarily add up to 1, logistic activation\n",
    "    * multiclass classification: one output neuron per class, softmax activation ensures that all estimated probas are between 0 andd 1, and add up to 1 (classes are exclusive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of iterations\n",
    "\n",
    "* Doesn't need to be tweaked, just use **early stopping** instead\n",
    "* **Optimal** learning rate depends of the other hyperparams (especially batch size),   \n",
    "  so if you modify any one of them, update learning rate as well"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
