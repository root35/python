{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARTIFICIAL NEURAL NETWORKS\n",
    "\n",
    "# Perceptron\n",
    "\n",
    "* **TLU** (threshold logic unit, or linear threshold unit, LTU):\n",
    "  * inputs and outputs are **numbers** (not binary on/off values)\n",
    "  * each input connection is associated with a **weight**\n",
    "  * TLU computes a **weighted sum** of its inputs,   \n",
    "    then applies a **step function** (later activation function) to that sum,  \n",
    "    and outputs the result  \n",
    "<br/>  \n",
    "  \n",
    "* **Hebb's rule**:\n",
    "  * *Cells that fire together, wire together*\n",
    "  * learning rule reinforces connections that help reduce the error:   \n",
    "    for every output neuron that produced a wrong prediction, if reinforces the weights from the inputs that would have contributed to the correct prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation\n",
    "\n",
    "* Rumelhart et al. 1985. \"Learning internal representations by error propagation\".   \n",
    "  First introduction of backpropagation algorithm\n",
    "<br/>\n",
    "  \n",
    "* Find out how each connection weight and each bias term should be tweaked in order to reduce the error.  \n",
    "  * just performs a regular Gradient Descent step\n",
    "  * **autodiff** = automatically computing gradients, the one used by backpropagation is *reverse-mode autodiff*  \n",
    "<br/> \n",
    "    \n",
    "* **IMPORTANT**:  \n",
    "\n",
    "  * initialize all hidden layers' connection weights **randomly**, or training will fail:\n",
    "    * to break the symmetry\n",
    "    * and allow backpropagation to train a diverse team of neurons\n",
    "  * if weights and biases initialized to zeros, all neurons in a given layer will be identical,   \n",
    "    thus backpropagation will affect them in exactly the same way so they will remain identical.  \n",
    "    i.e. model will act as if it had only one neuron per layer  \n",
    "    \n",
    "### Activation functions\n",
    "\n",
    "  * **Why?** To introduce some nonlinearity betweeen layers   \n",
    "    * if you chain several linear transformations, you get a linear transformation\n",
    "    * without nonlinearity between layers, even a deep stack of layers is equivalent to a single layer   \n",
    "      can't solve very complex problems\n",
    "  * **sigmoid** (logistic) function: $sigmoid(z) = 1 / (1 + exp(-z))$\n",
    "    * allows GD to make some progress at every step\n",
    "  * **hyperbolic tangent** function: $tanh(z) = (2 * s(2z)) - 1$\n",
    "    * S-shaped, continuous, differentiable\n",
    "    * output: -1 to 1\n",
    "    * range that makes each layer's output more or less centered around 0 at the beginning of training,   \n",
    "      which often **helps speed up convergence**\n",
    "  * **ReLU** (rectified linear unit) function: $ReLU(z) = max(0, z)$\n",
    "    * continuous, not differentiable at $z = 0$ (slope changes abruptly)\n",
    "    * its derivative is 0 for $z < 0$\n",
    "    * fast to compute -> has become the default\n",
    "    * no maximum value (helps reduce some issues during GD)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression MLPs\n",
    "\n",
    "* Prediction:\n",
    "  * single value (price of a house): one output neuron  \n",
    "  * multiple values: one output neuron per output dimension  \n",
    "<br/> \n",
    "  \n",
    "* **Do not** use any activation function for output neurons, so they can output any range of values\n",
    "* If you want **output always positive**, use ReLU in output layer, or softplus (smooth variant of ReLU)  \n",
    "  $softplus(z) = log(1 + exp(z))$\n",
    "* If you want **output within a range** of values, use the logistic function or hyperbolic tangent,   \n",
    "  then scale labels to the appropriate range (0 to 1 for logistic, -1 to 1 for tanh)\n",
    "* **Loss function** during training: mean squared error\n",
    "  * if lots of outliers in training: mean absolute error\n",
    "  * or Huber loss (combination of both):\n",
    "    * quadratic when error smaller that a threshold (typically 1)  \n",
    "      -> allows it to converge faster and be more precise that mean absolute error \n",
    "    * but linear when error larger that the threshold  \n",
    "      -> makes it less sensitive to outliers than mean squared error  \n",
    "\n",
    "### Typical regression MLP architecture:\n",
    "\n",
    "<table align='left' width=\"80%\">\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th width=\"30%\" style=\"text-align:left\">Hyperparameter</th>\n",
    "            <th style=\"text-align:left\">Typical value</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <th style=\"text-align:left\">nb. input neurons</th>\n",
    "            <td style=\"text-align:left\">one per input feature (eg. 28*28 for mnist)</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <th style=\"text-align:left\">nb. hidden layers</th>\n",
    "            <td style=\"text-align:left\">depends on the problem, but typically 1 to 5</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <th style=\"text-align:left\">nb. neurons per hidden layer</th>\n",
    "            <td style=\"text-align:left\">depends on the problem, but typically 10 to 100</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <th style=\"text-align:left\">nb. output neurons</th>\n",
    "            <td style=\"text-align:left\">1 per prediction dimension</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <th style=\"text-align:left\">hidden activation</th>\n",
    "            <td style=\"text-align:left\">ReLU (or SELU)</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <th style=\"text-align:left\">output activation</th>\n",
    "            <td style=\"text-align:left\">None, or ReLU/softplus (if positive outputs) or logistic/tanh (if bounded outputs)</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <th style=\"text-align:left\">loss function</th>\n",
    "            <td style=\"text-align:left\">MSE, or MAE/Huber (if outliers)</td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification MLPs\n",
    "\n",
    "* Prediction:\n",
    "  * binary classification: \n",
    "    * single output with logistic activation function:  \n",
    "    * output = between 0 and 1, interpreted as estimated probability of positive class  \n",
    "      proba negative class = 1 - output\n",
    "  * multilabel binary classification: \n",
    "    * multiple output neurons with logistic activation function\n",
    "    * output neurons do not necessarily add up to 1 -> output any combination of labels\n",
    "  * multiclass classification:\n",
    "    * each instance can be only a single class out of $x$ ones: \n",
    "    * one output neuron per class\n",
    "    * softmax activation on output layer: ensure that   \n",
    "      all estimated probas are between 0 andd 1,  \n",
    "      and add up to 1 (classes are exclusive)  \n",
    "<br/> \n",
    "  \n",
    "* Loss function: since we are predicting probability distributions, **cross-entropy** (*log loss*) is a good choice\n",
    "\n",
    "\n",
    "### Typical classification MLP architecture:\n",
    "\n",
    "<table align='left' width=\"80%\">\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th width=\"25%\" style=\"text-align:left\">Hyperparameter</th>\n",
    "            <th width=\"25%\"  style=\"text-align:left\">Binary classification</th>\n",
    "            <th width=\"25%\"  style=\"text-align:left\">Multilabel binary classification</th>\n",
    "            <th width=\"25%\"  style=\"text-align:left\">Multiclass classification</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <th style=\"text-align:left\">Input and hidden layers</th>\n",
    "            <td style=\"text-align:left\">Same as regression</td>\n",
    "            <td style=\"text-align:left\">Same as regression</td>\n",
    "            <td style=\"text-align:left\">Same as regression</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <th style=\"text-align:left\">Nb. output neurons</th>\n",
    "            <td style=\"text-align:left\">1</td>\n",
    "            <td style=\"text-align:left\">1 per label</td>\n",
    "            <td style=\"text-align:left\">1 per class</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <th style=\"text-align:left\">Output layer activation</th>\n",
    "            <td style=\"text-align:left\">Logistic</td>\n",
    "            <td style=\"text-align:left\">Logistic</td>\n",
    "            <td style=\"text-align:left\">Sotfmax</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <th style=\"text-align:left\">Loss function</th>\n",
    "            <td style=\"text-align:left\">Cross entropy</td>\n",
    "            <td style=\"text-align:left\">Cross entropy</td>\n",
    "            <td style=\"text-align:left\">Cross entropy</td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complex models\n",
    "\n",
    "* more complex architectures, or multiple inputs and outputs\n",
    "* use the **Functional API** instead of the **Sequential API**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf version   :  2.3.0\n",
      "keras version:  2.4.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "print('tf version   : ', tf.__version__)\n",
    "print('keras version: ', keras.__version__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
