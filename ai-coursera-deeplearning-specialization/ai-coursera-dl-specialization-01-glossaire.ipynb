{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Latest innovations: speech recognition, machine translation, transformers>embeddings  \n",
    "\n",
    "# 1. BUILDING\n",
    "\n",
    "### Shallow NN\n",
    "1 hidden layer (logistic regression)   \n",
    "\n",
    "### Deep NN\n",
    "more than 1 hidden layers  \n",
    "first layers: learn low level features (common to all related tasks) -> simple tasks   \n",
    "last layers: learn high level features -> complex tasks  \n",
    "ex: audio signal -> phonemes -> words -> sentences   \n",
    "\n",
    "### Standard NNs \n",
    "\n",
    "### Convolutional NNs  \n",
    "\n",
    "### Recurrent NNs  \n",
    "\n",
    "### Structured data\n",
    "supervised lrng  \n",
    "\n",
    "### Unstructured data\n",
    "audio, img  \n",
    "\n",
    "### Logistic Regression\n",
    "binary classification   \n",
    "\n",
    "### Linear Regression  \n",
    "\n",
    "### Deep Learning\n",
    "Learn the parameters that minimize the error  \n",
    "\n",
    "### Training in Deep Learning  \n",
    "  modifiy the parameters (w, b...) in order to minimize the cost function  \n",
    "\n",
    "### Parameters     \n",
    "  - weight (w)  \n",
    "  - bias (b)  \n",
    "\n",
    "### Hyperparameters   \n",
    "  they control the parameters and determine their final value   \n",
    "  - learning rate (alpha)   \n",
    "    - if alpha too large, we may overshoot the optimal value  \n",
    "    - if alpha too small, too many iterations to converge  \n",
    "    tuning = plot the curve with various values  \n",
    "    **See** ai specialization course 1 'w2-002-logistic-regression-with-nn'  \n",
    "  - regularization parameter (lambda)  \n",
    "    tuning = use the dev set   \n",
    "  - number of iterations   \n",
    "  - number of hidden layers\n",
    "  - number of hidden units\n",
    "  - choice of activation functions\n",
    "  - momentum\n",
    "  - mini-batch size\n",
    "\n",
    "### Learning rate\n",
    "- indicate at which pace the weights get updated   \n",
    "- methods:\n",
    "  - Adam (most popular)\n",
    "  - adaptive learning rates:\n",
    "    - Momentum\n",
    "    - RMSProp\n",
    "    - Adam\n",
    "\n",
    "### Hyperparameters tuning     \n",
    "deep learning = empirical process   \n",
    "trying different values of hyperparameters and plotting (nb iterations * cost J) to find the best curve    \n",
    "best values for a given problem can change in time (cpu, data update...)   \n",
    "\n",
    "### Loss function: \n",
    "cost for one example  \n",
    "- **L1** loss function  \n",
    "- **L2** loss function\n",
    " \n",
    "### Cost function: \n",
    "  average loss functions for all examples = `J(w,b)` surface of the curve  \n",
    "\n",
    "### Gradient Descent algorithm: \n",
    "update w and b  \n",
    "\n",
    "###  Derivative/Slope \n",
    "  (partial derivative = J function of 2+ args): height/width   \n",
    "  for any value of a, if you nudge it by a certain amount, the slope will be ~\n",
    "  Derivatives at each step are used to compute derivatives at following steps  \n",
    "\n",
    "### Computation Graph: \n",
    "  organizes the computations:  \n",
    "  1. Initialize parameters / Define hyperparameters: nn architecture  \n",
    "  2. Loop for num_iterations:   \n",
    "    a. Forward propagation: compute output (linear->activation)   \n",
    "    b. Compute cost function   \n",
    "    c. Backward propagation: compute gradients/derivatives using cost   \n",
    "    d. Update parameters: gradient descent, using parameters, and grads from backprop    \n",
    "  4. Predict: use trained parameters to predict labels\n",
    "  **See** ai specialization course 1 'w2-002-logistic-regression-with-nn' + exos c1w4\n",
    "\n",
    "### Chain rule  \n",
    "  in backpropagation, if we change a, we change z, so we change J  \n",
    "\n",
    "### Backpropagation  \n",
    "[coursera ai: backpropagation](https://www.coursera.org/learn/neural-networks-deep-learning/lecture/6dDj7/backpropagation-intuition-optional).  \n",
    "  compute the derivative of the final output with respect to some other variable usually called Â´dvarÂ´ in code  \n",
    "\n",
    "### Vectorization  \n",
    "  whenever possible, avoid explicit Â´forÂ´Â loops  \n",
    "\n",
    "### Broadcasting in Python  \n",
    "  https://numpy.org/doc/stable/user/basics.broadcasting.html  \n",
    "\n",
    "### Softmax   \n",
    "  normalizing function used when algorithm needs to classify two or more classes    \n",
    "\n",
    "### Hidden\n",
    "not observed, not seen in the training set\n",
    "\n",
    "### Activation functions\n",
    "[coursera ai: activation functions](https://www.coursera.org/learn/neural-networks-deep-learning/lecture/4dDC1/activation-functions)    \n",
    "[coursera ai: nns-non-linear-activation-functions](https://www.coursera.org/learn/neural-networks-deep-learning/lecture/OASKH/why-do-you-need-non-linear-activation-functions): why do nns need non-linear activation functions   \n",
    "   -> because the nn would just be outputing a linear function of the input   \n",
    "   = standard linear regression.   \n",
    "linear activation function: g(z) = z -> outputs the input  \n",
    "[coursera ai: derivatives of activation functions](https://www.coursera.org/learn/neural-networks-deep-learning/lecture/qcG1j/derivatives-of-activation-functions)   \n",
    "  -> output values based on possible input values (max, min, 0)     \n",
    "    \n",
    "can be different for different layers in the same nn\n",
    "- **sigmoid**: [0..1] -> only better for binary classification (0 or 1)  \n",
    "- **tanh**: hyperbolic tangent [-1..1] -> 0 mean. always better for hidden layers. makes learning easier for the next layer  \n",
    "- **relu**: [1 or 0 (or 0,00000000000x)] *most used* -> faster learning of all  \n",
    "- **leaky relu**:  \n",
    "\n",
    "### Initialization\n",
    "[coursera ai: random initialization](https://www.coursera.org/learn/neural-networks-deep-learning/lecture/XtFPI/random-initialization).   \n",
    "- **weights**: do not initialize with zeros -> all hidden units are symmetric, they all compute the same function. initialize with random values instead.   \n",
    "`np.random.randn((2,2)) * 0.01`   # '* 0.01' initializes to very small values   \n",
    "if weights too large, GD will be very slow (values in the flat parts of the activation functions)  \n",
    "- **bias**: can be initialized with zeros, as long as weights are note   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. OPTIMIZING NEURAL NETWORKS\n",
    "\n",
    "**!!! READ**: [deeplearning.ai: Initialization + Optimization](https://www.deeplearning.ai/ai-notes/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mismatch train/test distribution\n",
    " - training = cat picture from web pages               -> high res\n",
    " - test set = cat pictures from users using your app   -> blurry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias/Variance tradeoff\n",
    " (Optimal/human error = 0% - always compare to optimal error)   \n",
    " Training error: Tells you how much fitting is done, and if you have a bias problem  \n",
    " How much higher is error on dev set compared to training set tells you how bas is the variance problem    \n",
    " In the past, reducing bias always led to more variance, and reducing variance always let to more bias, so we had to choose   \n",
    " Now we have the tools to drive down one without hurting the other much       \n",
    "  \n",
    " - **high variance** = too much flexibility\n",
    "   - train set error 1%\n",
    "   - dev set error 11%   \n",
    " -> doing well on train set but poorly on dev set, not generalizing well, curve with too much flexibility\n",
    " - SOLUTION: get more data, regularization, change network architecture      \n",
    "    \n",
    "      \n",
    " - **high bias** = underfitting   \n",
    "   - train set error 15%\n",
    "   - dev set error 16%   \n",
    " -> not fitting the training set   \n",
    " - SOLUTION: bigger network, train longer, change network architecture  \n",
    "    \n",
    "      \n",
    " - **high bias + high variance** \n",
    "   - train set error 15%\n",
    "   - dev set error 30%   \n",
    " -> worst case scenario\n",
    "    \n",
    "      \n",
    " - **low bias + low variance** \n",
    "   - train set error 0.5%\n",
    "   - dev set error 1%   \n",
    " -> best case scenario. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal (Bayes) error:\n",
    "base assumption on human error. 0%   \n",
    "can change when even human can't judge (blurry images).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1. Standardization\n",
    "\n",
    "- substract the mean of the whole numpy array from each example,     \n",
    "- and then divide each example by the standard deviation of the whole numpy array  \n",
    "- ex: `train_set_x = train_set_x_flatten/255`  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2. Regularization\n",
    "\n",
    "- **Why it reduces overfitting**:   \n",
    "  - prevents weight matrices from being too large\n",
    "  - if `lambda` very big   \n",
    "  => `W` very small & zero out the impact of lots of hidden neurons    \n",
    "  => `z` very small   \n",
    "  - and nn becomes ***smaller and simpler***\n",
    "  - takes you closer to underfitting\n",
    "  - when `z` small, function is linear (see picture curve), only becomes less linear with larger values of z\n",
    "    \n",
    "     \n",
    "- **First** focus on finding a good algorithm, **then** correct overfitting\n",
    "- `W` is usually very high dimension, while `b` is just a scalar, so don't bother regularizing `b`\n",
    "- **lambd**: the regularization parameter, is set using the dev set    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **L2** regularization -> more often used, also called ***weight decay***   \n",
    "= multiplying the weight matrix by numbers sligthly less than 1.  \n",
    "cours 2 week 1 exo 2: \n",
    "  - L2 regularization makes your decision boundary smoother. If  Î»  is too large, it is also possible to \"oversmooth\", resulting in a model with high bias   \n",
    "  - L2-regularization relies on the assumption that a model with small weights is simpler than a model with large weights. Thus, by penalizing the square values of the weights in the cost function you drive all the weights to smaller values. It becomes too costly for the cost to have large weights! This leads to a smoother model in which the output changes more slowly as the input changes\n",
    "  - Weights end up smaller (\"weight decay\"): Weights are pushed to smaller values   \n",
    "  \n",
    "  \n",
    "- **L1** regularization -> W becomes sparse \n",
    "- **Data augmentation**      \n",
    "- **Dropout** regularization\n",
    "- **Weight regularization**:\n",
    "  - LASSO\n",
    "  - Ridge\n",
    "  - ElasticNet\n",
    "- **Early stopping**: stops the training process as soon as the validation loss reaches a plateau or starts to increase\n",
    "  - **orthogonalization**\n",
    "  -> good alternative = use L2 and train for as long as possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout regularization\n",
    "\n",
    "- Each training example is trained on a different smaller network:\n",
    "  - different units dropped for each training example\n",
    "  - and for each training example, different units dropped at each iteration\n",
    "- Prevents from putting too much weight on some features: inputs can be randomly eliminated, so training can't rely on any one feature, so has to spead out weights   \n",
    "  -> **no overfitting**   \n",
    "   \n",
    "    \n",
    "- **Method**:\n",
    "  - for each training example\n",
    "    - go through each layer of the nn\n",
    "      - for each node in a layer, compute the probability that it can be dropped out (random)\n",
    "      - then remove all those nodes   \n",
    "      -> much smaller nn\n",
    "    - then do backpropagation = training this one example with the smaller network\n",
    "  - for next training example, do the same:\n",
    "    - compute probas -> drop nodes -> backpropagation -> training   \n",
    "  \n",
    "  \n",
    "- **Implementation**:\n",
    "  - ***inverted dropout*** technique (cf. screenshots) -> by for most used today\n",
    "  - `keep_prob = 0.2` -> reduce layers by 20%   \n",
    "  - can be set differently for each layer, or not applied to some layers:\n",
    "    - big layers -> big weight matrices -> higher keep_prob\n",
    "    - layers with more risk of overfitting -> lower keep_prob     \n",
    "   \n",
    "   \n",
    "\n",
    "- Better to deactivate it during **optimization**, when plotting cost function (keep_prob=1) (cf. explanation: Course 2 Week 1 Understanding Dropout)     \n",
    "  - **First** focus on finding a good algorithm, **then** correct overfitting\n",
    "- Do not use at **test time**: you don't want your output to be random\n",
    "  - if you want to do it: run prediction process many times with different units dropped randomly at each time and average accross them   \n",
    "  -> computationally inefficient and gives roughly the same results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3. Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.1. Normalization  \n",
    "\n",
    "- **speed up training**. \n",
    "  - same variance for all features (all features on similar small scales)   \n",
    "  -> cost function more symmetric   \n",
    "  -> faster and easier to optimize\n",
    "- divide each row or `x` by its norm `ð‘¥ / â€–ð‘¥â€–`  \n",
    "- increases performance because **gradient descent converges faster**  \n",
    "- https://stats.stackexchange.com/questions/211436/why-normalize-images-by-subtracting-datasets-image-mean-instead-of-the-current    \n",
    "   \n",
    "   \n",
    "- 2 steps:\n",
    "  1. subtract the mean (mu)\n",
    "  2. normalize variances (sigma_^)   \n",
    "- **cf screenshot** course 2 week 1 Normalizing inputs: variance of x1 and x2 both equal to 1\n",
    "- use same values to normalize test set (mu and sigma_^)   \n",
    "-> all data must go throught the same transformations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.2. Vanishing/Exploding gradients\n",
    "\n",
    "- **Exploding**: activation values ***grows*** exponentially as a function of the number of layers\n",
    "- **Vanishing**: activation values ***decreases*** exponentially as a function of the number of layers  \n",
    "  \n",
    "  \n",
    "-> GD tiny little steps -> training very long   \n",
    "was for a long time a barrier for training deep learning\n",
    "  \n",
    "   \n",
    "- **SOLUTION** (partial): better choice of random initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.3. Weight initialization\n",
    "\n",
    "- prevents vanishing/exploding gradients\n",
    "   \n",
    "   \n",
    "\n",
    "- Set the variance of `W[l]` to be equal to `1/n`, or `2/n` with ReLU   \n",
    "  -> all input features are mean 0 and variance 1  \n",
    "  -> weights not too much bigger than 1 and not too much less than 1   \n",
    "  -> `Z` on the same scale   \n",
    "  -> doesn't completely solve the problem but helps prevent it    \n",
    "  \n",
    "  \n",
    "- To do that, we initialize the weights:   \n",
    "     `W[l] = np.random.randn(shape) * np.sqrt(1 / n[l-1])`    \n",
    "      \n",
    "    `n[l-1]` number of neurons in the previous layers, number of neurons being fed in each unit of current layer   \n",
    "    \n",
    "    \n",
    "- Variants:\n",
    "  - with ReLU: `np.sqrt(2 / n[l-1])`\n",
    "  - with tanh: `np.sqrt(2 / n[l-1])` = Xavier initialization\n",
    "  - `np.sqrt(2 / (n[l-1] + n[l]))`\n",
    "  \n",
    "  \n",
    "- **Cours 2 Week 1 Exo 1**: He et al., 2015. (If you have heard of \"Xavier initialization\", this is similar except Xavier initialization uses a scaling factor for the weights  W[l]W[l]  of sqrt(1./layers_dims[l-1]) where He initialization would use sqrt(2./layers_dims[l-1]).)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.4. Numerical approximation of gradients\n",
    "\n",
    "- Compute the slope not just using a point after, but using a point after `Î¸ + Îµ` and a point before `Î¸ - Îµ` with same distance to current point `Î¸`  \n",
    "   \n",
    "   `[f(Î¸ + Îµ) + f(Î¸ - Îµ)] / 2 * Îµ`\n",
    "   \n",
    "  instead of:   \n",
    "\n",
    "   `[f(Î¸ + Îµ) / Îµ`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.5. Gradient Checking\n",
    "\n",
    "- Used during the backpropagation: compares the value of the *analytical gradient* to the *numerical gradient* at given points and plays the role of a ***sanity-check for correctness***   \n",
    "   \n",
    "   \n",
    "- [Gradient Checking implementation](https://www.coursera.org/learn/deep-neural-network/lecture/6igIc/gradient-checking-implementation-notes)  \n",
    "  - do not use at training, only at debut\n",
    "  - does not work with dropout, so deactivate dropout (`keep_dims = 1.0`)\n",
    "  - analysis...\n",
    "  \n",
    "  \n",
    "- Concatenate all parameters into a giant flat vector:  \n",
    "  `Î¸` = `W[1] b[1] ... W[L] b[L]`   \n",
    "  \n",
    "  \n",
    "- Do the same with derivatives:   \n",
    "  `d_Î¸` = `d_W[1] d_b[1] ... d_W[L] d_b[L]`   \n",
    "  \n",
    "  \n",
    "- **Question**: Is   `d_Î¸` the gradient of the cost function `J(Î¸)`?\n",
    "  \n",
    "  - compute:\n",
    "   \n",
    "    ```\n",
    "    for each i:\n",
    "      d_Î¸_approx[i] = [J(Î¸_1 ... tÎ¸_i + Îµ ...) + J(Î¸_1 ... Î¸_i - Îµ ...)] / (2 * Îµ)\n",
    "    ```   \n",
    "  \n",
    "  - Check if `d_Î¸_approx[i]` is equal to `d_Î¸[i]` (euclidean distance)\n",
    "  - Result should be equal to the value set for `Îµ`\n",
    "  - Otherwise, check visualization of data to find data points for which value is much higher or lower = incorrect derivative computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.6. Optimization algorithms\n",
    "\n",
    "- Used to train NNs much faster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.6.1. Mini-batch gradient descent\n",
    "[coursera ai: c2 w2 Mini-batch GD](https://www.coursera.org/learn/deep-neural-network/lecture/qcogH/mini-batch-gradient-descent)\n",
    "\n",
    "[coursera ai: c2 w2 Understanding mini-batch GD](https://www.coursera.org/learn/deep-neural-network/lecture/lBXu8/understanding-mini-batch-gradient-descent)\n",
    "\n",
    "- Batch GD = process the entire training set all at the same time = one batch\n",
    "- Mini-batch GD = process mini-batches at the same time:    \n",
    "  forward propagation -> cost function -> backpropagation -> weights update     \n",
    "  = **much faster** because GD larger steps (vectorization)\n",
    "- 5.000.000 training examples, mini-batches of 1.000   \n",
    "  - = 5.000 mini-batches: `[X{1}, X{2}...]`\n",
    "  - mini-batch `t: X{t}, Y{t}` with `X{t}.shape = (Mx, 1000)` and `Y{t}.shape = (Mx, 1)`\n",
    "- mini-batch size: between 2 and m (nb trng ex), not too large or too small    \n",
    "  see coursera 'Understanding mini-batch GD' :\n",
    "  - shuffle training data before splitting into mini-batches\n",
    "  - 64, 128, 256, 512, 1024...\n",
    "  - fits in CPU/GPU memory\n",
    "  - try different values to find the one that makes GD the more efficient  \n",
    "- With a well-turned mini-batch size, usually it outperforms either gradient descent or stochastic gradient descent (particularly when the training set is large)\n",
    "- Shuffling and Partitioning are the two steps required to build mini-batches\n",
    "- Powers of two are often chosen to be the mini-batch size, e.g., 16, 32, 64, 128.\n",
    "  \n",
    "  \n",
    "- **Stochastic gradient descent**: \n",
    "  - Each training example is its own mini-batch, i.e. mini-batch size = 1    \n",
    "  - You use only 1 training example before updating the gradients\n",
    "  - When the training set is large, SGD can be faster. But the parameters will \"oscillate\" toward the minimum rather than converge smoothly  \n",
    "   \n",
    "   \n",
    "- **Batch GD**:mini-batch size = m (nb trng ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.6.2. Exponentially Weighted (Moving) Averages\n",
    "\n",
    "- Faster than mini-batch GD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.6.3. Gradient Descent with Momentum\n",
    " - Faster than standard GD\n",
    " - Speeds up GD when oscillations high vertically and small horizontally\n",
    " - Oscillating steps: ***smaller vertically and larger horizontally***   \n",
    "   = slows down learning in vertical direction + speeds it up in horizontal direction   \n",
    "   -> faster!\n",
    " - See **implementation details** in the course\n",
    "    \n",
    "     \n",
    " - **Momentum** takes into account the **past gradients** to smooth out the update. We will store the 'direction' of the previous gradients in the variable  `v` . Formally, this will be the ***exponentially weighted average of the gradient on previous steps***. You can also think of  `v`  as the **velocity** of a ball rolling downhill, building up **speed (and momentum)** according to the direction of the gradient/slope of the hill.\n",
    "  \n",
    "  \n",
    "- The velocity is initialized with zeros. So the algorithm will take a few iterations to \"build up\" velocity and start to take bigger steps.\n",
    "- If  Î²=0Î²=0 , then this just becomes standard gradient descent without momentum\n",
    "  \n",
    "  \n",
    "- **How do you choose `Î²`:**\n",
    "  - The larger the momentum `Î²` is, the smoother the update because the more we take the past gradients into account. But if `Î²` is too big, it could also smooth out the updates too much.\n",
    "  - Common values for `Î²` range from `0.8` to `0.999`. If you don't feel inclined to tune this, `Î²=0.9`  is often a reasonable default.\n",
    "  - Tuning the optimal `Î²` for your model might need trying several values to see what works best in term of reducing the value of the cost function `J`.\n",
    "  \n",
    "  \n",
    "- Momentum takes past gradients into account to smooth out the steps of gradient descent. It can be applied with batch gradient descent, mini-batch gradient descent or stochastic gradient descent.\n",
    "- You have to tune a momentum hyperparameter  Î²Î²  and a learning rate  Î±"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.6.4. RMSProp\n",
    "\n",
    " - Speeds up GD when oscillations high vertically and small horizontally\n",
    " - Oscillating steps: ***smaller vertically and larger horizontally***   \n",
    "   = slows down learning in vertical direction + speeds it up in horizontal direction   \n",
    "   -> faster!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.6.5. Adam Optimization: Adaptive Momentum Estimation\n",
    "course 2 week 2\n",
    "\n",
    "- **Combination of Momentum and RMSProp**:\n",
    "  - Compute `dW`, `db` using current mini-batch\n",
    "  - Compute Momentum: `V_dW` and `V_db` (parameter `beta1`)\n",
    "  - Compute RMSProp: `S_dW` and `S_db` (parameter `beta2`)\n",
    "  - Compute bias correction\n",
    "  - Update `W` and `b`  \n",
    "  \n",
    "  \n",
    "- **Parameters**:\n",
    "  - `alpha`: learning rate, to be tuned\n",
    "  - `beta1`: default `0.9` (inventors recommendation)\n",
    "  - `beta2`: default `0.999` (inventors recommendation)\n",
    "  - `Îµ`: `10^-8` (inventors recommendation, doesn't matter much, not effect on perf)  \n",
    "   \n",
    "   \n",
    "- **How does Adam work**:\n",
    "  - It calculates an exponentially weighted average of past gradients, and stores it in variables  vv  (before bias correction) and  vcorrectedvcorrected  (with bias correction).\n",
    "  - It calculates an exponentially weighted average of the squares of the past gradients, and stores it in variables  ss  (before bias correction) and  scorrectedscorrected  (with bias correction).\n",
    "  - It updates parameters in a direction based on combining information from \"1\" and \"2\"\n",
    "  \n",
    "    \n",
    "- Works well with deep learning problems and a very wide variety of architectures\n",
    "- Works with mini-batch GD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.6.6. Learning Rate Decay\n",
    "\n",
    "= Slowly reduce the learning rate over time\n",
    "  \n",
    "   \n",
    "- **Parameters**: \n",
    "  - `alpha0`\n",
    "  - `decay_rate`\n",
    "  \n",
    "  \n",
    "- **Method 1**:\n",
    "  - `alpha0 = 0.2` and `decay_rate = 1`\n",
    "  - `alpha = (1 / (1 + decay_rate * epoch_num)) * alpha0`\n",
    "  - epoch 1: `alpha = 0.1`   \n",
    "    epoch 2: `alpha = 0.67`   \n",
    "    epoch 3: `alpha = 0.5`   \n",
    "    epoch 4: `alpha = 0.4`... \n",
    "    -> plot (alpha * epoch_num) (x, y)  \n",
    "    \n",
    "    \n",
    "- Other methods:\n",
    "  - exponential learning rate decay: `alpha = 0.95^epoch_num * alpha0`\n",
    "  - discrete staircase\n",
    "  - etc.\n",
    "  - manual decay (small number of models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.6.7. Local optima\n",
    "\n",
    "- bad optima\n",
    "- saddle point\n",
    "- convex or concave like functions\n",
    "- problem of plateaus = derivative close to 0 for a long time   \n",
    "  -> GD very long slow path to get off the plateau towards optimum\n",
    "  \n",
    "-> can be avoided with optimization algorithms seen before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get your dimensions right\n",
    "\n",
    "[coursera ai: getting your dimensions right](https://www.coursera.org/learn/neural-networks-deep-learning/lecture/Rz47X/getting-your-matrix-dimensions-right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5,)\n",
      "[-0.64059518  0.44481699  0.96361191  0.02271607  3.31552572]\n",
      "12.52999907266234\n",
      "\n",
      "[[-1.04325495]\n",
      " [ 1.44211879]\n",
      " [-0.48980482]\n",
      " [-0.54597879]\n",
      " [ 0.72110785]]\n",
      "[[-1.04325495  1.44211879 -0.48980482 -0.54597879  0.72110785]]\n",
      "[[ 1.0883809  -1.50449757  0.51099131  0.56959508 -0.75229934]\n",
      " [-1.50449757  2.07970659 -0.70635674 -0.78736627  1.03992318]\n",
      " [ 0.51099131 -0.70635674  0.23990876  0.26742305 -0.3532021 ]\n",
      " [ 0.56959508 -0.78736627  0.26742305  0.29809284 -0.39370959]\n",
      " [-0.75229934  1.03992318 -0.3532021  -0.39370959  0.51999653]]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-b22947bce8e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# A trick when you want to flatten a matrix X of shape (a,b,c,d)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# to a matrix X_flatten of shape (b âˆ— c âˆ— d, a) is to use:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mX_flatten\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m## Many software bugs in deep learning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# --- Rank 1 arrays (value,)\n",
    "a = np.random.randn(5)\n",
    "print(a.shape)\n",
    "print(a.T)                # a and a.T are the same\n",
    "print(np.dot(a, a.T))     # value\n",
    "\n",
    "# --- Column vectors: (value, 1) - Row vectors: (1, value)\n",
    "print(\"\")\n",
    "a = np.random.randn(5,1)  # 5*1 matrix\n",
    "print(a)\n",
    "print(a.T)                # inversed shape\n",
    "print(np.dot(a, a.T))     # vector\n",
    "assert(a.shape == (5,1))  # can help as DOCUMENTATION\n",
    "\n",
    "# --- ALWAYS use column or row vectors, not rank 1 arrays\n",
    "# OR reshape:\n",
    "a.reshape((5,1))\n",
    "\n",
    "# A trick when you want to flatten a matrix X of shape (a,b,c,d) \n",
    "# to a matrix X_flatten of shape (b âˆ— c âˆ— d, a) is to use:\n",
    "X_flatten = X.reshape(X.shape[0], -1).T\n",
    "\n",
    "## Many software bugs in deep learning \n",
    "## come from having matrix/vector dimensions that don't fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best NN perfs = scale\n",
    "  scale algorithms: very large NN (lots of hidden layers, of parameters, of connections)  \n",
    "+ scale data: very large amount of data  \n",
    "+ scale computation: CPU, GPU  \n",
    "ex: switch from sigmoid (lrng becomes really slow at extremities) to relu  \n",
    " -> GD works much faster  \n",
    " (Neural Networks and Deep Learning - Semaine 1 - Why is Deep Learning taking off?)  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
